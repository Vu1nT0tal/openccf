{
    "2022": [
        {
            "dblp_url": "https://dblp.uni-trier.de/db/conf/ih/ihmmsec2022.html",
            "conf_title": "10th IH&MMSec 2022: Santa Barbara, CA, USA",
            "conf_url": "https://doi.org/10.1145/3531536",
            "papers": [
                {
                    "url": "https://doi.org/10.1145/3531536.3532956",
                    "title": "Towards Generalization in Deepfake Detection",
                    "authors": "Luisa Verdoliva",
                    "abstract": "In recent years there have been astonishing advances in AI-based synthetic media generation. Thanks to deep learning-based approaches it is now possible to generate data with a high level of realism. While this opens up new opportunities for the entertainment industry, it simultaneously undermines the reliability of multimedia content and supports the spread of false or manipulated information on the Internet. This is especially true for human faces, allowing to easily create new identities or change only some specific attributes of a real face in a video, so-called deepfakes. In this context, it is important to develop automated tools to detect manipulated media in a reliable and timely manner. This talk will describe the most reliable deep learning-based approaches for detecting deepfakes, with a focus on those that enable domain generalization [1]. The results will be presented on challenging datasets [2,3] with reference to realistic scenarios, such as the dissemination of manipulated images and videos on social networks. Finally, new possible directions will be outlined.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "近年来，基于人工智能的合成媒体一代取得了惊人的进展。由于基于深度学习的方法，现在可以生成高度真实的数据。虽然这为娱乐业带来了新的机遇，但同时也破坏了多媒体内容的可靠性，并助长了互联网上虚假或篡改信息的传播。对于人脸来说尤其如此，允许轻松地创建新的身份或只改变视频中真实人脸的某些特定属性，即所谓的deepfakes。在这种情况下，重要的是开发自动工具，以可靠和及时的方式检测被操纵的媒体。本次演讲将描述最可靠的基于深度学习的方法来检测deepfakes，重点是那些能够实现领域泛化的方法[1]。研究结果将在具有挑战性的数据集[2，3]上展示，并参考现实场景，如在社交网络上传播经过处理的图像和视频。最后，将概述新的可能方向。",
                    "title_zh": "走向深度人脸检测的一般化"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3533774",
                    "title": "Looking for Signals: A Systems Security Perspective",
                    "authors": "Christopher Kruegel",
                    "abstract": "Over the last 20 years, my students and I have built systems that look for signals of malice in large datasets. These datasets include network traffic, program code, web transactions, and social media posts. For many of our detection systems, we used feature engineering to model properties of the data and then leveraged different types of machine learning to find outliers or to build classifiers that could recognize unwanted inputs. In this presentation, I will cover three recent works that go beyond that basic approach. First, I will talk about cross-dataset analysis. The key idea is that we look at the same data from different vantage points. Instead of directly detecting malicious instances, the analysis compares the views across multiple angles and finds those cases where these views meaningfully differ. Second, I will cover an approach to perform meta-analysis of the outputs (events) that a detection model might produce. Sometimes, looking at a single event is insufficient to determine whether it is malicious. In such cases, it is necessary to correlate multiple events. We have built a semi-supervised analysis that leverages the context of an event to determine whether it should be treated as malicious or not. Third, I will discuss ways in which attackers might attempt to thwart our efforts to build detectors. Specifically, I will talk about a fast and efficient clean-label dataset poisoning attack. In this attack, correctly labeled poison samples are injected into the training dataset. While these poison samples look legitimate to a human observer, they contain malicious characteristics that trigger a targeted misclassification during detection (inference).",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在过去的20年里，我和我的学生建立了在大型数据集中寻找恶意信号的系统。这些数据集包括网络流量、程序代码、网络交易和社交媒体帖子。对于我们的许多检测系统，我们使用特征工程来建模数据的属性，然后利用不同类型的机器学习来发现异常值或建立可以识别不需要的输入的分类器。在这次演讲中，我将介绍三个超越这一基本方法的最新作品。首先，我将谈论跨数据集分析。关键的想法是，我们从不同的角度看同样的数据。该分析不是直接检测恶意实例，而是从多个角度比较视图，并找出这些视图有意义差异的情况。其次，我将介绍一种对检测模型可能产生的输出(事件)进行元分析的方法。有时，只看单个事件不足以确定它是否是恶意的。在这种情况下，有必要将多个事件关联起来。我们建立了一个半监督分析，利用事件的上下文来确定它是否应该被视为恶意的。第三，我将讨论攻击者可能试图阻挠我们构建检测器的方法。具体来说，我将讨论一种快速有效的干净标签数据集中毒攻击。在这种攻击中，正确标记的毒药样本被注入到训练数据集中。虽然这些病毒样本对人类观察者来说看起来是合法的，但它们包含恶意特征，在检测(推断)过程中会触发有针对性的错误分类。",
                    "title_zh": "寻找信号:系统安全视角"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532957",
                    "title": "Intellectual Property (IP) Protection for Deep Learning and Federated Learning Models",
                    "authors": "Farinaz Koushanfar",
                    "abstract": "This talk focuses on end-to-end protection of the present and emerging Deep Learning (DL) and Federated Learning (FL) models. On the one hand, DL and FL models are usually trained by allocating significant computational resources to process massive training data. The built models are therefore considered as the owner's IP and need to be protected. On the other hand, malicious attackers may take advantage of the models for illegal usages. IP protection needs to be considered during the design and training of the DL models before the owners make their models publicly available. The tremendous parameter space of DL models allows them to learn hidden features automatically. We explore the 'over-parameterization' of DL models and demonstrate how to hide additional information within DL. Particularly, we discuss a number of our end-to-end automated frameworks over the past few years that leverage information hiding for IP protection, including: DeepSigns[5] and DeepMarks[2], the first DL watermarking and fingerprinting frameworks that work by embedding the owner's signature in the dynamic activations and output behaviors of the DL model; DeepAttest[1], the first hardware-based attestation framework for verifying the legitimacy of the deployed model via on-device attestation. We also develop a multi-bit black-box DNN watermarking scheme[3] and demonstrate spread spectrum-based DL watermarking[4]. In the context of Federated Learning (FL), we show how these results can be leveraged for the design of a novel holistic covert communication framework that allows stealthy information sharing between local clients while preserving FL convergence. We conclude by outlining the open challenges and emerging directions.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "本次演讲的重点是当前和新兴的深度学习(DL)和联合学习(FL)模型的端到端保护。一方面，DL和FL模型通常通过分配大量计算资源来处理海量训练数据来训练。因此，建造的模型被认为是所有者的知识产权，需要得到保护。另一方面，恶意攻击者可能会利用模型进行非法使用。在所有者公开他们的模型之前，需要在DL模型的设计和培训期间考虑知识产权保护。DL模型的巨大参数空间允许它们自动学习隐藏特征。我们探索了DL模型的“过度参数化”,并演示了如何在DL中隐藏附加信息。特别是，我们讨论了过去几年中利用信息隐藏进行知识产权保护的许多端到端自动化框架，包括:DeepSigns[5]和DeepMarks[2]，这是第一个通过在DL模型的动态激活和输出行为中嵌入所有者签名来工作的DL水印和指纹框架；DeepAttest[1]，第一个基于硬件的证明框架，用于通过设备上的证明来验证部署模型的合法性。我们还开发了一个多位黑盒DNN水印方案[3]，并演示了基于扩频的DL水印[4]。在联邦学习(FL)的背景下，我们展示了如何利用这些结果来设计一种新颖的整体隐蔽通信框架，该框架允许在本地客户端之间秘密共享信息，同时保持FL收敛。最后，我们概述了面临的挑战和新出现的方向。",
                    "title_zh": "深度学习和联合学习模型的知识产权(IP)保护"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532946",
                    "title": "FMFCC-V: An Asian Large-Scale Challenging Dataset for DeepFake Detection",
                    "authors": "Gen Li, Xianfeng Zhao, Yun Cao, Pengfei Pei, Jinchuan Li, Zeyu Zhang",
                    "abstract": "The abuse of DeepFake technique has raised enormous public concerns in recent years. Currently, the existing DeepFake datasets suffer some weaknesses of obvious visual artifacts, minimal Asian proportion, backward synthesis methods and short video length. To make up these weaknesses, we have constructed an Asian large-scale challenging DeepFake dataset to enable the training of DeepFake detection models and organized the accompanying video track of the first Fake Media Forensics Challenge of China Society of Image and Graphics (FMFCC-V). The FMFCC-V dataset is by far the first and the largest public available Asian dataset for DeepFake detection, which contains 38102 DeepFake videos and 44290 pristine videos, corresponding more than 23 million frames. The source videos in the FMFCC-V dataset are carefully collected from 83 paid individuals and all of them are Asians. The DeepFake videos are generated by four of the most popular face swapping methods. Extensive perturbations are applied to obtain a more challenging benchmark of higher diversity. The FMFCC-V dataset can lend powerful support to the development of more effective DeepFake detection methods. We contribute a comprehensive evaluation of six representative DeepFake detection methods to demonstrate the level of challenge posed by FMFCC-V dataset. Meanwhile, we provide a detailed analysis of the top submissions from the FMFCC-V competition.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3531536.3532946"
                    },
                    "abstract_zh": "近年来，DeepFake技术的滥用引起了公众的极大关注。目前，已有的DeepFake数据集存在视觉伪影明显、亚洲人比例极小、合成方法落后和视频长度短等缺点。为了弥补这些弱点，我们构建了一个亚洲大规模挑战性的DeepFake数据集，以支持DeepFake检测模型的训练，并组织了中国图像与图形学会首届虚假媒体取证挑战赛(FMFCC-V)的配套视频轨道。FMFCC-V数据集是迄今为止第一个也是最大的公开可用的亚洲DeepFake检测数据集，包含38102个DeepFake视频和44290个原始视频，对应超过2300万帧。FMFCC-V数据集中的源视频是从83个付费个人中精心收集的，他们都是亚洲人。DeepFake视频由四种最流行的面部交换方法生成。广泛的扰动被用于获得更高多样性的更具挑战性的基准。FMFCC-V数据集可以为开发更有效的DeepFake检测方法提供有力的支持。我们对六种有代表性的DeepFake检测方法进行了综合评估，以展示FMFCC-V数据集所带来的挑战水平。与此同时，我们提供了对FMFCC-V竞赛中排名靠前的作品的详细分析。",
                    "title_zh": "FMFCC-V:一个亚洲大规模挑战性的DeepFake检测数据集"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532962",
                    "title": "Know Your Library: How the libjpeg Version Influences Compression and Decompression Results",
                    "authors": "Martin Benes, Nora Hofer, Rainer Böhme",
                    "abstract": "Introduced in 1991, libjpeg has become a well-established library for processing JPEG images. Many libraries in high-level languages use libjpeg under the hood. So far, little attention has been paid to the fact that different versions of the library produce different outputs for the same input. This may have implications on security-related applications, such as image forensics or steganalysis, where evidence is generated by tracking small, imperceptible changes in JPEG-compressed signals. This paper systematically analyses all libjpeg versions since 1998, including the forked libjpeg-turbo (in its latest version). It compares the outputs of compression and decompression operations for a range of parameter settings. We identify up to three distinct behaviors for compression and up to six for decompression.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3531536.3532962"
                    },
                    "abstract_zh": "自1991年推出以来，libjpeg已经成为一个成熟的jpeg图像处理库。许多高级语言的库在幕后使用libjpeg。到目前为止，很少有人注意到不同版本的库对相同的输入产生不同的输出。这可能会对安全相关应用产生影响，如图像取证或隐写分析，其中证据是通过跟踪JPEG压缩信号中微小的、察觉不到的变化来生成的。本文系统分析了自1998年以来的所有libjpeg版本，包括分叉的libjpeg-turbo(在其最新版本中)。它比较一系列参数设置的压缩和解压缩操作的输出。我们为压缩确定了三种不同的行为，为解压缩确定了六种不同的行为。",
                    "title_zh": "了解你的库:libjpeg版本如何影响压缩和解压缩结果"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532964",
                    "title": "Identity-Referenced Deepfake Detection with Contrastive Learning",
                    "authors": "Dongyao Shen, Youjian Zhao, Chengbin Quan",
                    "abstract": "With current advancements in deep learning technology, it is becoming easier to create high-quality face forgery videos, causing concerns about the misuse of deepfake technology. In recent years, research on deepfake detection has become a popular topic. Many detection methods have been proposed, most of which focus on exploiting image artifacts or frequency domain features for detection. In this work, we propose using real images of the same identity as a reference to improve detection performance. Specifically, a real image of the same identity is used as a reference image and input into the model together with the image to be tested to learn the distinguishable identity representation, which is achieved by contrastive learning. Our method achieves superior performance on both FaceForensics++ and Celeb-DF with relatively little training data, and also achieves very competitive results on cross-manipulation and cross-dataset evaluations, demonstrating the effectiveness of our solution.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3531536.3532964"
                    },
                    "abstract_zh": "随着当前深度学习技术的进步，创建高质量的人脸伪造视频变得越来越容易，引起了对deepfake技术滥用的担忧。近年来，对deepfake检测的研究已经成为一个热门话题。已经提出了许多检测方法，其中大多数集中在利用图像伪影或频域特征进行检测。在这项工作中，我们建议使用相同身份的真实图像作为参考，以提高检测性能。具体来说，使用相同身份的真实图像作为参考图像，与待测试图像一起输入到模型中，以学习可区分的身份表征，这是通过对比学习来实现的。我们的方法在FaceForensics++和Celeb-DF上以相对较少的训练数据获得了优越的性能，并且在交叉操作和交叉数据集评估上也获得了非常有竞争力的结果，证明了我们的解决方案的有效性。",
                    "title_zh": "基于对比学习的身份参照深度伪造检测"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532961",
                    "title": "Sparse Trigger Pattern Guided Deep Learning Model Watermarking",
                    "authors": "Chun-Shien Lu",
                    "abstract": "Watermarking neural networks (NNs) for ownership protection has received considerable attention recently. Resisting both model pruning and fine-tuning is commonly considered to evaluate the robustness of a watermarked NN. However, the rationale behind such a robustness is still relatively unexplored in the literature. In this paper, we study this problem to propose a so-called sparse trigger pattern (STP) guided deep learning model watermarking method. We provide empirical evidence to show that trigger patterns are able to make the distribution of model parameters compact, and thus exhibit interpretable resilience to model pruning and fine-tuning. We find the effect of STP can also be technically interpreted as the first layer dropout. Extensive experiments demonstrate the robustness of our method.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "最近，用于所有权保护的水印神经网络受到了广泛关注。抵制模型修剪和微调通常被认为是评估水印神经网络的鲁棒性。然而，这种稳健性背后的基本原理在文献中仍然相对未被探索。本文针对这一问题，提出了一种稀疏触发模式(STP)引导的深度学习模型水印方法。我们提供的经验证据表明，触发模式能够使模型参数的分布紧凑，从而表现出对模型修剪和微调的可解释的弹性。我们发现STP的影响也可以从技术上解释为第一层脱落。大量的实验证明了我们方法的鲁棒性。",
                    "title_zh": "稀疏触发模式引导的深度学习模型水印"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532950",
                    "title": "BlindSpot: Watermarking Through Fairness",
                    "authors": "Sofiane Lounici, Melek Önen, Orhan Ermis, Slim Trabelsi",
                    "abstract": "With the increasing development of machine learning models in daily businesses, a strong need for intellectual property protection arised. For this purpose, current works suggest to leverage backdoor techniques to embed a watermark into the model, by overfitting to a set of particularly crafted and secret input-output pairs called triggers. By sending verification queries containing triggers, the model owner can analyse the behavior of any suspect model on the queries to claim its ownership. However, when it comes to scenarios where frequent monitoring is needed, the computational overhead of these verification queries in terms of volume demonstrates that backdoor-based watermarking appears to be too sensitive to outlier detection attacks and cannot guarantee the secrecy of the triggers. To solve this issue, we introduce BlindSpot, to watermark machine learning models through fairness. Our trigger-less approach is compatible with a high number of verification queries while being robust to outlier detection attacks. We show on Fashion-MNIST and CIFAR-10 datasets that BlindSpot is efficiently watermarking models while robust to outlier detection attacks, at a performance cost on the accuracy of 2%.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "随着机器学习模型在日常业务中的不断发展，出现了对知识产权保护的强烈需求。为此，目前的工作建议利用后门技术，通过过度拟合一组特别制作的秘密输入输出对(称为触发器)，将水印嵌入到模型中。通过发送包含触发器的验证查询，模型所有者可以分析查询上任何可疑模型的行为，以声明其所有权。然而，当涉及到需要频繁监控的场景时，这些验证查询在数量方面的计算开销表明，基于后门的水印似乎对异常值检测攻击过于敏感，并且不能保证触发的保密性。为了解决这个问题，我们引入盲点，通过公平性对机器学习模型进行水印。我们的无触发器方法与大量验证查询兼容，同时对离群点检测攻击具有鲁棒性。我们在时尚MNIST和CIFAR-10数据集上显示，盲点是有效的水印模型，同时对离群点检测攻击具有鲁棒性，性能成本为2%的准确性。",
                    "title_zh": "盲点:通过公平性水印"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532966",
                    "title": "Hiding Needles in a Haystack: Towards Constructing Neural Networks that Evade Verification",
                    "authors": "Árpád Berta, Gábor Danner, István Hegedüs, Márk Jelasity",
                    "abstract": "Machine learning models are vulnerable to adversarial attacks, where a small, invisible, malicious perturbation of the input changes the predicted label. A large area of research is concerned with verification techniques that attempt to decide whether a given model has adversarial inputs close to a given benign input. Here, we show that current approaches to verification have a key vulnerability: we construct a model that is not robust but passes current verifiers. The idea is to insert artificial adversarial perturbations by adding a backdoor to a robust neural network model. In our construction, the adversarial input subspace that triggers the backdoor has a very small volume, and outside this subspace the gradient of the model is identical to that of the clean model. In other words, we seek to create a \"needle in a haystack\" search problem. For practical purposes, we also require that the adversarial samples be robust to JPEG compression. Large \"needle in the haystack\" problems are practically impossible to solve with any search algorithm. Formal verifiers can handle this in principle, but they do not scale up to real-world networks at the moment, and achieving this is a challenge because the verification problem is NP-complete. Our construction is based on training a hiding and a revealing network using deep steganography. Using the revealing network, we create a separate backdoor network and integrate it into the target network. We train our deep steganography networks over the CIFAR-10 dataset. We then evaluate our construction using state-of-the-art adversarial attacks and backdoor detectors over the CIFAR-10 and the ImageNet datasets. We made the code and models publicly available at https://github.com/szegedai/hiding-needles-in-a-haystack.",
                    "files": {
                        "openAccessPdf": "https://publicatio.bibl.u-szeged.hu/24585/1/3531536.3532966.pdf"
                    },
                    "abstract_zh": "机器学习模型容易受到对抗性攻击，其中输入的微小、不可见的恶意扰动会改变预测的标签。一个很大的研究领域与验证技术有关，这些验证技术试图决定一个给定的模型是否具有接近于一个给定的良性输入的对抗性输入。在这里，我们表明，目前的验证方法有一个关键的漏洞:我们构建了一个不健壮的模型，但通过了目前的验证。这个想法是通过给一个健壮的神经网络模型添加一个后门来插入人为的对抗性扰动。在我们的构造中，触发后门的对抗性输入子空间具有非常小的体积，并且在这个子空间之外，模型的梯度与干净模型的梯度相同。换句话说，我们试图创造一个“大海捞针”的搜索问题。出于实用目的，我们还要求对立样本对JPEG压缩具有鲁棒性。大型的“大海捞针”问题实际上不可能用任何搜索算法来解决。形式验证器原则上可以处理这一点，但目前它们无法扩展到现实世界的网络，实现这一点是一个挑战，因为验证问题是NP完全的。我们的构建基于使用深度隐写术训练隐藏和揭示网络。使用显示网络，我们创建一个单独的后门网络，并将其集成到目标网络中。我们在CIFAR-10数据集上训练我们的深度隐写术网络。然后，我们在CIFAR-10和ImageNet数据集上使用最先进的对抗性攻击和后门检测器来评估我们的构建。我们在https://github.com/szegedai/hiding-needles-in-a-haystack.公开了代码和模型",
                    "title_zh": "大海捞针:构建逃避验证的神经网络"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532959",
                    "title": "Covert Communications through Imperfect Cancellation",
                    "authors": "Daniel Chew, Christine Nguyen, Samuel Berhanu, Chris Baumgart, A. Brinton Cooper",
                    "abstract": "We propose a method for covert communications using an IEEE 802.11 OFDM/QAM packet as a carrier. We show how to hide the covert message so that the transmitted signal does not violate the spectral mask specified by the standard, and we determine its impact on the OFDM packet error rate (PER). We show conditions under which the hidden signal is not usable and those under which it can be retrieved with a usable bit error rate (BER). The hidden signal is extracted by cancellation of the OFDM signal in the covert receiver. We explore the effects of the hidden signal on OFDM parameter estimation and the covert signal BER. We test the detectability of the covert signal with and without cancellation. We conclude with an experiment where we inject the hidden signal into Over-The-Air (OTA) recordings of 802.11 packets and demonstrate the effectiveness of the technique using that real-world OTA data.",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/2201.10611"
                    },
                    "abstract_zh": "我们提出了一种使用IEEE 802.11 OFDM/QAM包作为载体的隐蔽通信方法。我们展示了如何隐藏隐蔽消息，以使传输信号不违反标准规定的频谱屏蔽，并确定其对OFDM分组差错率(PER)的影响。我们展示了隐藏信号不可用的条件，以及可以用可用的误码率(BER)检索隐藏信号的条件。通过在隐蔽接收器中消除OFDM信号来提取隐藏信号。我们研究了隐藏信号对OFDM参数估计和隐藏信号误码率的影响。我们测试了取消和不取消的隐蔽信号的可检测性。我们以一个实验结束，在该实验中，我们将隐藏信号注入到802.11分组的空中下载(OTA)记录中，并使用真实世界的OTA数据来演示该技术的有效性。",
                    "title_zh": "通过不完全抵消的隐蔽通信"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532947",
                    "title": "Covert Channels in Network Time Security",
                    "authors": "Kevin Lamshöft, Jana Dittmann",
                    "abstract": "Network Time Security (NTS) specified in RFC8915 is a mechanism to provide cryptographic security for clock synchronization using the Network Time Protocol (NTP) as foundation. By using Transport Layer Security (TLS) and Authenticated Encryption with Associated Data (AEAD) NTS is able to ensure integrity and authenticity between server and clients synchronizing time. However, in the past it was shown that time synchronisation protocols such as the Network Time Protocol (NTP) and the Precision Time Protocol (PTP) might be leveraged as carrier for covert channels, potentially infiltrating or exfiltrating information or to be used as Command-and-Control channels in case of malware infections. By systematically analyzing the NTS specification, we identified 12 potential covert channels, which we describe and discuss in this paper. From the 12 channels, we exemplary selected an client-side approach for a proof-of-concept implementation using NTS random UIDs. Further, we analyze and investigate potential countermeasures and propose a design for an active warden capable of mitigating the covert channels described in this paper.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3531536.3532947"
                    },
                    "abstract_zh": "RFC8915中规定的网络时间安全(NTS)是一种使用网络时间协议(NTP)作为基础为时钟同步提供加密安全的机制。通过使用传输层安全性(TLS)和关联数据认证加密(AEAD ), NTS能够确保服务器和客户端之间同步时间的完整性和真实性。然而，过去的情况表明，时间同步协议，如网络时间协议(NTP)和精确时间协议(PTP)可能被用作隐蔽信道的载体，潜在地渗透或泄露信息，或者在恶意软件感染的情况下被用作命令和控制信道。通过系统地分析NTS规范，我们确定了12个潜在的隐蔽通道，我们在本文中描述和讨论。从12个通道中，我们示例性地选择了一种客户端方法，用于使用NTS随机uid的概念验证实施。此外，我们分析和研究了潜在的对策，并提出了一种能够减轻本文中描述的隐蔽通道的主动典狱长的设计。",
                    "title_zh": "网络时间安全中的隐蔽通道"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532953",
                    "title": "Collusion-resistant Fingerprinting of Parallel Content Channels",
                    "authors": "Basheer Joudeh, Boris Skoric",
                    "abstract": "The fingerprinting game is analysed when the coalition size k is known to the tracer, but the colluders can distribute themselves across L TV channels. The collusion channel is introduced and the extra degrees of freedom for the coalition are made manifest in our formulation. We introduce a payoff functional that is analogous to the single TV channel case, and is conjectured to be closely related to the fingerprinting capacity. For the binary alphabet case under the marking assumption, and the restriction of access to one TV channel per person per segment, we derive the asymptotic behavior of the payoff functional. We find that the value of the maximin game for our payoff is asymptotically equal to L2/k2 2 ln 2, with optimal strategy for the tracer being the arcsine distribution, and for the coalition being the interleaving attack across all TV channels, as well as assigning an equal number of colluders across the L TV channels.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3531536.3532953"
                    },
                    "abstract_zh": "当追踪者知道联盟大小k时，分析指纹游戏，但是共谋者可以将他们自己分布在L个电视频道上。在我们的公式中，引入了共谋通道，并显示了联盟的额外自由度。我们引入了一个类似于单个电视频道的支付函数，并推测其与指纹容量密切相关。对于在标记假设下的二进制字母表的情况，并且限制每个人每段访问一个电视频道，我们导出了支付泛函的渐近行为。我们发现，我们的收益的马希民博弈的值渐近等于L2/k2ln 2，追踪器的最优策略是反正弦分布，联盟的最优策略是在所有电视频道上交错攻击，以及在L个电视频道上分配相等数量的共谋。",
                    "title_zh": "并行内容频道的抗共谋指纹识别"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532963",
                    "title": "Domain Adaptational Text Steganalysis Based on Transductive Learning",
                    "authors": "Yiming Xue, Boya Yang, Yaqian Deng, Wanli Peng, Juan Wen",
                    "abstract": "Traditional text steganalysis methods rely on a large amount of labeled data. At the same time, the test data should be independent and identically distributed with the training data. However, in practice, a large number of text types make it difficult to satisfy the i.i.d condition between the training set and the test set, which leads to the problem of domain mismatch and significantly reduces the detection performance. In this paper, we draw on the ideas of domain adaptation and transductive learning to design a novel text steganalysis method. In this method, we design a distributed adaptation layer and adopt three loss functions to achieve domain adaptation, so that the model can learn the domain-invariant text features. The experimental results show that the method has better steganalysis performance in the case of domain mismatch.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "传统的文本隐写分析方法依赖于大量的标记数据。同时，测试数据应该是独立的，与训练数据同分布。然而，在实践中，大量的文本类型使得训练集和测试集之间难以满足i.i.d .条件，从而导致域不匹配的问题，显著降低了检测性能。本文借鉴领域自适应和直推学习的思想，设计了一种新的文本隐写分析方法。在该方法中，我们设计了一个分布式自适应层，并采用三种损失函数来实现领域自适应，使模型能够学习领域不变的文本特征。实验结果表明，该方法在域不匹配的情况下具有较好的隐写分析性能。",
                    "title_zh": "基于直推学习的领域自适应文本隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532949",
                    "title": "Few-shot Text Steganalysis Based on Attentional Meta-learner",
                    "authors": "Juan Wen, Ziwei Zhang, Yu Yang, Yiming Xue",
                    "abstract": "Text steganalysis is a technique to distinguish between steganographic text and normal text via statistical features. Current state-of-the-art text steganalysis models have two limitations. First, they need sufficient amounts of labeled data for training. Second, they lack the generalization ability on different detection tasks. In this paper, we propose a meta-learning framework for text steganalysis in the few-shot scenario to ensure model fast-adaptation between tasks. A general feature extractor based on BERT is applied to extract universal features among tasks, and a meta-learner based on attentional Bi-LSTM is employed to learn task-specific representations. A classifier trained on the support set calculates the prediction loss on the query set with a few samples to update the meta-learner. Extensive experiments show that our model can adapt fast among different steganalysis tasks through extremely few-shot samples, significantly improving detection performance compared with the state-of-the-art steganalysis models and other meta-learning methods.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "文本隐写分析是一种通过统计特征区分隐写文本和普通文本的技术。当前最先进的文本隐写分析模型有两个局限性。首先，他们需要足够数量的标记数据进行训练。第二，它们缺乏对不同探测任务的泛化能力。在本文中，我们提出了一个元学习框架，用于少镜头场景下的文本隐写分析，以确保任务之间的模型快速适应。基于BERT的通用特征提取器用于提取任务间的通用特征，基于注意双LSTM的元学习器用于学习特定任务的表征。在支持集上训练的分类器用几个样本计算查询集上的预测损失，以更新元学习器。大量实验表明，我们的模型可以通过极少的镜头样本快速适应不同的隐写分析任务，与最先进的隐写分析模型和其他元学习方法相比，显著提高了检测性能。",
                    "title_zh": "基于注意力元学习器的少镜头文本隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532965",
                    "title": "Hidden in Plain Sight - Persistent Alternative Mass Storage Data Streams as a Means for Data Hiding With the Help of UEFI NVRAM and Implications for IT Forensics",
                    "authors": "Stefan Kiltz, Robert Altschaffel, Jana Dittmann",
                    "abstract": "This article presents a first study on the possibility of hiding data using the UEFI NVRAM of today's computer systems as a storage channel. Embedding and extraction of executable data as well as media data are discussed and demonstrated as a proof of concept. This is successfully evaluated using 10 different systems. This paper further explores the implications of data hiding within UEFI NVRAM for computer forensic investigations and provides forensics measures to address this new challenge.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3531536.3532965"
                    },
                    "abstract_zh": "本文首次研究了使用当今计算机系统的UEFI NVRAM作为存储通道来隐藏数据的可能性。可执行数据以及媒体数据的嵌入和提取将作为概念验证进行讨论和演示。使用10个不同的系统对此进行了成功评估。本文进一步探讨了UEFI NVRAM中的数据隐藏对计算机取证调查的影响，并提供了应对这一新挑战的取证措施。",
                    "title_zh": "隐藏在众目睽睽之下——借助UEFI NVRAM作为数据隐藏手段的持久替代大容量存储数据流及其对IT取证的影响"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532955",
                    "title": "Fighting the Reverse JPEG Compatibility Attack: Pick your Side",
                    "authors": "Jan Butora, Patrick Bas",
                    "abstract": "In this work we aim to design a steganographic scheme undetectable by the Reverse JPEG Compatibility Attack (RJCA). The RJCA, while only effective for JPEG images compressed with quality factors 99 and 100, was shown to work mainly due to change in variance of the rounding errors after decompression of the DCT coefficients, which is induced by embedding changes incompatible with the JPEG format. One remedy to preserve the aforementioned format is utilizing during the embedding the rounding errors created during the JPEG compression, but no steganographic method is known to be resilient to RJCA without this knowledge. Inspecting the effect of embedding changes on variance and also mean of decompression rounding errors, we propose a steganographic method allowing resistance against RJCA without any side-information. To resist RJCA, we propose a distortion metric making all embedding changes within a DCT block dependent, resulting in a lattice-based embedding. Then it turns out it is enough to cleverly pick the side of the (binary) embedding changes through inspection of their effect on the variance of decompression rounding errors and simply use uniform costs in order to enforce their sparsity across DCT blocks. To increase security against detectors in the spatial (pixel) domain, we show an easy way of combining the proposed methodology with steganography designed for spatial domain security, further improving the undetectability for quality factor 99. The improvements over existing non-informed steganography are up to 40% in terms of detector's accuracy.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-03662307/file/counter_RJCA_04.pdf"
                    },
                    "abstract_zh": "在这项工作中，我们的目标是设计一个隐写方案不可检测的反向JPEG兼容性攻击(RJCA)。虽然RJCA仅对用质量因子99和100压缩的JPEG图像有效，但它主要是由于DCT系数解压缩后舍入误差方差的变化而工作的，这是由嵌入与JPEG格式不兼容的变化引起的。保存前述格式的一种补救方法是在嵌入期间利用在JPEG压缩期间产生的舍入误差，但是没有已知的隐写方法在没有这种知识的情况下对RJCA具有弹性。考察嵌入变化对方差和解压缩舍入误差均值的影响，我们提出了一种在没有任何辅助信息的情况下抵抗RJCA的隐写方法。为了抵抗RJCA，我们提出了一种失真度量，使得DCT块内的所有嵌入变化都是相关的，从而导致基于网格的嵌入。然后，通过检查(二进制)嵌入变化对解压缩舍入误差的方差的影响，巧妙地挑选嵌入变化的一边，并简单地使用统一成本，以便在DCT块之间加强其稀疏性，这就足够了。为了在空间(像素)域中提高对检测器的安全性，我们展示了一种将所提出的方法与为空间域安全性设计的隐写术相结合的简单方法，进一步提高了品质因子99的不可检测性。在检测器的准确性方面，比现有的无信息隐写术提高了40%。",
                    "title_zh": "对抗反向JPEG兼容性攻击:选择你的立场"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532960",
                    "title": "A Nearest Neighbor Under-sampling Strategy for Vertical Federated Learning in Financial Domain",
                    "authors": "Denghao Li, Jianzong Wang, Lingwei Kong, Shijing Si, Zhangcheng Huang, Chenyu Huang, Jing Xiao",
                    "abstract": "Machine learning techniques have been widely applied in modern financial activities. Participants in the field are aware of the importance of data privacy. Vertical federated learning (VFL) was proposed as a solution to multi-party secure computation for machine learning to obtain the huge data required by the models as well as keep the privacy of the data holders. However, previous research majorly analyzed the algorithms under ideal conditions. Data imbalance in VFL is still an open problem. In this paper, we propose a privacy-preserving sampling strategy for imbalanced VFL based on federated graph embedding of the samples, without leaking any distribution information. The participants of the federation provide partial neighbor information for each sample during the intersection stage and the controversial negative sample will be filtered out. Experiments were conducted on commonly used financial datasets and one real-world dataset. Our proposed approach obtained the leading F1 score on all tested datasets on comparing with the baseline under sampling strategies for VFL.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "机器学习技术已经广泛应用于现代金融活动中。该领域的参与者都意识到数据隐私的重要性。垂直联邦学习(Vertical federated learning，VFL)是一种针对机器学习的多方安全计算解决方案，用于获取模型所需的海量数据，同时保护数据持有者的隐私。然而，以前的研究主要是在理想条件下分析算法。VFL的数据不平衡仍然是一个公开的问题。本文提出了一种基于样本联邦图嵌入的非平衡VFL隐私保护采样策略，不泄漏任何分布信息。在相交阶段，联邦的参与者为每个样本提供部分邻居信息，有争议的负样本将被过滤掉。实验在常用的金融数据集和一个真实数据集上进行。我们提出的方法在所有测试数据集上获得领先的F1分数，与VFL采样策略下的基线进行比较。",
                    "title_zh": "金融领域垂直联邦学习的最近邻欠采样策略"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532952",
                    "title": "Colmade: Collaborative Masking in Auditable Decryption for BFV-based Homomorphic Encryption",
                    "authors": "Alberto Ibarrondo, Hervé Chabanne, Vincent Despiegel, Melek Önen",
                    "abstract": "This paper proposes a novel collaborative decryption protocol for the Brakerski-Fan-Vercauteren (BFV) homomorphic encryption scheme in a multiparty distributed setting, and puts it to use in designing a leakage-resilient biometric identification solution. Allowing the computation of standard homomorphic operations over encrypted data, our protocol reveals only one least significant bit (LSB) of a scalar/vectorized result resorting to a pool of N parties. By employing additively shared masking, our solution preserves the privacy of all the remaining bits in the result as long as one party remains honest. We formalize the protocol, prove it secure in several adversarial models, implement it on top of the open-source library Lattigo and showcase its applicability as part of a biometric access control scenario.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "本文提出了一种新的协作解密协议，用于多方分布式环境下的布拉克斯基-范-维考特伦(BFV)同态加密方案，并将其用于设计一个抗泄漏的生物识别解决方案。允许对加密数据进行标准同态运算的计算，我们的协议只揭示了求助于N方池的标量/矢量化结果的一个最低有效位(LSB)。通过使用附加共享屏蔽，只要一方保持诚实，我们的解决方案就能保护结果中所有剩余位的隐私。我们形式化了该协议，证明了它在几种对抗模型中的安全性，在开源库Lattigo上实现了它，并展示了它作为生物识别访问控制场景的一部分的适用性。",
                    "title_zh": "Colmade:基于BFV同态加密的可审计解密中的协同掩蔽"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532958",
                    "title": "AMR Steganalysis based on Adversarial Bi-GRU and Data Distillation",
                    "authors": "Zhijun Wu, Junjun Guo",
                    "abstract": "Existing AMR (Adaptive Multi-Rate) steganalysis algorithms based on pitch delay have low detection accuracy on samples with short time or low embedding rate, and the model shows fragility under the attack of adversarial samples. To solve this problem, we design an advanced AMR steganalysis method based on adversarial Bi-GRU (Bi-directional Gated Recurrent Unit) and data distillation. First, Gaussian white noise is randomly added to part of the original speech to form adversarial data set, then artificially annotate a small amount of voice to train the model. Second, perform three transformations of 1.5 times speed, 0.5 times speed, and mirror flip on the remaining original voice data, then put them into Bi-GRU for classification, and the final predicted label obtained by the decision fusion corresponds to the original data. All data with the label is put back into the Bi-GRU model for final training at last. What needs to be pointed out is that each batch of final training data includes normal and adversarial samples. This method adopts a semi-supervised learning method, which greatly saves the resources consumed by manual labeling, and introduces adversarial Bi-GRU, which can realize the two-direction analysis of samples for a long time. Based on improving the detection accuracy, the safety and robustness of the model are greatly improved. The experimental results show that for normal and adversarial samples, the algorithm can achieve accuracy of 96.73% and 95.6% respectively.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3531536.3532958"
                    },
                    "abstract_zh": "现有基于基音延迟的AMR隐写分析算法对时间短或嵌入率低的样本检测准确率较低，且模型在敌对样本攻击下表现出脆弱性。针对这一问题，设计了一种基于双向门控递归单元和数据提取的AMR隐写分析方法。首先在部分原始语音中随机加入高斯白噪声形成对抗性数据集，然后人工标注少量语音来训练模型。第二，对剩余的原始语音数据进行1.5倍速、0.5倍速、镜像翻转三种变换，然后放入双GRU进行分类，决策融合得到的最终预测标签对应原始数据。最后将所有带标签的数据放回双GRU模型进行最终训练。需要指出的是，每批最终训练数据都包含正态样本和敌对样本。该方法采用了半监督学习的方法，大大节省了人工标注所消耗的资源，并引入了对抗性双GRU，可以实现长时间的样本双向分析。在提高检测精度的基础上，大大提高了模型的安全性和鲁棒性。实验结果表明，对于正常样本和敌对样本，该算法的准确率分别达到96.73%和95.6%。",
                    "title_zh": "基于对抗双GRU和数据提取的AMR隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532948",
                    "title": "Capacity Laws for Steganography in a Crowd",
                    "authors": "Andrew D. Ker",
                    "abstract": "A steganographer is not only hiding a payload inside their cover, they are also hiding themselves amongst the non-steganographers. In this paper we study asymptotic rates of growth for steganographic data -- analogous to the classical Square-Root Law -- in the context of a 'crowd' of K actors, one of whom is a steganographer. This converts steganalysis from a binary to a K-class classification problem, and requires some new information-theoretic tools. Intuition suggests that larger K should enable the steganographer to hide a larger payload, since their stego signal is mixed in with larger amounts of cover noise from the other actors. We show that this is indeed the case, in a simple independent-pixel model, with payload growing at O(√(log K)) times the classical Square-Root capacity in the case of homogeneous actors. Further, examining the effects of heterogeneity reveals a subtle dependence on the detector's knowledge about the payload size, and the need for them to use negative as well as positive information to identify the steganographer.",
                    "files": {
                        "openAccessPdf": "https://ora.ox.ac.uk/objects/uuid:b6c1bfa0-33d6-4885-9c08-988fa68cbecd/download_file?safe_filename=Ker_2022_Capacity_laws_for.pdf&file_format=pdf&type_of_work=Conference+item"
                    },
                    "abstract_zh": "隐写术者不仅将有效载荷隐藏在他们的封面内，他们还将自己隐藏在非隐写术者中间。在本文中，我们研究了隐写数据的渐近增长率——类似于经典的平方根定律——在K个参与者的“人群”中，其中一个是隐写员。这将隐写分析从二进制问题转化为K类分类问题，并需要一些新的信息论工具。直觉表明，较大的K应该使隐写术者能够隐藏更大的有效载荷，因为他们的隐写信号与来自其他参与者的大量覆盖噪声混合在一起。我们证明了这一点，在一个简单的独立像素模型中，在同质因素的情况下，有效负载以O(√(log K))倍于经典平方根容量的速度增长。此外，检查异质性的影响揭示了对检测器关于有效载荷大小的知识的微妙依赖，以及检测器使用正面和负面信息来识别隐写者的需要。",
                    "title_zh": "人群中隐写术的容量定律"
                },
                {
                    "url": "https://doi.org/10.1145/3531536.3532951",
                    "title": "Detector-Informed Batch Steganography and Pooled Steganalysis",
                    "authors": "Yassine Yousfi, Eli Dworetzky, Jessica J. Fridrich",
                    "abstract": "We study the problem of batch steganography when the senders use feedback from a steganography detector. This brings an additional level of complexity to the table due to the highly non-linear and non-Gaussian response of modern steganalysis detectors as well as the necessity to study the impact of the inevitable mismatch between senders' and Warden's detectors. Two payload spreaders are considered based on the oracle generating possible cover images. Three different pooling strategies are devised and studied for a more comprehensive assessment of security. Substantial security gains are observed with respect to previous art - the detector-agnostic image-merging sender. Close attention is paid to the impact of the information available to the Warden on security.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "我们研究当发送者使用来自隐写检测器的反馈时的批量隐写问题。由于现代隐写分析检测器的高度非线性和非高斯响应，以及研究发送者和监管者检测器之间不可避免的不匹配的影响的必要性，这给表格带来了额外的复杂性。基于oracle生成可能的封面图像，考虑两个有效载荷扩展器。为了更全面地评估安全性，设计并研究了三种不同的池策略。相对于现有技术，检测器不可知的图像合并发送器，观察到了显著的安全增益。密切关注典狱长可获得的信息对安全的影响。",
                    "title_zh": "检测器通知的批量隐写术和混合隐写分析"
                }
            ]
        }
    ],
    "2021": [
        {
            "dblp_url": "https://dblp.uni-trier.de/db/conf/ih/ihmmsec2021.html",
            "conf_title": "9th IH&MMSec 2021: Virtual Event, Belgium",
            "conf_url": "https://doi.org/10.1145/3437880",
            "papers": [
                {
                    "url": "https://doi.org/10.1145/3437880.3458441",
                    "title": "Evaluating and Designing against Side-Channel Leakage: White Box or Black Box?",
                    "authors": "François-Xavier Standaert",
                    "abstract": "Side-channel analysis is an important concern for the security of cryptographic implementations, and may lead to powerful key recovery attacks if no countermeasures are deployed. Therefore, various types of protection mechanisms have been proposed over the last 20 years. In view of the cost and performance overheads caused by these protections, their fair evaluation and scarce use are a primary concern for hardware and software designers. Yet, the physical nature of side-channel analysis also renders the security evaluation of cryptographic implementations very different from the one of cryptographic algorithms against mathematical cryptanalysis. That is, while the latter can be quantified based on (well-defined) time, data and memory complexities, the evaluation of side-channel security additionally requires to quantify the informativeness and exploitability of the physical leakages. This implies that a part of these security evaluations is inherently heuristic and dependent on engineering expertise. It also raises the question of the capabilities given to the adversary/evaluator. For example, should she get full (unrestricted) access to the implementation to gain a precise understanding of its functioning (which I will denote as the white box approach) or should she be more restricted? In this talk, I will argue that a white box approach is not only desirable in order to avoid designing and evaluating implementations with a \"false sense of security\" but also that such designs become feasible in view of the research progresses made over the last two decades.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "旁路分析是加密实现安全性的一个重要考虑因素，如果不采取对策，可能会导致强大的密钥恢复攻击。因此，在过去的20年中，已经提出了各种类型的保护机制。考虑到由这些保护引起的成本和性能开销，它们的公平评估和稀少使用是硬件和软件设计者主要关心的问题。然而，旁道分析的物理本质也使得密码实现的安全性评估与针对数学密码分析的密码算法的安全性评估非常不同。也就是说，虽然后者可以基于(明确定义的)时间、数据和存储复杂性来量化，但侧信道安全性的评估还需要量化物理泄漏的信息含量和可利用性。这意味着这些安全评估的一部分是固有的启发式的，并依赖于工程专业知识。这也提出了赋予对手/评估者能力的问题。例如，她应该获得对实现的完全(无限制的)访问权以获得对其功能的精确理解(我称之为白盒方法)还是应该受到更多的限制？在这次演讲中，我将论证白盒方法不仅是可取的，以避免设计和评估带有“虚假安全感”的实现，而且鉴于过去二十年的研究进展，这种设计变得可行。",
                    "title_zh": "抗侧信道泄漏的评估和设计:白盒还是黑盒？"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3458440",
                    "title": "How Private is Machine Learning?",
                    "authors": "Nicholas Carlini",
                    "abstract": "A machine learning model is private if it doesn't reveal (too much) about its training data. This three-part talk examines to what extent current networks are private. Standard models are not private. We develop an attack that extracts rare training examples (for example, individual people's names, phone numbers, or addresses) out of GPT-2, a language model trained on gigabytes of text from the Internet [2]. As a result there is a clear need for training models with privacy-preserving techniques. We show that InstaHide, a recent candidate, is not private. We develop a complete break of this scheme and can again recover verbatim inputs [1]. Fortunately, there exists provably-correct \"differentiallyprivate\" training that guarantees no adversary could ever succeed at the above attacks. We develop techniques to that allow us to empirically evaluate the privacy offered by such schemes, and find they may be more private than can be proven formally [3].",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "如果机器学习模型没有透露(太多)关于它的训练数据，那么它就是私有的。这个由三部分组成的演讲考察了当前网络的私有程度。标准模型不是私有的。我们开发了一种攻击，从GPT-2(一种基于互联网上千兆字节文本训练的语言模型)中提取罕见的训练样本(例如，个人的姓名、电话号码或地址)[2]。因此，显然需要使用隐私保护技术的训练模型。我们表明，最近的候选人InstaHide不是私有的。我们开发了一个完全打破这种方案，并可以再次恢复逐字输入[1]。幸运的是，存在可证明是正确的“差异私人”训练，保证没有对手能够在上述攻击中成功。我们开发了一些技术，使我们能够凭经验评估这种方案提供的隐私，并发现它们可能比正式证明的更加隐私[3]。",
                    "title_zh": "机器学习有多私密？"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3458442",
                    "title": "Tracing Data through Learning with Watermarking",
                    "authors": "Alexandre Sablayrolles",
                    "abstract": "How can we gauge the privacy provided by machine learning algorithms? Models trained with differential privacy (DP) provably limit information leakage, but the question remains open for non-DP models. In this talk, we present multiple techniques for membership inference, which estimates if a given data sample is in the training set of a model. In particular, we introduce a watermarking-based method that allows for a very fast verification of data usage in a model: this technique creates marks called radioactive that propagates from the data to the model during training. This watermark is barely visible to the naked eye and allows data tracing even when the radioactive data represents only 1% of the training set.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "我们如何衡量机器学习算法提供的隐私？用差分隐私(DP)训练的模型可以证明限制了信息泄漏，但这个问题对于非DP模型仍然是开放的。在本次演讲中，我们将介绍多种隶属度推断技术，该技术用于评估给定的数据样本是否在模型的训练集中。特别是，我们引入了一种基于水印的方法，该方法允许非常快速地验证模型中的数据使用:这种技术创建了称为放射性的标记，该标记在训练期间从数据传播到模型。这种水印肉眼几乎看不见，即使放射性数据仅代表训练集的1%,也可以进行数据追踪。",
                    "title_zh": "使用水印通过学习跟踪数据"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460400",
                    "title": "PRNU-based Deepfake Detection",
                    "authors": "Florian Lugstein, Simon Baier, Gregor Bachinger, Andreas Uhl",
                    "abstract": "As deepfakes become harder to detect by humans, more reliable detection methods are required to fight the spread of fake images and videos. In our work, we focus on PRNU-based detection methods, which, while popular in the image forensics scene, have not been given much attention in the context of deepfake detection. We adopt a PRNU-based approach originally developed for the detection of face morphs and facial retouching, and performed the first large scale test of PRNU-based deepfake detection methods on a variety of standard datasets. We show the impact of often neglected parameters of the face extraction stage on detection accuracy. We also document that existing PRNU-based methods cannot compete with state of the art methods based on deep learning but may be used to complement those in hybrid detection schemes.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "随着deepfakes越来越难以被人类检测到，需要更可靠的检测方法来打击虚假图像和视频的传播。在我们的工作中，我们重点研究了基于PRNU的检测方法，这些方法虽然在图像取证领域很流行，但在deepfake检测方面却没有得到太多的关注。我们采用了最初为检测面部变形和面部修饰而开发的基于PRNU的方法，并在各种标准数据集上对基于PRNU的deepfake检测方法进行了首次大规模测试。我们显示了人脸提取阶段经常被忽略的参数对检测精度的影响。我们还证明了现有的基于PRNU的方法无法与基于深度学习的最先进方法竞争，但可以用于补充混合检测方案中的方法。",
                    "title_zh": "基于PRNU的Deepfake检测"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460408",
                    "title": "Fake Speech Detection Using Residual Network with Transformer Encoder",
                    "authors": "Zhenyu Zhang, Xiaowei Yi, Xianfeng Zhao",
                    "abstract": "Fake speech detection aims to distinguish fake speech from natural speech. This paper presents an effective fake speech detection scheme based on residual network with transformer encoder (TE-ResNet) for improving the performance of fake speech detection. Firstly, considering inter-frame correlation of the speech signal, we utilize transformer encoder to extract contextual representations of the acoustic features. Then, a residual network is used to process deep features and calculate score that the speech is fake. Besides, to increase the quantity of training data, we apply five speech data augmentation techniques on the training dataset. Finally, we fuse the different fake speech detection models on score-level by logistic regression for compensating the shortcomings of each single model. The proposed scheme is evaluated on two public speech datasets. Our experiments demonstrate that the proposed TE-ResNet outperforms the existing state-of-the-art methods both on development and evaluation datasets. In addition, the proposed fused model achieves improved performance for detection of unseen fake speech technology, which can obtain equal error rates (EERs) of 3.99% and 5.89% on evaluation set of FoR-normal dataset and ASVspoof 2019 LA dataset respectively.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3437880.3460408"
                    },
                    "abstract_zh": "伪语音检测旨在区分伪语音和自然语音。为了提高伪语音检测的性能，提出了一种有效的基于带变换编码器的残差网络的伪语音检测方案。首先，考虑到语音信号的帧间相关性，我们利用变换编码器提取声学特征的上下文表示。然后，使用残差网络处理深层特征并计算语音是假的得分。此外，为了增加训练数据的数量，我们在训练数据集上应用了五种语音数据增强技术。最后，通过逻辑回归将不同的伪语音检测模型在分数级进行融合，以弥补单个模型的不足。在两个公开的语音数据集上对该方案进行了评估。我们的实验表明，所提出的TE-ResNet在开发和评估数据集上都优于现有的最先进的方法。此外，所提出的融合模型实现了对未知伪语音技术的检测性能的改善，在FoR-normal数据集和ASVspoof 2019 LA数据集的评估集上分别获得了3.99%和5.89%的等错误率(EERs)。",
                    "title_zh": "基于残差网络和变换编码器的伪语音检测"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460412",
                    "title": "Meta and Media Data Stream Forensics in the Encrypted Domain of Video Conferences",
                    "authors": "Robert Altschaffel, Jonas Hielscher, Stefan Kiltz, Jana Dittmann",
                    "abstract": "Our paper presents a systematic approach to investigate whether and how events can be identified and extracted during the use of video conferencing software. Our approach is based on the encrypted meta and multimedia data exchanged during video conference sessions. It relies on the network data stream which contains data interpretable without decryption (plain data) and encrypted data (encrypted content) some of which is decrypted using our approach (decrypted content). This systematic approach uses a forensic process model and the fission of network data streams before applying methods on the specific individual data types. Our approach is applied exemplary to the Zoom Videoconferencing Service with Client Version 5.4.57862.0110 [4], the mobile Android App Client Version 5.5.2 (1328) [4], the webbased client and the servers (accessed between Jan 21st and Feb 4th). The investigation includes over 50 different configurations. For the heuristic speaker identification, two series of nine sets for eight different speakers are collected. The results show that various user data can be derived from characteristics of encrypted media streams, even if end-to-end encryption is used. The findings suggest user privacy risks. Our approach offers the identification of various events, which enable activity tracking (e.g. camera on/off, increased activity in front of camera) by evaluating heuristic features of the network streams. Further research into user identification within the encrypted audio stream based on pattern recognition using heuristic features of the corresponding network data stream is conducted and suggests the possibility to identify users within a specific set.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3437880.3460412"
                    },
                    "abstract_zh": "我们的论文提出了一个系统的方法来研究在视频会议软件的使用过程中，事件是否以及如何被识别和提取。我们的方法基于视频会议期间交换的加密元数据和多媒体数据。它依赖于网络数据流，该数据流包含无需解密即可解释的数据(普通数据)和加密数据(加密内容)，其中一些数据使用我们的方法解密(解密内容)。这种系统方法在对特定的单个数据类型应用方法之前，使用取证过程模型和网络数据流的分裂。我们的方法示例性地应用于Zoom视频会议服务，客户端版本为5.4.57862.0110 [4]，移动Android应用客户端版本为5.5.2 (1328) [4]，基于网络的客户端和服务器(在1月21日和2月4日之间访问)。调查包括50多种不同的配置。对于启发式说话人识别，收集八个不同说话人的两个系列的九个集合。结果表明，即使使用端到端加密，也可以从加密媒体流的特征中导出各种用户数据。研究结果表明用户隐私存在风险。我们的方法提供了对各种事件的识别，这使得能够通过评估网络流的启发式特征来进行活动跟踪(例如，摄像机开/关、摄像机前的活动增加)。基于使用相应网络数据流的启发式特征的模式识别，对加密音频流内的用户识别进行进一步研究，并提出在特定集合内识别用户的可能性。",
                    "title_zh": "视频会议加密领域的元数据流和媒体数据流取证"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460405",
                    "title": "Exploitation and Sanitization of Hidden Data in PDF Files: Do Security Agencies Sanitize Their PDF Files?",
                    "authors": "Supriya Adhatarao, Cédric Lauradoux",
                    "abstract": "Organizations publish and share more and more electronic documents like PDF files. Unfortunately, most organizations are unaware that these documents can compromise sensitive information like authors names, details on the information system and architecture. All these information can be exploited easily by attackers to footprint and later attack an organization. In this paper, we analyze hidden data found in the PDF files published by an organization. We gathered a corpus of 39664 PDF files published by 75 security agencies from 47 countries. We have been able to measure the quality and quantity of information exposed in these PDF files. It can be effectively used to find weak links in an organization: the employees who are running outdated software. We have also measured the adoption of PDF files sanitization by security agencies. We identified only 7 security agencies which sanitize few of their PDF files before publishing. Unfortunately, we were still able to find sensitive information within 65% of these sanitized PDF files. Some agencies are using weak sanitization techniques: it requires to remove all the hidden sensitive information from the file and not just to remove the data at the surface. Security agencies need to change their sanitization methods.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "组织发布和共享越来越多的电子文档，如PDF文件。不幸的是，大多数组织都没有意识到这些文档会危及敏感信息，如作者姓名、信息系统和架构的细节。攻击者可以很容易地利用所有这些信息来跟踪并随后攻击组织。在本文中，我们分析了在一个组织发布的PDF文件中发现的隐藏数据。我们收集了来自47个国家的75个安全机构发布的39664个PDF文件。我们已经能够测量这些PDF文件中暴露的信息的质量和数量。它可以有效地用于发现组织中的薄弱环节:运行过时软件的员工。我们还测量了安全机构对PDF文件进行清理的情况。我们发现只有7家安全机构在发布前对他们的PDF文件进行了消毒。不幸的是，我们仍然能够在这些经过清理的PDF文件中找到65%的敏感信息。一些机构正在使用弱净化技术:它需要从文件中删除所有隐藏的敏感信息，而不仅仅是删除表面的数据。安全机构需要改变他们的消毒方法。",
                    "title_zh": "PDF文件中隐藏数据的利用和清理:安全机构清理他们的PDF文件吗？"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460414",
                    "title": "Angular Margin Softmax Loss and Its Variants for Double Compressed AMR Audio Detection",
                    "authors": "Aykut Büker, Cemal Hanilçi",
                    "abstract": "Double compressed (DC) adaptive multi-rate (AMR) audio detection is an important but challenging audio forensic task which has received great attention over the last decade. Although the majority of the existing studies extract hand-crafted features and classify these features using traditional pattern matching algorithms such as support vector machines (SVM), recently convolutional neural network (CNN) based DC AMR audio detection system was proposed which yields very promising detection performance. Similar to any traditional CNN based classification system, CNN based DC AMR recognition system uses standard softmax loss as the training criterion. In this paper, we propose to use angular margin softmax loss and its variants for DC AMR detection problem. Although using angular margin softmax was originally proposed for face recognition, we adapt it to the CNN based end-to-end DC audio detection system. The angular margin softmax basically introduces a margin between two classes so that the system can learn more discriminative embeddings for the problem. Experimental results show that adding angular margin penalty to the traditional softmax loss increases the average DC AMR audio detection from 95.83% to 100%. It is also found that the angular margin softmax loss functions boost the DC AMR audio detection performance when there is a mismatch between training and test datasets.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "双压缩(DC)自适应多速率(AMR)音频检测是一项重要但具有挑战性的音频取证任务，在过去十年中受到了极大的关注。虽然大多数现有的研究提取手工制作的特征，并使用传统的模式匹配算法如支持向量机(SVM)对这些特征进行分类，但最近提出了基于卷积神经网络(CNN)的DC AMR音频检测系统，该系统产生了非常有前途的检测性能。类似于任何传统的基于CNN的分类系统，基于CNN的DC AMR识别系统使用标准softmax损失作为训练标准。在本文中，我们建议使用角裕量softmax损失及其变体来解决DC AMR检测问题。虽然使用角度裕量softmax最初是为人脸识别而提出的，但我们将其应用于基于CNN的端到端DC音频检测系统。角度裕度softmax基本上在两个类之间引入了一个裕度，以便系统可以学习更多有区别的问题嵌入。实验结果表明，在传统的softmax损失上增加角度裕度损失，将平均DC AMR音频检测从95.83%提高到100%。还发现，当训练和测试数据集之间存在失配时，角裕度softmax损失函数提升了DC AMR音频检测性能。",
                    "title_zh": "双重压缩AMR音频检测的角度裕量软最大损失及其变体"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460403",
                    "title": "FederatedReverse: A Detection and Defense Method Against Backdoor Attacks in Federated Learning",
                    "authors": "Chen Zhao, Yu Wen, Shuailou Li, Fucheng Liu, Dan Meng",
                    "abstract": "Federated learning is a secure machine learning technology proposed to protect data privacy and security in machine learning model training. However, recent studies show that federated learning is vulnerable to backdoor attacks, such as model replacement attacks and distributed backdoor attacks. Most backdoor defense techniques are not appropriate for federated learning since they are based on entire data samples that cannot be hold in federated learning scenarios. The newly proposed methods for federated learning sacrifice the accuracy of models and still fail once attacks persist in many training rounds. In this paper, we propose a novel and effective detection and defense technique called FederatedReverse for federated learning. We conduct extensive experimental evaluation of our solution. The experimental results show that, compared with the existing techniques, our solution can effectively detect and defend against various backdoor attacks in federated learning, where the success rate and duration of backdoor attacks can be greatly reduced and the accuracies of trained models are almost not reduced.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "联合学习是一种安全的机器学习技术，旨在保护机器学习模型训练中的数据隐私和安全。然而，最近的研究表明，联邦学习容易受到后门攻击，如模型替换攻击和分布式后门攻击。大多数后门防御技术都不适合联合学习，因为它们基于完整的数据样本，而这些样本在联合学习场景中是无法保存的。新提出的联合学习方法牺牲了模型的准确性，并且一旦攻击在许多训练回合中持续，仍然会失败。在本文中，我们提出了一种新的有效的检测和防御技术，称为FederatedReverse。我们对我们的解决方案进行了广泛的实验评估。实验结果表明，与现有技术相比，该方案能有效检测和防御联邦学习中的各种后门攻击，后门攻击的成功率和持续时间大大降低，而训练模型的准确率几乎没有降低。",
                    "title_zh": "FederatedReverse:一种针对联邦学习中后门攻击的检测和防御方法"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460394",
                    "title": "Banners: Binarized Neural Networks with Replicated Secret Sharing",
                    "authors": "Alberto Ibarrondo, Hervé Chabanne, Melek Önen",
                    "abstract": "Binarized Neural Networks (BNN) provide efficient implementations of Convolutional Neural Networks (CNN). This makes them particularly suitable to perform fast and memory-light inference of neural networks running on resource-constrained devices. Motivated by the growing interest in CNN-based biometric recognition on potentially insecure devices, or as part of strong multi-factor authentication for sensitive applications, the protection of BNN inference on edge devices is rendered imperative. We propose a new method to perform secure inference of BNN relying on secure multiparty computation. While preceding papers offered security in a semi-honest setting for BNN or malicious security for standard CNN, our work yields security with abort against one malicious adversary for BNN by leveraging on Replicated Secret Sharing (RSS) for an honest majority with three computing parties. Experimentally, we implement Banners on top of MP-SPDZ and compare it with prior work over binarized models trained for MNIST and CIFAR10 image classification datasets. Our results attest the efficiency of Banners as a privacy-preserving inference technique.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "二进制神经网络(BNN)提供了卷积神经网络(CNN)的有效实现。这使得它们特别适合于在资源受限的设备上运行的神经网络的快速和内存较少的推断。出于对潜在不安全设备上基于CNN的生物识别日益增长的兴趣，或者作为敏感应用的强大多因素身份验证的一部分，保护边缘设备上的BNN推断势在必行。我们提出了一种基于安全多方计算实现BNN安全推理的新方法。虽然之前的论文为BNN提供了半诚实设置中的安全性，或者为标准CNN提供了恶意安全性，但是我们的工作通过利用复制秘密共享(RSS)为三方计算的诚实多数提供了针对BNN的一个恶意对手的abort安全性。实验上，我们在MP-SPDZ上实现了横幅，并将其与针对MNIST和CIFAR10图像分类数据集训练的二值化模型的先前工作进行了比较。我们的结果证明了横幅作为隐私保护推理技术的有效性。",
                    "title_zh": "横幅:复制秘密共享的二值化神经网络"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460415",
                    "title": "Deep Neural Exposure: You Can Run, But Not Hide Your Neural Network Architecture!",
                    "authors": "Sayed Erfan Arefin, Abdul Serwadda",
                    "abstract": "Deep Neural Networks (DNNs) are at the heart of many of today's most innovative technologies. With companies investing lots of resources to design, build and optimize these networks for their custom products, DNNs are now integral to many companies' tightly guarded Intellectual Property. As is the case for every high-value product, one can expect bad actors to increasingly design techniques aimed to uncover the architectural designs of proprietary DNNs. This paper investigates if the power draw patterns of a GPU on which a DNN runs could be leveraged to glean key details of its design architecture. Based on ten of the most well-known Convolutional Neural Network (CNN) architectures, we study this line of attack under varying assumptions about the kind of data available to the attacker. We show the attack to be highly effective, attaining an accuracy in the 80 percentage range for the best performing attack scenario.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "深度神经网络(DNNs)是当今许多最具创新性技术的核心。随着公司投入大量资源为其定制产品设计、构建和优化这些网络，dnn现已成为许多公司严格保护的知识产权的一部分。正如每个高价值产品的情况一样，可以预期坏人会越来越多地设计旨在揭露专有dnn架构设计的技术。本文研究运行DNN的GPU的功耗模式是否可以用来收集其设计架构的关键细节。基于十个最著名的卷积神经网络(CNN)架构，我们在对攻击者可用数据种类的不同假设下研究了这种攻击方式。我们显示这种攻击非常有效，在最佳攻击情况下，准确率达到了80%。",
                    "title_zh": "深度神经暴露:你可以跑，但不能隐藏你的神经网络架构！"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460411",
                    "title": "iNNformant: Boundary Samples as Telltale Watermarks",
                    "authors": "Alexander Schlögl, Tobias Kupek, Rainer Böhme",
                    "abstract": "Boundary samples are special inputs to artificial neural networks crafted to identify the execution environment used for inference by the resulting output label. The paper presents and evaluates algorithms to generate transparent boundary samples. Transparency refers to a small perceptual distortion of the host signal (i.e., a natural input sample). For two established image classifiers, ResNet on FMNIST and CIFAR10, we show that it is possible to generate sets of boundary samples which can identify any of four tested microarchitectures. These sets can be built to not contain any sample with a worse peak signal-to-noise ratio than 70dB. We analyze the relationship between search complexity and resulting transparency.",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/2106.07303"
                    },
                    "abstract_zh": "边界样本是人工神经网络的特殊输入，人工神经网络被精心制作以通过结果输出标签来识别用于推断的执行环境。本文提出并评估了生成透明边界样本的算法。透明度是指宿主信号(即，自然输入样本)的微小感知失真。对于两个已建立的图像分类器，FMNIST上的ResNet和CIFAR10，我们表明有可能生成能够识别四个测试微体系结构中任何一个的边界样本集。这些集合可以构建为不包含任何峰值信噪比低于70dB的样本。我们分析了搜索复杂度和结果透明度之间的关系。",
                    "title_zh": "不一致性:作为指示水印的边界样本"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460406",
                    "title": "Towards Match-on-Card Finger Vein Recognition",
                    "authors": "Michael Linortner, Andreas Uhl",
                    "abstract": "Security and privacy is of great interest in biometric systems which can be offered by Match-on-Card (MoC) technology, successfully applied in several areas of biometrics. In finger vein recognition such a system is not available yet. Utilizing minutiae points from vein images in combination with classical minutiae-based fingerprint comparison software offers a great opportunity to integrate vein recognition on MoC systems. In this work a publicly available and two commercial fingerprint comparison tools are used to evaluate the recognition performance of vein minutiae, represented in a standardized data format, on three publicly available databases. The results strongly indicate that minutiae-based comparison technology from fingerprint recognition can be applied to finger vein recognition and is able to compete with and even outperform classical correlation-based methods utilized in this field. The work done here prepares the way for vein recognition on MoC systems.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3437880.3460406"
                    },
                    "abstract_zh": "卡上匹配(MoC)技术可以提供生物特征识别系统的安全性和隐私性，并成功应用于生物特征识别的多个领域。在手指静脉识别中，这样的系统还不可用。利用静脉图像的细节点结合经典的基于细节点的指纹比较软件，为在MoC系统上集成静脉识别提供了一个很好的机会。在这项工作中，一个公开可用的和两个商业指纹比较工具被用于在三个公开可用的数据库上评估以标准化数据格式表示的静脉细节的识别性能。该结果强烈地表明，来自指纹识别的基于细节的比较技术可以应用于手指静脉识别，并且能够与在该领域中使用的经典的基于相关性的方法竞争甚至优于该方法。所做的工作为MoC系统上的静脉识别铺平了道路。",
                    "title_zh": "面向卡上匹配的手指静脉识别"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460410",
                    "title": "General Requirements on Synthetic Fingerprint Images for Biometric Authentication and Forensic Investigations",
                    "authors": "Andrey Makrushin, Christof Kauba, Simon Kirchgasser, Stefan Seidlitz, Christian Kraetzer, Andreas Uhl, Jana Dittmann",
                    "abstract": "Generation of synthetic biometric samples such as, for instance, fingerprint images gains more and more importance especially in view of recent cross-border regulations on security of private data. The reason is that biometric data is designated in recent regulations such as the EU GDPR as a special category of private data, making sharing datasets of biometric samples hardly possible even for research purposes. The usage of fingerprint images in forensic research faces the same challenge. The replacement of real datasets by synthetic datasets is the most advantageous straightforward solution which bears, however, the risk of generating \"unrealistic\" samples or \"unrealistic distributions\" of samples which may visually appear realistic. Despite numerous efforts to generate high-quality fingerprints, there is still no common agreement on how to define \"high-quality'' and how to validate that generated samples are realistic enough. Here, we propose general requirements on synthetic biometric samples (that are also applicable for fingerprint images used in forensic application scenarios) together with formal metrics to validate whether the requirements are fulfilled. Validation of our proposed requirements enables establishing the quality of a generative model (informed evaluation) or even the quality of a dataset of generated samples (blind evaluation). Moreover, we demonstrate in an example how our proposed evaluation concept can be applied to a comparison of real and synthetic datasets aiming at revealing if the synthetic samples exhibit significantly different properties as compared to real ones.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3437880.3460410"
                    },
                    "abstract_zh": "合成生物特征样本(例如指纹图像)的生成变得越来越重要，尤其是考虑到最近对私人数据安全性的跨境规定。原因在于，生物特征数据在最近的法规(如欧盟GDPR)中被指定为特殊类别的私有数据，这使得即使出于研究目的也几乎不可能共享生物特征样本数据集。在法医研究中使用指纹图像面临着同样的挑战。用合成数据集代替真实数据集是最有利的直接解决方案，然而，这种方案具有产生“不真实”样本或样本的“不真实分布”的风险，这些样本在视觉上看起来可能是真实的。尽管为生成高质量的指纹做了许多努力，但对于如何定义“高质量”以及如何验证生成的样本是否足够真实，仍然没有达成共识。在这里，我们提出了对合成生物特征样本的一般要求(也适用于法医应用场景中使用的指纹图像)以及验证这些要求是否得到满足的正式指标。对我们提出的需求进行验证，可以建立生成模型的质量(知情评估)，甚至是生成样本数据集的质量(盲评估)。此外，我们在一个示例中演示了我们提出的评估概念如何应用于真实数据集和合成数据集的比较，旨在揭示合成样本与真实样本相比是否表现出显著不同的属性。",
                    "title_zh": "生物认证和法庭调查用合成指纹图像的一般要求"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460407",
                    "title": "Optimizing Additive Approximations of Non-additive Distortion Functions",
                    "authors": "Solène Bernard, Patrick Bas, Tomás Pevný, John Klein",
                    "abstract": "The progress in steganography is hampered by a gap between non-additive distortion functions, which capture well complex dependencies in natural images, and their additive counterparts, which are efficient for data embedding. This paper proposes a theoretically justified method to approximate the former by the latter. The proposed method, called Backpack (for BACKPropagable AttaCK), combines new results in the approximation of gradients of discrete distributions with a gradient of implicit functions in order to derive a gradient w.r.t. the distortion of each JPEG coefficient. Backpack combined with the min max iterative protocol leads to a very secure steganographic algorithm. For example, the error rate of XuNet on 512 X 512 JPEG images, compressed with quality factor 100 and a payload of 0.4 bits per non-zero AC coefficient is 37.3% with Backpack, compared to a 26.5% error rate using ADV-EMB with minmax (considered state of the art in this work) and a 16.9% error rate with J-UNIWARD.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-03208143v1/file/main_tp2_forHAL.pdf"
                    },
                    "abstract_zh": "非加性失真函数可以很好地捕捉自然图像中复杂的相关性，而加性失真函数可以有效地嵌入数据，两者之间的差距阻碍了隐写术的发展。本文提出一种理论上合理的方法，用后者来近似前者。被称为Backpack(用于反向传播攻击)的所提出的方法将离散分布的梯度近似中的新结果与隐函数的梯度相结合，以便导出关于每个JPEG系数的失真的梯度。背包结合最小最大迭代协议产生了非常安全的隐写算法。例如，XuNet在512 X 512 JPEG图像上的错误率，以质量因子100和每个非零AC系数0.4位的有效载荷压缩，使用Backpack是37.3%，相比之下，使用ADV-EMB和minmax(在本工作中被认为是最先进的)的错误率是26.5%，使用J-UNIWARD的错误率是16.9%。",
                    "title_zh": "优化非加性失真函数的加性逼近"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460413",
                    "title": "Information Hiding in Cyber Physical Systems: Challenges for Embedding, Retrieval and Detection using Sensor Data of the SWAT Dataset",
                    "authors": "Kevin Lamshöft, Tom Neubert, Christian Krätzer, Claus Vielhauer, Jana Dittmann",
                    "abstract": "In this paper, we present an Information Hiding approach that would be suitable for exfiltrating sensible information of Industrial Control Systems (ICS) by leveraging the long-term storage of process data in historian databases. We show how hidden messages can be embedded in sensor measurements as well as retrieved asynchronously by accessing the historian. We evaluate this approach at the example of water-flow and water-level sensors of the Secure Water Treatment (SWAT) dataset from iTrust. To generalize from specific cover channels (sensors and their transmitted data), we reflect upon general challenges that arise in such Information Hiding scenarios creating network covert channels and discuss aspects of cover channel selection and and sender receiver synchronisation as well as temporal aspects such as the potential persistence of hidden messages in Cyber Physical Systems (CPS). For an empirical evaluation we design and implement a covert channel that makes use of different embedding strategies to perform an adaptive approach in regards to the noise in sensor measurements, resulting in dynamic capacity and bandwidth selection to reduce detection probability. The results of this evaluation show that, using such methods, the exfiltration of sensible information in long-term scaled attacks would indeed be possible. Additionally, we present two detection approaches for the introduced hidden channel and carry out an extensive evaluation of our detectors with multiple test data sets and different parameters. We determine a detection accuracy of up to 87.8% on test data at a false positive rate (FPR) of 0%.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3437880.3460413"
                    },
                    "abstract_zh": "在本文中，我们提出了一种信息隐藏方法，该方法通过利用历史数据库中过程数据的长期存储，适用于工业控制系统(ICS)的敏感信息的泄漏。我们展示了如何将隐藏消息嵌入传感器测量中，以及如何通过访问历史记录来异步检索隐藏消息。我们以iTrust的安全水处理(SWAT)数据集的水流和水位传感器为例来评估这种方法。为了从特定的覆盖信道(传感器及其传输的数据)中进行归纳，我们反思了在创建网络隐蔽信道的信息隐藏场景中出现的一般挑战，并讨论了覆盖信道选择和发送者/接收者同步的各个方面，以及时间方面，如网络物理系统(CPS)中隐藏消息的潜在持久性。对于经验评估，我们设计并实现了一个隐蔽信道，它利用不同的嵌入策略来执行关于传感器测量中的噪声的自适应方法，导致动态容量和带宽选择以降低检测概率。评估结果表明，使用这种方法，在长期大规模攻击中泄露敏感信息确实是可能的。此外，我们提出了两种检测方法，并使用多个测试数据集和不同的参数对我们的检测器进行了广泛的评估。在0%的假阳性率(FPR)下，我们在测试数据上确定了高达87.8%的检测准确度。",
                    "title_zh": "网络物理系统中的信息隐藏:使用SWAT数据集的传感器数据进行嵌入、检索和检测的挑战"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460396",
                    "title": "Revisiting Perturbed Quantization",
                    "authors": "Jan Butora, Jessica J. Fridrich",
                    "abstract": "In this work, we revisit Perturbed Quantization steganography with modern tools available to the steganographer today, including near-optimal ternary coding and content-adaptive embedding with side-information. In PQ, side-information in the form of rounding errors is manufactured by recompressing a JPEG image with a judiciously selected quality factor. This side-information, however, cannot be used in the same fashion as in conventional side-informed schemes nowadays as this leads to highly detectable embedding. As a remedy, we utilize the steganographic Fisher information to allocate the payload among DCT modes. In particular, we show that the embedding should not be constrained to contributing coefficients only as in the original PQ but should be expanded to the so-called \"contributing DCT modes.\" This approach is extended to color images by slightly modifying the SI-UNIWARD algorithm. Using the best detectors currently available, it is shown that by manufacturing side information with double compression, one can embed the same amount of information into the doubly-compressed cover image with a significantly better security than applying J-UNIWARD directly in the single-compressed image. At the end of the paper, we show that double compression with the same quality makes side-informed steganography extremely detectable and should be avoided.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在这篇文章中，我们用现代工具重新审视了扰动量化隐写术，包括近似最优的三进制编码和带边信息的内容自适应嵌入。在PQ中，舍入误差形式的边信息是通过用明智选择的质量因子再压缩JPEG图像来产生的。然而，这种辅助信息不能以与现在传统的辅助信息方案相同的方式使用，因为这会导致高度可检测的嵌入。作为补救，我们利用隐写Fisher信息在DCT模式之间分配有效载荷。特别地，我们表明嵌入不应该像在原始PQ中那样只限于贡献系数，而是应该扩展到所谓的“贡献DCT模式”通过稍微修改SI-UNIWARD算法，这种方法被扩展到彩色图像。使用当前可用的最佳检测器，示出了通过利用双重压缩来制造辅助信息，可以将相同量的信息嵌入到双重压缩的封面图像中，并且具有比在单一压缩的图像中直接应用J-UNIWARD明显更好的安全性。在文章的最后，我们证明了相同质量的双重压缩使得边信息隐写术非常容易被检测到，应该避免。",
                    "title_zh": "再谈扰动量子化"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460404",
                    "title": "Fast Detection of Heterogeneous Parallel Steganography for Streaming Voice",
                    "authors": "Huili Wang, Zhongliang Yang, Yuting Hu, Zhen Yang, Yongfeng Huang",
                    "abstract": "Heterogeneous parallel steganography (HPS) has become a new trend of current streaming media voice steganography, which hides secret information in the frames of streaming media with multiple kinds of orthogonal steganography. Because of the complexity and imperceptibility of HPS, detecting its existence is a challenge for previous steganalysis methods, especially in the case of short sliding window length and low embedding rate. In order to improve the situation, we design a fast and efficient detection method named the key feature extraction and fusion network (KFEF) based on attention mechanism. The proposed model is able to effectively extract the key characteristic of the exceptions due to steganography and fuse the extracted features for different steganographic algorithms used in HPS. Experimental results show that the proposed method significantly improves the classification accuracy in detecting both low embedding rate samples and short segment samples. In addition, the detection time consumption is shorter than other methods and meets real-time requirements. Finally, with the help of attention we can predict the approximate locations of secret information which may bring new ideas to further steganalysis.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "异构并行隐写(HPS)已经成为当前流媒体语音隐写的新趋势，它利用多种正交隐写技术在流媒体帧中隐藏秘密信息。由于HPS的复杂性和不可感知性，检测它的存在对以前的隐写分析方法是一个挑战，特别是在滑动窗口长度短和嵌入率低的情况下。为了改善这种情况，我们设计了一种基于注意机制的快速有效的检测方法——关键特征提取与融合网络。该模型能够有效提取隐写异常的关键特征，并针对HPS使用的不同隐写算法融合提取的特征。实验结果表明，该方法显著提高了低嵌入率样本和短片段样本的分类精度。此外，检测耗时比其他方法短，满足实时性要求。最后，在注意力的帮助下，我们可以预测秘密信息的大概位置，这可能为进一步的隐写分析带来新的思路。",
                    "title_zh": "流式语音异构并行隐写的快速检测"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460395",
                    "title": "How to Pretrain for Steganalysis",
                    "authors": "Jan Butora, Yassine Yousfi, Jessica J. Fridrich",
                    "abstract": "In this paper, we investigate the effect of pretraining CNNs on ImageNet on their performance when refined for steganalysis of digital images. In many cases, it seems that just 'seeing' a large number of images helps with the convergence of the network during the refinement no matter what the pretraining task is. To achieve the best performance, the pretraining task should be related to steganalysis, even if it is done on a completely mismatched cover and stego datasets. Furthermore, the pretraining does not need to be carried out for very long and can be done with limited computational resources. An additional advantage of the pretraining is that it is done on color images and can later be applied for steganalysis of color and grayscale images while still having on-par or better performance than detectors trained specifically for a given source. The refining process is also much faster than training the network from scratch. The most surprising part of the paper is that networks pretrained on JPEG images are a good starting point for spatial domain steganalysis as well.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在本文中，我们研究了在ImageNet上预处理细胞神经网络对其性能的影响。在许多情况下，无论预训练任务是什么，仅仅“看到”大量图像似乎就有助于网络在细化过程中的收敛。为了实现最佳性能，预训练任务应该与隐写分析相关，即使它是在完全不匹配的封面和隐写数据集上完成的。此外，预训练不需要执行很长时间，并且可以用有限的计算资源来完成。预训练的另一个优点是，它是在彩色图像上进行的，以后可以应用于彩色和灰度图像的隐写分析，同时仍然具有与专门为给定源训练的检测器相当或更好的性能。提炼过程也比从头开始训练网络要快得多。该论文最令人惊讶的部分是，在JPEG图像上预处理的网络也是空域隐写分析的良好起点。",
                    "title_zh": "如何为隐写分析进行预处理"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460397",
                    "title": "Improving EfficientNet for JPEG Steganalysis",
                    "authors": "Yassine Yousfi, Jan Butora, Jessica J. Fridrich, Clément Fuji Tsang",
                    "abstract": "In this paper, we study the EfficientNet family pre-trained on ImageNet when used for steganalysis using transfer learning. We show that certain \"surgical modifications\" aimed at maintaining the input resolution in EfficientNet architectures significantly boost their performance in JPEG steganalysis, establishing thus new benchmarks. The modified models are evaluated by their detection accuracy, the number of parameters, the memory consumption, and the total floating point operations (FLOPs) on the ALASKA II dataset. We also show that, surprisingly, EfficientNets in their \"vanilla form\" do not perform as well as the SRNet in BOSSbase+BOWS2. This is because, unlike ALASKA II images, BOSSbase+BOWS2 contains aggressively subsampled images with more complex content. The surgical modifications in EfficientNet remedy this underperformance as well.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在本文中，我们研究了在ImageNet上预训练的EfficientNet族在使用迁移学习进行隐写分析时的情况。我们发现，旨在保持EfficientNet架构中输入分辨率的某些“外科手术式修改”显著提高了它们在JPEG隐写分析中的性能，从而建立了新的基准。修改后的模型通过其检测精度、参数数量、内存消耗和阿拉斯加II数据集上的总浮点运算(FLOPs)进行评估。我们还发现，令人惊讶的是，在BOSSbase+BOWS2中,“普通形式”的效率网的性能不如SRNet。这是因为，与阿拉斯加II图像不同，BOSSbase+BOWS2包含具有更复杂内容的积极二次抽样图像。有效网络的外科手术修改也弥补了这一不足。",
                    "title_zh": "提高JPEG隐写分析的效率"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460398",
                    "title": "Piracy-Resistant DNN Watermarking by Block-Wise Image Transformation with Secret Key",
                    "authors": "April Pyone Maung Maung, Hitoshi Kiya",
                    "abstract": "In this paper, we propose a novel DNN watermarking method that utilizes a learnable image transformation method with a secret key. The proposed method embeds a watermark pattern in a model by using learnable transformed images and allows us to remotely verify the ownership of the model. As a result, it is piracy-resistant, so the original watermark cannot be overwritten by a pirated watermark, and adding a new watermark decreases the model accuracy unlike most of the existing DNN watermarking methods. In addition, it does not require a special pre-defined training set or trigger set. We empirically evaluated the proposed method on the CIFAR-10 dataset. The results show that it was resilient against fine-tuning and pruning attacks while maintaining a high watermark-detection accuracy.",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/2104.04241"
                    },
                    "abstract_zh": "在本文中，我们提出了一种新的DNN水印方法，它利用了一种带有密钥的可学习的图像变换方法。所提出的方法通过使用可学习的变换图像在模型中嵌入水印图案，并允许我们远程验证模型的所有权。结果，它是抗盗版的，因此原始水印不能被盗版水印覆盖，并且与大多数现有的DNN水印方法不同，添加新水印会降低模型精度。此外，它不需要特殊的预定义训练集或触发集。我们在CIFAR-10数据集上对提出的方法进行了经验评估。实验结果表明，该算法在保持较高的水印检测精度的同时，能够抵抗微调和剪枝攻击。",
                    "title_zh": "基于密钥分块图像变换的抗盗版DNN水印"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460402",
                    "title": "White-Box Watermarking Scheme for Fully-Connected Layers in Fine-Tuning Model",
                    "authors": "Minoru Kuribayashi, Takuro Tanaka, Shunta Suzuki, Tatsuya Yasui, Nobuo Funabiki",
                    "abstract": "For the protection of trained deep neural network(DNN) models, embedding watermarks into the weights of the DNN model have been considered. However, the amount of change in the weights is large in the conventional methods, and it is reported that the existence of hidden watermark can be detected from the analysis of weight variance. This helps attackers to modify the watermark by effectively adding noise to the weight. In this paper, we focus on the fully-connected layers of fine-tuning models and apply a quantization-based watermarking method to the weights sampled from the layers. The advantage of the proposed method is that the change caused by watermark embedding is much smaller and the distortion converges gradually without using any loss function. The validity of the proposed method was evaluated by varying the conditions during the training of DNN model. The results shows the impact of training for DNN model, effectiveness of the embedding method, and high robustness against pruning attacks.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3437880.3460402"
                    },
                    "abstract_zh": "为了保护训练好的深度神经网络(DNN)模型，已经考虑在DNN模型的权重中嵌入水印。然而，在常规方法中，权重的变化量很大，并且据报道，可以从权重方差的分析中检测隐藏水印的存在。这有助于攻击者通过有效地向权重添加噪声来修改水印。在本文中，我们关注于微调模型的全连通层，并将基于量化的水印方法应用于从这些层采样的权重。该方法的优点是水印嵌入引起的变化小得多，失真逐渐收敛，而不使用任何损失函数。通过改变DNN模型训练过程中的条件来评估所提出方法的有效性。实验结果表明了DNN模型的训练效果，嵌入方法的有效性，以及对剪枝攻击的高鲁棒性。",
                    "title_zh": "微调模型中全连通层的白盒水印方案"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460409",
                    "title": "A Protocol for Secure Verification of Watermarks Embedded into Machine Learning Models",
                    "authors": "Katarzyna Kapusta, Vincent Thouvenot, Olivier Bettan, Hugo Beguinet, Hugo Senet",
                    "abstract": "Machine Learning is a well established tool used in a variety of applications. As training advanced models requires considerable amounts of meaningful data in addition to specific knowledge, a new business model separate models creators from model users. Pre-trained models are sold or made available as a service. This raises several security challenges, among others the one of intellectual property protection. Therefore, a new research track actively seeks to provide techniques for model watermarking that would enable model identification in case of suspicion of model theft or misuse. In this paper, we focus on the problem of secure watermarks verification, which affects all of the proposed techniques and until now was barely tackled. First, we revisit the existing threat model. In particular, we explain the possible threats related to a semi-honest or dishonest verification authority. Secondly, we show how to reduce trust requirements between participants by performing the watermarks verification on encrypted data. Finally, we describe a novel secure verification protocol as well as detail its possible implementation using Multi-Party Computation. The proposed solution does not only preserve the confidentiality of the watermarks but also helps detecting evasion attacks. It could be adopted to work with other authentication schemes based on watermarking, especially with image watermarking schemes.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "机器学习是一种在各种应用中使用的成熟工具。由于训练高级模型除了特定的知识之外还需要大量有意义的数据，一种新的商业模型将模型创建者与模型使用者分开。预先训练的模型作为服务出售或提供。这带来了几个安全挑战，其中包括知识产权保护。因此，一个新的研究方向积极地寻求提供用于模型水印的技术，该技术能够在怀疑模型被盗用或误用的情况下识别模型。在本文中，我们集中讨论安全水印验证的问题，它影响到所有提出的技术，直到现在几乎没有解决。首先，我们重新审视现有的威胁模型。特别是，我们解释了与半诚实或不诚实的验证机构相关的可能威胁。其次，我们展示了如何通过对加密数据进行水印验证来降低参与者之间的信任要求。最后，我们描述了一个新的安全验证协议，并详细说明了它的可能实现使用多方计算。所提出的解决方案不仅保持了水印的机密性，而且有助于检测规避攻击。它可以适用于其他基于水印的认证方案，特别是图像水印方案。",
                    "title_zh": "嵌入机器学习模型的水印安全验证协议"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460401",
                    "title": "On the Robustness of Backdoor-based Watermarking in Deep Neural Networks",
                    "authors": "Masoumeh Shafieinejad, Nils Lukas, Jiaqi Wang, Xinda Li, Florian Kerschbaum",
                    "abstract": "Watermarking algorithms have been introduced in the past years to protect deep learning models against unauthorized re-distribution. We investigate the robustness and reliability of state-of-the-art deep neural network watermarking schemes. We focus on backdoor-based watermarking and propose two simple yet effective attacks -- a black-box and a white-box -- that remove these watermarks without any labeled data from the ground truth. Our black-box attack steals the model and removes the watermark with only API access to the labels. Our white-box attack proposes an efficient watermark removal when the parameters of the marked model are accessible, and improves the time to steal a model up to twenty times over the time to train a model from scratch. We conclude that these watermarking algorithms are insufficient to defend against redistribution by a motivated attacker.",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/1906.07745"
                    },
                    "abstract_zh": "在过去几年中，已经引入了水印算法来保护深度学习模型免受未经授权的重新分发。我们研究了最先进的深度神经网络水印方案的鲁棒性和可靠性。我们关注基于后门的水印技术，并提出了两种简单而有效的攻击——黑盒攻击和白盒攻击——这两种攻击在没有任何标记数据的情况下从真实数据中移除这些水印。我们的黑盒攻击窃取了模型并移除了水印，只有API可以访问标签。我们的白盒攻击提出了一种当标记模型的参数可访问时的有效水印去除，并且将窃取模型的时间提高了从零开始训练模型的时间的20倍。我们的结论是，这些水印算法不足以抵御有动机的攻击者的重新分发。",
                    "title_zh": "深度神经网络中后门水印的鲁棒性研究"
                },
                {
                    "url": "https://doi.org/10.1145/3437880.3460399",
                    "title": "DNN Watermarking: Four Challenges and a Funeral",
                    "authors": "Mauro Barni, Fernando Pérez-González, Benedetta Tondi",
                    "abstract": "The demand for methods to protect the Intellectual Property Rights (IPR) associated to Deep Neural Networks (DNNs) is rising. Watermarking has been recently proposed as a way to protect the IPR of DNNs and track their usages. Although a number of techniques for media watermarking have been proposed and developed over the past decades, their direct translation to DNN watermarking faces the problem of the embedding being carried out on functionals instead of signals. This originates differences not only in the way performance, robustness and unobtrusiveness are measured, but also on the embedding domain, since there is the possibility of hiding information in the model behavior. In this paper, we discuss these dissimilarities that lead to a DNN-specific taxonomy of watermarking techniques. Then, we present four challenges specific to DNN watermarking that, for their practical importance and theoretical interest, should occupy the agenda of researchers in the next years. Finally, we discuss some bad practices that negatively affected research in media watermarking and that should not be repeated in the case of DNNs.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "对保护与深度神经网络(DNNs)相关的知识产权(IPR)的方法的需求正在上升。最近提出了水印作为保护dnn的知识产权和跟踪其使用的一种方式。虽然在过去的几十年中已经提出并开发了许多媒体水印技术，但是它们直接转换成DNN水印面临着在功能上而不是信号上进行嵌入的问题。这不仅在性能、鲁棒性和不唐突性的测量方式上产生差异，而且在嵌入域上也产生差异，因为有可能在模型行为中隐藏信息。在本文中，我们讨论了这些不同，导致DNN特定的水印技术分类。然后，我们提出了DNN水印特有的四个挑战，由于它们的实际重要性和理论兴趣，应该在未来几年占据研究者的议程。最后，我们讨论了一些对媒体水印研究产生负面影响的不良实践，这些不良实践不应该在DNNs中重复出现。",
                    "title_zh": "DNN水印:四个挑战和一个葬礼"
                }
            ]
        }
    ],
    "2020": [
        {
            "dblp_url": "https://dblp.uni-trier.de/db/conf/ih/ihmmsec2020.html",
            "conf_title": "8th IH&MMSec 2020: Denver, CO, USA",
            "conf_url": "https://doi.org/10.1145/3369412",
            "papers": [
                {
                    "url": "https://doi.org/10.1145/3369412.3396882",
                    "title": "Exploiting Micro-Signals for Physiological Forensics",
                    "authors": "Min Wu",
                    "abstract": "A variety of nearly invisible \"micro-signals\" have played important roles in media security and forensics. These noise-like micro-signals are ubiquitous and typically an order of magnitude lower in strength or scale than the dominant ones. They are traditionally removed or ignored as nuances outside the forensic domain. This keynote talk discusses the recent research harnessing micro-signals to infer a person's physiological conditions. One type of such signals is the subtle changes in facial skin color in accordance with the heartbeat. Video analysis of this repeating change provides a contact-free way to capture photo-plethysmogram (PPG). While heart rate can be tracked from videos of resting cases, it is challenging to do so for cases involving substantial motion, such as when a person is walking around, running on a treadmill, or driving on a bumpy road. It will be shown in this talk how the expertise with micro-signals from media forensics has enabled the exploration of new opportunities in physiological forensics and a broad range of applications.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3369412.3396882"
                    },
                    "abstract_zh": "各种几乎看不见的“微信号”在媒体安全和取证中发挥了重要作用。这些类似噪声的微信号无处不在，通常在强度或规模上比主导信号低一个数量级。传统上，它们被视为法医领域之外的细微差别而被删除或忽略。本次主题演讲讨论了最近利用微信号来推断一个人的生理状况的研究。其中一种信号是面部皮肤颜色随着心跳的细微变化。对这种重复变化的视频分析提供了一种无接触的方式来捕捉照片体积描记图(PPG)。虽然可以从静止情况的视频中跟踪心率，但对于涉及大量运动的情况，例如当一个人四处走动、在跑步机上跑步或在颠簸的道路上行驶时，这样做是有挑战性的。本次演讲将展示媒体取证的微信号专业技术如何在生理取证和广泛的应用中探索新的机遇。",
                    "title_zh": "利用微信号进行生理取证"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3396883",
                    "title": "Game-Theoretic Perspectives and Algorithms for Cybersecurity",
                    "authors": "Christopher Kiekintveld",
                    "abstract": "Information plays a key role in many games, and game theory includes reasoning about how agents should perceive signals, and how they should strategically decide what signals to send. This can involve complex tradeoffs about how revealing certain information will affect the beliefs and actions of other players. I will overview some basic approaches for modeling information in game theory, such as signaling games, and applications to games such as Poker. The second part of the talk with focus on our work applying game theoretic models and algorithms in cybersecurity. I will discuss how we apply game theory to optimize strategies for deception in cybersecurity, including honeypots, honey traffic, and other deceptive objects. I will also cover work that considers dynamic deception using sequential models that capture uncertainty. Finally, I will discuss some recent work in adversarial learning and connections between this area and game theory.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3369412.3396883"
                    },
                    "abstract_zh": "信息在许多游戏中起着关键作用，博弈论包括推理代理人应该如何感知信号，以及他们应该如何战略性地决定发送什么信号。这可能涉及到复杂的权衡，即披露某些信息将如何影响其他参与者的信念和行为。我将概述一些在博弈论中建模信息的基本方法，比如信号游戏，以及在游戏中的应用，比如扑克。演讲的第二部分着重于我们在网络安全中应用博弈论模型和算法的工作。我将讨论我们如何应用博弈论来优化网络安全中的欺骗策略，包括蜜罐、蜜罐流量和其他欺骗对象。我还将介绍使用捕捉不确定性的序列模型来考虑动态欺骗的工作。最后，我将讨论一些最近在对抗性学习方面的工作，以及这个领域和博弈论之间的联系。",
                    "title_zh": "网络安全的博弈论观点和算法"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395067",
                    "title": "Linguistic Steganalysis via Densely Connected LSTM with Feature Pyramid",
                    "authors": "Hao Yang, YongJian Bao, Zhongliang Yang, Sheng Liu, Yongfeng Huang, Saimei Jiao",
                    "abstract": "With the growing attention on multimedia security and rapid development of natural language processing technologies, various linguistic steganographic algorithms based on automatic text generation technology have been proposed increasingly, which brings great challenges in maintaining security of cyberspace. The prevailing linguistic steganalysis methods based on neural networks only conduct linguistic steganalysis with feature vectors from last layer of neural network, which may be insufficient for neural linguistic steganalysis. In this paper, we propose a neural linguistic steganalysis scheme based on densely connected Long short-term memory networks (LSTM) with feature pyramids which can incorporate more low level features to detect generative text steganographic algorithms. In the proposed framework, words in text are firstly mapped into semantic space with a hidden representation for better exploitation of the semantic features. Then, stacked bidirectional Long short-term memory networks are ultilized to extract different levels of semantic features. In order to incorporate more low level features from neural networks, we introduced two components: dense connections and feature pyramids to enhance the low level features in feature vectors. Finally, the semantic features from all levels are fused and we use a sigmoid layer to categorize the input text as cover or stego. Experiments showed that the proposed scheme can achieve the state-of-the-art results in detecting recently proposed linguistic steganographic algorithms.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "随着人们对多媒体安全的日益关注和自然语言处理技术的快速发展，各种基于自动文本生成技术的语言隐写算法不断被提出，这给维护网络空间的安全带来了巨大的挑战。目前流行的基于神经网络的语言隐写分析方法只利用神经网络最后一层的特征向量进行语言隐写分析，这对于神经语言隐写分析可能是不够的。本文提出了一种基于密集连接的长短期记忆网络(LSTM)的神经语言学隐写分析方案，该方案具有特征金字塔，可以包含更多的低层特征来检测生成文本隐写算法。在所提出的框架中，文本中的词首先被映射到语义空间，并以一种隐藏的表示方式来更好地利用语义特征。然后，利用堆叠双向长短时记忆网络提取不同层次的语义特征。为了结合更多来自神经网络的低层特征，我们引入了两个组件:密集连接和特征金字塔来增强特征向量中的低层特征。最后，融合所有级别的语义特征，并使用sigmoid层将输入文本分类为隐藏文本或隐写文本。实验表明，该方案在检测最近提出的语言隐写算法时可以达到最先进的结果。",
                    "title_zh": "基于特征金字塔的密集连通LSTM的语言隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395064",
                    "title": "Deep Audio Steganalysis in Time Domain",
                    "authors": "Daewon Lee, Tae-Woo Oh, Kibom Kim",
                    "abstract": "Digital audio, as well as image, is one of the most popular media for information hiding. However, even the state-of-the-art deep learning model still has a limitation for detecting basic LSB steganography algorithms that hide secret messages in time domain of WAV audio. To advance audio steganalysis based on deep learning, deep audio steganalysis, in time domain of lossless audio format, we have developed a convolutional neural network that incorporates bit-plane separation, weight-standardized convolution, and channel attention. Training through payload curriculum learning and testing for six steganography methods demonstrated that our proposed model is superior to the other two deep learning models, achieving state-of-the-art performance. We expect our approach will provide insights to create a breakthrough for deep audio steganalysis.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "数字音频和图像是最流行的信息隐藏媒体之一。然而，即使是最先进的深度学习模型，对于检测在WAV音频的时域中隐藏秘密消息的基本LSB隐写算法仍有局限性。为了在无损音频格式的时域中推进基于深度学习的音频隐写分析，深度音频隐写分析，我们开发了一种卷积神经网络，该网络结合了位平面分离、权重标准化卷积和通道注意。通过有效载荷课程学习的训练和对六种隐写方法的测试表明，我们提出的模型优于其他两种深度学习模型，达到了最先进的性能。我们希望我们的方法将为深度音频隐写分析提供突破性的见解。",
                    "title_zh": "时域深度音频隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395060",
                    "title": "Reinforcement Learning Aided Network Architecture Generation for JPEG Image Steganalysis",
                    "authors": "Jianhua Yang, Beiling Lu, Liang Xiao, Xiangui Kang, Yunqing Shi",
                    "abstract": "The architectures of convolutional neural networks used in steganalysis have been designed heuristically. In this paper, an automatic Network Architecture Generation algorithm based on reinforcement learning for JPEG image Steganalysis (JS-NAG) has been proposed. Different from the automatic neural network generation methods in computer vision which are based on the strong content signals, steganalysis is based on the weak embedded signals, thus needs specific design. In the proposed method, the agent is trained to sequentially select some high-performing blocks using Q-learning to generate networks. An early stop strategy and a well-designed performance prediction function have been utilized to reduce the search time. To generate the optimal networks, hundreds of networks have been searched and trained on 3 GPUs for 15 days. To further improve the detection accuracy, we make an ensemble classifier out of the generated convolutional neural networks. The experimental results have shown that the proposed method significantly outperforms the current state-of-the-art CNN based methods.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "隐写分析中使用的卷积神经网络的结构是启发式设计的。提出了一种基于强化学习的JPEG图像隐写分析网络结构自动生成算法(JS-NAG)。与计算机视觉中基于强内容信号的神经网络自动生成方法不同，隐写分析基于弱嵌入信号，因此需要特殊设计。在所提出的方法中，使用Q-learning训练代理顺序地选择一些高性能的块来生成网络。使用了早期停止策略和精心设计的性能预测函数来减少搜索时间。为了生成最佳网络，数百个网络已经在3个GPU上搜索和训练了15天。为了进一步提高检测精度，我们用生成的卷积神经网络做了一个集成分类器。实验结果表明，该方法明显优于当前最先进的基于CNN的方法。",
                    "title_zh": "用于JPEG图像隐写分析的强化学习辅助网络结构生成"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395072",
                    "title": "Feature Aggregation Networks for Image Steganalysis",
                    "authors": "Haneol Jang, Tae-Woo Oh, Kibom Kim",
                    "abstract": "Since convolutional neural networks have shown remarkable performance on various computer vision tasks, many network architectures for image steganalysis have been introduced. Many of them use fixed preprocessing filters for stable learning, which have a disadvantage of limited use of the information of the input image. The recently introduced end-to-end learning method uses a structure that limits the number of channels of feature maps close to the input and stacks residual blocks. This method has limitations in generating feature maps of various levels and resolutions that can be effective for steganalysis. We therefore propose the feature aggregation-based steganalysis networks: expand the number of channels of convolutional blocks close to the input data, aggregate feature maps of various levels and resolutions, and utilize rich information to improve steganalysis performance. In addition, the capped activation function is applied to obtain better generalization performance. The proposed method outperforms the state-of-the-art steganalysis on detection of the advanced steganography algorithms J-UNIWARD and UED, for JPEG quality factor 75 and 95.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "由于卷积神经网络在各种计算机视觉任务中表现出显著的性能，许多用于图像隐写分析的网络结构被引入。它们中的许多使用固定的预处理滤波器用于稳定学习，这具有限制使用输入图像信息的缺点。最近引入的端到端学习方法使用了一种结构，该结构限制了靠近输入的特征图的通道数，并堆叠了残余块。这种方法在生成对隐写分析有效的各种级别和分辨率的特征图方面具有局限性。因此，我们提出了基于特征聚合的隐写分析网络:扩展接近输入数据的卷积块的通道数，聚合各种级别和分辨率的特征图，并利用丰富的信息来提高隐写分析性能。此外，为了获得更好的泛化性能，采用了封顶激活函数。对于JPEG质量因子75和95，所提出的方法在检测高级隐写算法J-UNIWARD和UED上优于最先进的隐写分析。",
                    "title_zh": "用于图像隐写分析的特征聚合网络"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395061",
                    "title": "Pixels-off: Data-augmentation Complementary Solution for Deep-learning Steganalysis",
                    "authors": "Mehdi Yedroudj, Marc Chaumont, Frédéric Comby, Ahmed Oulad Amara, Patrick Bas",
                    "abstract": "After 2015, CNN-based steganalysis approaches have started replacing the two-step machine-learning-based steganalysis approaches (feature extraction and classification), mainly due to the fact that they offer better performance. In many instances, the performance of these networks depend on the size of the learning database. Until a certain point, the larger the database, the better the results. However, working with a large database with controlled acquisition conditions is usually rare or unrealistic in an operational context. An easy and efficient approach is thus to augment the database, in order to increase its size, and therefore to improve the efficiency of the steganalysis process. In this article, we propose a new way to enrich a database in order to improve the CNN-based steganalysis performance. We have named our technique \"pixels-off\". This approach is efficient, generic, and is usable in conjunction with other data-enrichment approaches. Additionally, it can be used to build an informed database that we have named \"Side-Channel-Aware databases\" (SCA-databases).",
                    "files": {
                        "openAccessPdf": "https://hal-lirmm.ccsd.cnrs.fr/lirmm-02559838/file/IHMMSec-2016_Yedroudj_Chaumont_Comby_Amara_Bas_Pixels-off.pdf"
                    },
                    "abstract_zh": "2015年后，基于CNN的隐写分析方法开始取代两步基于机器学习的隐写分析方法(特征提取和分类)，主要是因为它们提供了更好的性能。在许多情况下，这些网络的性能取决于学习数据库的大小。直到某一点，数据库越大，结果越好。然而，在操作环境中，使用具有受控采集条件的大型数据库通常很少或不现实。因此，一种简单而有效的方法是扩充数据库，以增加其大小，从而提高隐写分析过程的效率。在这篇文章中，我们提出了一种新的方法来丰富数据库，以提高基于CNN的隐写分析性能。我们将这项技术命名为“像素关闭”。这种方法高效、通用，可以与其他数据丰富方法结合使用。此外，它还可以用来构建一个可靠的数据库，我们称之为“侧信道感知数据库”(SCA-databases)。",
                    "title_zh": "像素关闭:深度学习隐写分析的数据增强补充解决方案"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395077",
                    "title": "Protecting Smartphone Screen Notification Privacy by Verifying the Gripping Hand",
                    "authors": "Chen Wang, Jingjing Mu, Long Huang",
                    "abstract": "As the most common personal devices, smartphones contain the user's private information. While people use mobile devices anytime and anywhere, the sensitive contents might be leaked from the screens. The smartphone notifications cause such privacy leakages even on a lock screen. With the aim to alert the user of an event (e.g., text messages, phone calls and calendar reminders), these onscreen notifications usually contain the sender's name and even a clip of the contents for preview. Such information, if not displayed appropriately, may cause the leakages of the user's social relations, personal hobbies and private message contents. This work focuses on wisely displaying the notifications to avoid leaking the user's privacy. We develop an unobtrusive user authentication system to confirm the user identity via their gripping-hands before displaying notifications. In particular, we carefully design an inaudible acoustic signal and emit it from the smartphone speaker to sense the gripping hand, when there is a need to push notifications. The signal propagating to the smartphone's microphones carries the user's biometric information related to the gripping hand (e.g., palm size and gripping strength). We further derive the Mel Frequency Cepstral Coefficient time series and develop a machine learning-based algorithm to identify the user. The experimental results show that our system can identify 8 users with 92% accuracy.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "作为最常见的个人设备，智能手机包含用户的私人信息。当人们随时随地使用移动设备时，敏感内容可能会从屏幕上泄露。即使在锁屏状态下，智能手机的通知也会导致这样的隐私泄露。为了提醒用户注意某个事件(例如，文本消息、电话和日历提醒)，这些屏幕通知通常包含发送者的姓名，甚至是预览内容的剪辑。如果显示不当，这些信息可能会导致用户的社交关系、个人爱好和私人消息内容的泄露。这项工作的重点是明智地显示通知，以避免泄露用户的隐私。我们开发了一个不引人注目的用户认证系统，在显示通知之前通过用户的抓手来确认用户的身份。特别是，当需要推送通知时，我们精心设计了一种听不见的声音信号，并从智能手机扬声器中发出，以感知紧握的手。传播到智能手机麦克风的信号携带了与抓握手相关的用户生物特征信息(例如，手掌大小和抓握力度)。我们进一步推导Mel频率倒谱系数时间序列，并开发基于机器学习的算法来识别用户。实验结果表明，我们的系统能够以92%的准确率识别8个用户。",
                    "title_zh": "通过验证紧握的手来保护智能手机屏幕通知隐私"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395062",
                    "title": "What if Adversarial Samples were Digital Images?",
                    "authors": "Benoît Bonnet, Teddy Furon, Patrick Bas",
                    "abstract": "Although adversarial sampling is a trendy topic in computer vision, very few works consider the integral constraint: The result of the attack is a digital image whose pixel values are integers. This is not an issue at first sight since applying a rounding after forging an adversarial sample trivially does the job. Yet, this paper shows theoretically and experimentally that this operation has a big impact. The adversarial perturbations are fragile signals whose quantization destroys its ability to delude an image classifier. This paper presents a new quantization mechanism which preserves the adversariality of the perturbation. Its application outcomes to a new look at the lessons learnt in adversarial sampling.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-02553006v2/file/main_finalWOcopy.pdf"
                    },
                    "abstract_zh": "虽然对抗性采样是计算机视觉中的一个热门话题，但很少有作品考虑整数约束:攻击的结果是像素值为整数的数字图像。乍看之下，这不是一个问题，因为在伪造一个对立样本之后应用舍入可以轻松地完成这项工作。然而，本文从理论和实验上表明，这种操作有很大的影响。对抗性扰动是脆弱的信号，其量化破坏了欺骗图像分类器的能力。本文提出了一种新的量子化机制，它保持了微扰的对抗性。它的应用结果，以一种新的眼光来看待在对立抽样中吸取的教训。",
                    "title_zh": "如果对立的样本是数字图像呢？"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395069",
                    "title": "LiveDI: An Anti-theft Model Based on Driving Behavior",
                    "authors": "Hashim Abu-gellban, Long Hoang Nguyen, Mahdi Moghadasi, Zhenhe Pan, Fang Jin",
                    "abstract": "Anti-theft problem has been challenging since it mainly depends on the existence of external devices to defend from thefts. Recently, driver behavior analysis using supervised learning has been investigated with the goal to detect burglary by identifying drivers. In this paper, we propose a data-driven technique, LiveDI, which uses driving behavior removing the use of external devices in order to identify drivers. The built model utilizes Gated Recurrent Unit (GRU) and Fully Convolutional Networks (FCN) to learn long-short term patterns of the driving behaviors from drivers. Additionally, we improve the training time by utilizing the Segmented Feature Generation (SFG) algorithm to reduce the state space where the driving behaviors are split with a time window for analysis. Extensive experiments are conducted which show the impact of parameters on our technique and verify that our proposed approach outperforms the state-of-the-art baseline methods.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "防盗问题一直具有挑战性，因为它主要依赖于外部设备的存在来防止盗窃。最近，已经研究了使用监督学习的驾驶员行为分析，目的是通过识别驾驶员来检测盗窃。在本文中，我们提出了一种数据驱动的技术，LiveDI，它使用驾驶行为来识别驾驶员，而不使用外部设备。所建立的模型利用门控递归单元(GRU)和全卷积网络(FCN)来学习驾驶员驾驶行为的长短期模式。此外，我们通过使用分段特征生成(SFG)算法来减少状态空间，从而改进训练时间，在该状态空间中，驾驶行为被分割成一个时间窗口用于分析。进行了大量的实验，显示了参数对我们的技术的影响，并验证了我们提出的方法优于最先进的基线方法。",
                    "title_zh": "LiveDI:一种基于驾驶行为的防盗模型"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395076",
                    "title": "On the Difficulty of Hiding Keys in Neural Networks",
                    "authors": "Tobias Kupek, Cecilia Pasquini, Rainer Böhme",
                    "abstract": "In order to defend neural networks against malicious attacks, recent approaches propose the use of secret keys in the training or inference pipelines of learning systems. While this concept is innovative and the results are promising in terms of attack mitigation and classification accuracy, the effectiveness relies on the secrecy of the key. However, this aspect is often not discussed. In this short paper, we explore this issue for the case of a recently proposed key-based deep neural network. White-box experiments on multiple models and datasets, using the original key-based method and our own extensions, show that it is currently possible to extract secret key bits with relatively limited effort.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "为了保护神经网络免受恶意攻击，最近的方法提出在学习系统的训练或推理管道中使用秘密密钥。虽然这个概念是创新的，并且在攻击缓解和分类准确性方面的结果是有希望的，但是有效性依赖于密钥的保密性。但是，这方面往往不被讨论。在这篇短文中，我们以最近提出的基于密钥的深度神经网络为例来探讨这个问题。使用原始的基于密钥的方法和我们自己的扩展，在多个模型和数据集上的白盒实验表明，目前可以用相对有限的努力提取秘密密钥位。",
                    "title_zh": "神经网络中隐藏密钥的困难性"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395063",
                    "title": "Nested Tailbiting Convolutional Codes for Secrecy, Privacy, and Storage",
                    "authors": "Thomas Jerkovits, Onur Günlü, Vladimir Sidorenko, Gerhard Kramer",
                    "abstract": "The key agreement problem with biometric or physical identifiers and two terminals for key enrollment and reconstruction is considered. A nested convolutional code construction that performs lossy compression with side information is proposed. Nested convolutional codes are an alternative to nested polar codes and nested random linear codes that achieve all points of the key-leakage-storage regions of the generated-secret and chosen-secret models for long block lengths. Our design uses a convolutional code for vector quantization during enrollment and a subcode of it for error correction during reconstruction. Physical identifiers with small bit error probability are considered to illustrate the gains of the proposed construction. One variant of nested convolutional codes improves on all previous constructions in terms of the key vs. storage rate ratio but it has high complexity. Another variant of nested convolutional codes with lower complexity performs similarly to previously designed nested polar codes. The results suggest that the choice of convolutional or polar codes for key agreement with identifiers depends on the complexity constraints.",
                    "files": {
                        "openAccessPdf": "https://arxiv.org/pdf/2004.13095.pdf"
                    },
                    "abstract_zh": "考虑了具有生物特征或物理标识符以及用于密钥注册和重建的两个终端的密钥协商问题。提出了一种利用辅助信息执行有损压缩的嵌套卷积码构造。嵌套卷积码是嵌套极性码和嵌套随机线性码的替代码，它们实现了长块长度的生成秘密和选择秘密模型的密钥泄漏存储区域的所有点。我们的设计在注册期间使用卷积码进行矢量量化，在重构期间使用卷积码的子码进行纠错。具有小比特误差概率的物理标识符被考虑来说明所提出的构造的增益。嵌套卷积码的一个变体在密钥与存储速率比方面改进了所有先前的构造，但是它具有高复杂度。具有较低复杂度的嵌套卷积码的另一种变体的性能类似于先前设计的嵌套极性码。结果表明，选择卷积码或极性码来与标识符进行密钥协商取决于复杂度限制。",
                    "title_zh": "用于保密、隐私和存储的嵌套咬尾卷积码"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395073",
                    "title": "Simulation of Border Control in an Ongoing Web-based Experiment for Estimating Morphing Detection Performance of Humans",
                    "authors": "Andrey Makrushin, Dennis Siegel, Jana Dittmann",
                    "abstract": "A morphed face image injected into an identity document destroys the unique link between a person and a document meaning that such a multi-identity document may be successfully used by several persons for face-recognition-based identity verification. A morphed face in an electronic machine readable travel document may allow a wanted criminal to illicitly cross a border. This paper describes an improvement of our ongoing web-based experiment for a border control simulation in which human examiners should first detect high-resolution morphed face images and second match potentially morphed document images against \"live\" faces of travelers. The error rates of humans in both parts of the experiment are compared with those of automated morphing detectors and face recognition systems. This experiment improves understanding the capabilities and limits of humans in withstanding the face morphing attack as well as the factors influencing their performance.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3369412.3395073"
                    },
                    "abstract_zh": "注入身份证件的变形人脸图像破坏了人和证件之间的唯一联系，这意味着这种多重身份证件可以被几个人成功地用于基于人脸识别的身份验证。电子机器可读旅行证件中的变形脸可能允许被通缉的罪犯非法越境。本文描述了我们正在进行的基于网络的边界控制模拟实验的改进，在该实验中，人类检查员应该首先检测高分辨率的变形人脸图像，然后将潜在变形的文档图像与旅行者的“活”人脸进行匹配。在实验的两个部分中，人类的错误率与自动变形检测器和人脸识别系统的错误率进行了比较。该实验提高了对人类在抵抗面部变形攻击方面的能力和限制以及影响其表现的因素的理解。",
                    "title_zh": "在正在进行的基于网络的实验中模拟边界控制以评估人类的变形检测性能"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395070",
                    "title": "Exploiting Prediction Error Inconsistencies through LSTM-based Classifiers to Detect Deepfake Videos",
                    "authors": "Irene Amerini, Roberto Caldelli",
                    "abstract": "The ability of artificial intelligence techniques to build synthesized brand new videos or to alter the facial expression of already existing ones has been efficiently demonstrated in the literature. The identification of such new threat generally known as Deepfake, but consisting of different techniques, is fundamental in multimedia forensics. In fact this kind of manipulated information could undermine and easily distort the public opinion on a certain person or about a specific event. Thus, in this paper, a new technique able to distinguish synthetic generated portrait videos from natural ones is introduced by exploiting inconsistencies due to the prediction error in the re-encoding phase. In particular, features based on inter-frame prediction error have been investigated jointly with a Long Short-Term Memory (LSTM) model network able to learn the temporal correlation among consecutive frames. Preliminary results have demonstrated that such sequence-based approach, used to distinguish between original and manipulated videos, highlights promising performances.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "人工智能技术构建合成的全新视频或改变现有视频的面部表情的能力已经在文献中得到有效证明。这种新威胁通常被称为Deepfake，但由不同的技术组成，其识别是多媒体取证的基础。事实上，这种被操纵的信息会破坏并轻易地扭曲公众对某个人或某个特定事件的看法。因此，在本文中，通过利用由于再编码阶段的预测误差导致的不一致性，引入了一种能够区分合成生成的人像视频和自然人像视频的新技术。特别地，已经与能够学习连续帧之间的时间相关性的长短期记忆(LSTM)模型网络联合研究了基于帧间预测误差的特征。初步结果表明，这种基于序列的方法，用于区分原始和操纵的视频，突出了有希望的性能。",
                    "title_zh": "通过基于LSTM的分类器利用预测误差不一致性检测深度伪造视频"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395059",
                    "title": "Photo Forensics From Rounding Artifacts",
                    "authors": "Shruti Agarwal, Hany Farid",
                    "abstract": "Many aspects of JPEG compression have been successfully used in the domain of photo forensics. Adding to this literature, we describe a JPEG artifact that can arise depending upon seemingly innocuous implementation details in a JPEG encoder. We describe the nature of these artifacts and show how a generic JPEG encoder can be configured to explain a wide range of these artifacts found in real-world cameras. We also describe an algorithm to simultaneously estimate the nature of these artifacts and localize inconsistencies that can arise from a wide range of image manipulations.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3369412.3395059"
                    },
                    "abstract_zh": "JPEG压缩的许多方面已经成功地用于照片取证领域。除此之外，我们还描述了一个JPEG伪像，它可能是由JPEG编码器中看似无害的实现细节引起的。我们描述了这些伪像的性质，并展示了如何配置一个通用的JPEG编码器来解释在现实世界的相机中发现的各种伪像。我们还描述了一种算法，以同时估计这些伪影的性质和定位不一致，可能出现的各种图像操作。",
                    "title_zh": "圆形文物的照片取证"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395068",
                    "title": "Information Hiding in Industrial Control Systems: An OPC UA based Supply Chain Attack and its Detection",
                    "authors": "Mario Hildebrandt, Kevin Lamshöft, Jana Dittmann, Tom Neubert, Claus Vielhauer",
                    "abstract": "Industrial Control Systems (ICS) help to automate various cyber-physical systems in our world. The controlled processes range from rather simple traffic lights and elevators to complex networks of ICS in car manufacturing or controlling nuclear power plants. With the advent of industrial Ethernet ICS are increasingly connected to networks of Information Technology (IT). Thus, novel attack vectors on ICS are possible. In IT networks information hiding and steganography is increasingly used in advanced persistent threats to conceal the infection of the systems allowing the attacker to retain control over the compromised networks. In parallel ICS are more and more a target for attacks as well. Here, simple automated attacks as well as targeted attacks of nation state actors with the intention of damaging components or infrastructures as a part of cyber crime have already been observed. Information hiding could bring such attacks to a new level by integrating backdoors and hidden/covert communication channels that allow for attacking specific processes whenever it is deemed necessary. This paper sheds light on potential attack vectors on Programmable Logic Controllers (PLCs) using OPC Unified Architecture (OPC UA) network protocol based communication. We implement an exemplary supply chain attack consisting of an OPC UA server (Bob, B) and a Siemens S7-1500 PLC as OPC UA client (Alice, A). The hidden storage channel is using source timestamps to embed encrypted control sequences allowing for setting digital outputs to arbitrary values. The attack is solely relying on the programming of the PLC and does not require firmware level access. Due to the potential harm to life caused by attacks on cyber-physical systems any presentation of novel attack vectors need to present suitable mitigation strategies. Thus, we investigate potential approaches for the detection of the hidden storage channel for a warden W as well as potential countermeasures in order to increase the warden-compliance. Our machine learning based detection approach using a One-Class-Classifier yields a detection performance of 89.5% with zero false positives within an experiment with 46,159 OPC UA read responses without a steganographic message and 7,588 OPC UA read responses with an embedded steganographic message.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3369412.3395068"
                    },
                    "abstract_zh": "工业控制系统(ICS)有助于我们世界中各种信息物理系统的自动化。受控过程的范围从相当简单的交通灯和电梯到汽车制造或控制核电站的复杂集成电路网络。随着工业以太网的出现，IC越来越多地连接到信息技术(IT)网络。因此，集成电路上的新型攻击载体是可能的。在IT网络中，信息隐藏和隐写术越来越多地用于高级持久威胁中，以隐藏系统的感染，从而使攻击者能够保持对受损网络的控制。同时，集成电路也越来越成为攻击的目标。在这里，已经观察到简单的自动攻击以及国家行为者的有针对性的攻击，其目的是破坏作为网络犯罪一部分的组件或基础设施。信息隐藏可以通过集成后门和隐藏/隐蔽的通信通道将这种攻击带到一个新的水平，这些通道允许在认为必要时攻击特定的进程。本文揭示了潜在的攻击向量对可编程逻辑控制器(PLC)使用OPC统一架构(OPC UA)网络协议为基础的通信。我们实现了由OPC UA服务器(Bob，B)和作为OPC UA客户端的西门子S7-1500 PLC(Alice，A)组成的示例性供应链攻击。隐藏存储通道使用源时间戳嵌入加密控制序列，允许将数字输出设置为任意值。这种攻击仅仅依靠PLC的编程，不需要固件级别的访问。由于对网络物理系统的攻击会对生命造成潜在危害，因此任何新型攻击媒介的呈现都需要呈现合适的缓解策略。因此，我们研究了用于检测典狱长W的隐藏存储通道的潜在方法以及潜在对策，以便增加典狱长的依从性。我们的基于机器学习的检测方法使用一类分类器，在46，159个没有隐写消息的OPC UA读取响应和7，588个具有嵌入隐写消息的OPC UA读取响应的实验中，产生了89.5%的检测性能，并且没有假阳性。",
                    "title_zh": "工业控制系统中的信息隐藏:基于OPC UA的供应链攻击及其检测"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395071",
                    "title": "Simulating Suboptimal Steganographic Embedding",
                    "authors": "Christy Kin-Cleaves, Andrew D. Ker",
                    "abstract": "Researchers who wish to benchmark the detectability of steganographic distortion functions typically simulate stego objects. However, the difference (coding loss) between simulated stego objects, and real stego objects is significant, and dependent on multiple factors. In this paper, we first identify some factors affecting the coding loss, then propose a method to estimate and correct for coding loss by sampling a few covers and messages. This allows us to simulate suboptimally-coded stego objects which are more accurate representations of real stego objects. We test our results against real embeddings, and naive PLS simulation, showing our simulated stego objects are closer to real embeddings in terms of both distortion and detectability. This is the case even when only a single image and message as used to estimate the loss.",
                    "files": {
                        "openAccessPdf": "https://ora.ox.ac.uk/objects/uuid:a748c193-cf59-4143-afbf-e96c61f25911/files/snp193938z"
                    },
                    "abstract_zh": "希望对隐写失真函数的可检测性进行基准测试的研究人员通常会模拟隐写对象。然而，模拟隐写对象和真实隐写对象之间的差异(编码损失)是显著的，并且取决于多个因素。在本文中，我们首先确定了一些影响编码损失的因素，然后提出了一种方法来估计和纠正编码损失抽样几个封面和消息。这允许我们模拟次优编码的隐写对象，其是真实隐写对象的更精确的表示。我们针对真实嵌入和朴素PLS模拟测试我们的结果，显示我们的模拟隐写对象在失真和可检测性方面更接近真实嵌入。即使当仅使用单个图像和消息来估计损失时，情况也是如此。",
                    "title_zh": "模拟次优隐写嵌入"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395066",
                    "title": "A Robust Video Steganographic Method against Social Networking Transcoding Based on Steganographic Side Channel",
                    "authors": "Pingan Fan, Hong Zhang, Yifan Cai, Pei Xie, Xianfeng Zhao",
                    "abstract": "The social networks transcode uploaded videos in a lossy way, which makes most video steganographic methods become unusable. In this paper, a robust video steganographic method is proposed to resist video transcoding on social networking sites. The luminance component of the raw video is selected as the cover and Quantization Index Modulation (QIM) algorithm based on block statistical features is applied to embed secret messages. To make a good tradeoff between the robustness and visual quality, an iteration in the local transcoder is designed to determine the minimum quantization step for each video. Then, a strategy of selecting robust video frames is proposed to further improve the robustness and security. To avoid sharing information beforehand between the sender and the receiver, a steganographic side channel is built for correct message extraction. Experimental results have shown that our proposed method can provide strong robustness against social networks transcoding, the average bit error rate is less than 1%. Meanwhile, our proposed method achieves a satisfactory level of security performance. It's a robust and secure method for covert communication on social networking sites such as YouTube and Vimeo.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "社交网络以有损的方式对上传的视频进行转码，这使得大多数视频隐写方法变得不可用。提出一种鲁棒的视频隐写方法来抵抗社交网站上的视频转码。选择原始视频的亮度分量作为覆盖，并应用基于块统计特征的量化索引调制(QIM)算法来嵌入秘密消息。为了在鲁棒性和视觉质量之间取得良好的平衡，本地代码转换器中的迭代被设计为确定每个视频的最小量化步长。然后，提出了一种选择鲁棒视频帧的策略，以进一步提高鲁棒性和安全性。为了避免发送者和接收者之间事先共享信息，建立隐写辅助信道用于正确的消息提取。实验结果表明，该方法对社交网络转码具有很强的鲁棒性，平均误码率小于1%。同时，我们提出的方法获得了令人满意的安全性能。这是一种在YouTube和Vimeo等社交网站上进行秘密交流的强大而安全的方法。",
                    "title_zh": "基于隐写侧信道的抗社交网络转码的鲁棒视频隐写方法"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395074",
                    "title": "JPEG Steganography and Synchronization of DCT Coefficients for a Given Development Pipeline",
                    "authors": "Théo Taburet, Patrick Bas, Wadih Sawaya, Rémi Cogranne",
                    "abstract": "This paper proposes to use the statistical analysis of the correlation between DCT coefficients to design a new synchronization strategy that can be used for cost-based steganographic schemes in the JPEG domain. First, an analysis is performed on the covariance matrix of DCT coefficients of neighboring blocks after a development pipeline similar to the one used to generate BossBase, and applied on a photonic noise. This analysis exhibits (i) a decomposition into 8 disjoint sets of uncorrelated coefficients (4 sets per block used by 2 disjoint lattices) and (ii) the fact that each DCT coefficient is correlated with 38 other coefficients belonging either to the same block or to connected blocks. Using the uncorrelated groups, an embedding scheme can be designed using only 8 disjoint lattices. The proposed embedding scheme relies on ingredients. Firstly, we convert the empirical costs associated to one each coefficient into a Gaussian distribution whose variance is directly computed from the embedding costs. Secondly we derive conditional Gaussian distributions from a multivariate distribution considering only the correlated coefficients which have been already modified by the embedding scheme. This covariance matrix takes into account both the correlations exhibited by the analysis of the covariance matrix and the variance derived from the costs. This synchronization scheme enables to obtain a gain of $P_E$ of at least $7%$ at $QF95$ for an embedding rate close to 0.3 bnzac coefficient using DCTR feature sets for both UERD and JUniward.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-02553023/file/IH-woCopy.pdf"
                    },
                    "abstract_zh": "本文提出利用DCT系数相关性的统计分析来设计一种新的同步策略，该策略可用于JPEG域中基于代价的隐写方案。首先，在类似于用于生成BossBase的开发流水线之后，对相邻块的DCT系数的协方差矩阵进行分析，并应用于光子噪声。这种分析展示了(I)分解成8个不相关系数的不相交集合(2个不相交网格使用的每个块4个集合)和(ii)每个DCT系数与属于同一块或连接块的38个其他系数相关的事实。使用不相关的群，可以仅使用8个不相交的格来设计嵌入方案。所提出的嵌入方案依赖于成分。首先，我们将与每个系数相关的经验成本转换为高斯分布，其方差直接由嵌入成本计算得出。其次，我们从多元分布中导出条件高斯分布，只考虑已经被嵌入方案修改的相关系数。该协方差矩阵考虑了由协方差矩阵的分析显示的相关性和从成本导出的方差。对于接近0.3 bnzac系数的嵌入率，使用UERD和JUniward的DCTR特征集，该同步方案能够获得至少7%$的$P_E$增益。",
                    "title_zh": "用于给定开发流水线的JPEG隐写术和DCT系数的同步"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395065",
                    "title": "Turning Cost-Based Steganography into Model-Based",
                    "authors": "Jan Butora, Yassine Yousfi, Jessica J. Fridrich",
                    "abstract": "Abstract Most modern steganographic schemes embed secrets by minimizing the total expected cost of modifications. However, costs are usually computed using heuristics and cannot be directly linked to statistical detectability. Moreover, as previously shown by Ker at al., cost-based schemes fundamentally minimize the wrong quantity that makes them more vulnerable to knowledgeable adversary aware of the embedding change rates. In this paper, we research the possibility to convert cost-based schemes to model-based ones by postulating that there exists payload size for which the change rates derived from costs coincide with change rates derived from some (not necessarily known) model. This allows us to find the steganographic Fisher information for each pixel (DCT coefficient), and embed other payload sizes by minimizing deflection. This rather simple measure indeed brings sometimes quite significant improvements in security especially with respect to steganalysis aware of the selection channel. Steganographic algorithms in both spatial and JPEG domains are studied with feature-based classifiers as well as CNNs.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3369412.3395065"
                    },
                    "abstract_zh": "摘要大多数现代隐写方案通过最小化修改的总期望成本来嵌入秘密。然而，成本通常是使用试探法计算的，不能直接与统计可检测性联系起来。此外，如先前由Ker在al .基于成本的方案从根本上最小化错误的数量，这使得它们更容易受到知道嵌入变化率的有知识的对手的攻击。在本文中，我们研究了将基于成本的方案转换为基于模型的方案的可能性，通过假设存在有效载荷大小，对于该有效载荷大小，从成本导出的变化率与从某个(不一定已知的)模型导出的变化率一致。这允许我们找到每个像素的隐写Fisher信息(DCT系数)，并通过最小化偏转来嵌入其他有效载荷大小。这种相当简单的措施有时确实带来了相当显著的安全性改进，特别是对于知道选择信道的隐写分析。利用基于特征的分类器和细胞神经网络研究了空域和JPEG域的隐写算法。",
                    "title_zh": "将基于成本的隐写术转变为基于模型的隐写术"
                },
                {
                    "url": "https://doi.org/10.1145/3369412.3395075",
                    "title": "Steganography by Minimizing Statistical Detectability: The cases of JPEG and Color Images",
                    "authors": "Rémi Cogranne, Quentin Giboulot, Patrick Bas",
                    "abstract": "This short paper presents a novel method for steganography in JPEG-compressed images, extended the so-called MiPOD scheme based on minimizing the detection accuracy of the most-powerful test using a Gaussian model of independent DCT coefficients. This method is also applied to address the problem of embedding into color JPEG images. The main issue in such case is that color channels are not processed in the same way and, hence, a statistically based approach is expected to bring significant improvements when one needs to consider heterogeneous channels together. The results presented show that, on the one hand, the extension of MiPOD for JPEG domain, referred to as J-MiPOD, is very competitive as compared to current state-of-the-art embedding schemes. On the other hands, we also show that addressing the problem of embedding in JPEG color images is far from being straightforward and that future works are required to understand better how to deal with color channels in JPEG images.",
                    "files": {
                        "openAccessPdf": "https://hal-utt.archives-ouvertes.fr/hal-02542075/file/J_MiPOD_vPub.pdf"
                    },
                    "abstract_zh": "本文提出了一种在JPEG压缩图像中进行隐写的新方法，扩展了所谓的MiPOD方案，该方案基于使用独立DCT系数的高斯模型最小化最强大测试的检测精度。这种方法也适用于解决嵌入到彩色JPEG图像的问题。这种情况下的主要问题是颜色通道不以相同的方式处理，因此，当需要一起考虑异类通道时，基于统计的方法有望带来显著的改进。给出的结果表明，一方面，与当前最先进的嵌入方案相比，JPEG域的MiPOD扩展(称为J-MiPOD)是非常有竞争力的。另一方面，我们还表明，解决JPEG彩色图像中的嵌入问题远非易事，未来的工作需要更好地理解如何处理JPEG图像中的颜色通道。",
                    "title_zh": "最小化统计可检测性的隐写术:JPEG和彩色图像的情况"
                }
            ]
        }
    ],
    "2019": [
        {
            "dblp_url": "https://dblp.uni-trier.de/db/conf/ih/ihmmsec2019.html",
            "conf_title": "7th IH&MMSec 2019: Paris, France",
            "conf_url": "https://doi.org/10.1145/3335203",
            "papers": [
                {
                    "url": "https://doi.org/10.1145/3335203.3335707",
                    "title": "Reverse Engineering: What Can We Learn From a Digital Image About Its Own History ?",
                    "authors": "Jean-Michel Morel",
                    "abstract": "This keynote presentation reviews the algorithms able to analyse a digital image and able to retrieve part of its processing history.This problem is relevant because more and more images happen to have lost their native EXIF metadata. In this presentation is described several tools gathering information about an image's compression, resampling, cropping, its gamma correction and its demosaicing process. This information may be used to detect images manipulations and sometimes its tampering. A common denominator of all detection tools is that they need a false alarms control. I'll illustrate how a false alarm rate can be rigorously associated to each detection. Joint work with Quentin Bammey, Miguel Colom, Thibaud Ehret, Rafael Grompone, Tina Nikoukhah",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "本主题演讲回顾了能够分析数字图像并能够检索其部分处理历史的算法。这个问题是相关的，因为越来越多的图像碰巧丢失了它们的原生EXIF元数据。本演示介绍了几种收集图像压缩、重采样、裁剪、伽玛校正和去马赛克过程信息的工具。该信息可用于检测图像操作，有时是篡改。所有检测工具的一个共同点是它们需要一个错误警报控制。我将说明错误警报率是如何与每个检测严格关联的。与昆廷·巴姆米、米盖尔·科洛姆、提博·埃雷特、拉斐尔·贡庞内、蒂娜·尼库哈合作",
                    "title_zh": "逆向工程:我们能从数字图像中了解其自身的历史吗？"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335708",
                    "title": "Recurrent Convolutional Neural Networks for AMR Steganalysis Based on Pulse Position",
                    "authors": "Chen Gong, Xiaowei Yi, Xianfeng Zhao, Yi Ma",
                    "abstract": "With the rapid development of stream multimedia, the adaptive multi-rate (AMR) audio steganography are emerging recently. However, the traditional steganalysis methods face great challenges in detecting short time speech at low embedding rates. To address this problem, we propose a steganalytic scheme by combining Recurrent Neural Network (RNN) and Convolutional Neural Network (CNN), SRCNet. AMR fixed codebook (FCB) steganography embed messages by modifying the pulse positions, which would destroy the FCB correlation. Firstly we analyzed the FCB correlations at different distances, and summarized these correlations into four categories. Furthermore, we utilizes RNN to extract higher level contextual representations of FCBs and CNN to fuse spatial-temporal features for the steganalysis. The proposed approach was evaluated on a public data-set. The experiment results validate that the proposed framework greatly outperforms the existing state-of-the-art methods. The correct detection rate of SRCNet has been improved above at least 10% when the sample is as short as 100ms at the 20% embedding rate. In particular, the network achieves the significant improvements for detecting the STCs based adaptive AMR steganography.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "随着流媒体的快速发展，自适应多码率音频隐写技术应运而生。然而，传统的隐写分析方法在检测低嵌入率的短时语音时面临巨大挑战。为了解决这个问题，我们提出了一种结合递归神经网络(RNN)和卷积神经网络(CNN)的隐写分析方案。AMR固定码本(FCB)隐写术通过修改脉冲位置来嵌入信息，这会破坏FCB相关性。首先我们分析了不同距离下的FCB相关性，并将这些相关性归纳为四类。此外，我们利用RNN提取fcb和CNN的高层上下文表示，融合时空特征进行隐写分析。在公共数据集上对所提出的方法进行了评估。实验结果表明，该框架明显优于现有的方法。在20%的嵌入率下，当样本短至100ms时，SRCNet的正确检测率至少提高了10%以上。特别地，该网络在检测基于STCs的自适应AMR隐写术方面实现了显著的改进。",
                    "title_zh": "基于脉冲位置的递归卷积神经网络AMR隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335710",
                    "title": "Defining Joint Embedding Distortion for Adaptive MP3 Steganography",
                    "authors": "Yunzhao Yang, Yuntao Wang, Xiaowei Yi, Xianfeng Zhao, Yi Ma",
                    "abstract": "In this paper, a universal joint embedding distortion function (JED) is proposed to improve the undetectability and imperceptibility of MP3 steganography, which can be applied to Huffman codeword mapping (HCM) and sign bit flipping (SBF). Content-aware and statistical distortions are synthetically modeled to formulate the atom modification of the quantified modified discrete cosine transform (QMDCT) coefficients. On the one hand, to retain the hearing imperceptibility, the absolute threshold of hearing is employed to measure the auditory sensitivity of each QMDCT coefficient. On the other hand, considering most of the existing universal MP3 steganalysis features are designed based on correlations, the forward and backward transition probability are utilized to characterize the correlations between adjacent QMDCT coefficients. What's more, we present an implementation of JED in sign bits domain. Experimental results demonstrate that our method is able to achieve higher embedding capacity and better imperceptibility. The detection accuracy of the proposed scheme is about 75% with the bitrate of 320kbps and embedding rate of 11kbit/s, which is respectively decreased by 9.54% ~ 16.94% than existing MP3 steganographic methods.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "提出了一种通用联合嵌入失真函数(JED)来提高MP3隐写术的不可检测性和不可感知性，该函数可应用于霍夫曼码字映射(HCM)和符号位翻转(SBF)。内容感知和统计失真被综合建模，以制定量化的修改离散余弦变换(QMDCT)系数的原子修改。一方面，为了保持听觉的不可感知性，采用绝对听觉阈值来度量每个QMDCT系数的听觉灵敏度。另一方面，考虑到现有的大多数通用MP3隐写分析特征都是基于相关性设计的，因此利用向前和向后转移概率来表征相邻QMDCT系数之间的相关性。此外，我们还给出了一个符号位域的JED实现。实验结果表明，该方法能够获得更高的嵌入容量和更好的不可感知性。在比特率为320kbps、嵌入速率为11kbit/s的情况下，该方案的检测准确率约为75%，比现有的MP3隐写方法分别降低了9.54% ~ 16.94%。",
                    "title_zh": "为自适应MP3隐写术定义联合嵌入失真"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335711",
                    "title": "Adaptive VP8 Steganography Based on Deblocking Filtering",
                    "authors": "Pei Xie, Hong Zhang, Weike You, Xianfeng Zhao, Jianchang Yu, Yi Ma",
                    "abstract": "In this paper, a novel deblocking filtering-based VP8 steganographic scheme is proposed. The unique aspect of this work and one that distinguishes it from the prior art is that we effectively exploit the characteristics of deblocking filtering. We propose to embed the secret messages by comparing the quantized discrete cosine transform coefficients before and after the in-loop filtering. In the process of encoding, given one frame, first, we encode it to obtain the quantized discrete cosine transform coefficients. Second, a new set of coefficients is obtained by re-encoding the filtered frame. Third, the distortion function is defined by comparing the difference between the two sets of coefficients. Finally, adaptive embedding is realized by using the syndrome-trellis codes. Experimental results show that satisfactory levels of visual quality and steganographic security could be achieved with adequate payloads.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "提出了一种新的基于去块滤波的VP8隐写方案。这项工作的独特之处以及与现有技术的区别在于，我们有效地利用了去块滤波的特性。我们建议通过比较环路滤波前后的量化离散余弦变换系数来嵌入秘密消息。在编码过程中，给定一帧，首先对其进行编码，得到量化的离散余弦变换系数。第二，通过对滤波后的帧进行重新编码来获得一组新的系数。第三，通过比较两组系数之间的差异来定义失真函数。最后，利用伴随式网格码实现自适应嵌入。实验结果表明，在有效载荷足够的情况下，可以获得令人满意的视觉质量和隐写安全性。",
                    "title_zh": "基于去块滤波的自适应VP8隐写术"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335712",
                    "title": "Vibrational Covert Channels using Low-Frequency Acoustic Signals",
                    "authors": "Nikolay Matyunin, Yujue Wang, Stefan Katzenbeisser",
                    "abstract": "In this paper, we examine how acoustic signals in sub-bass and infrasonic range can be used to establish a vibrational covert channel between speaker-equipped computers and mobile devices. We show that typical consumer speakers are capable of producing low-frequency sounds, which are not perceivable by humans. At the same time, we show that producing such sounds by the speaker's woofer inevitably generates slight vibrations of the speaker and the surface where it is located. Being unnoticeable to people, such vibrations can be captured by the accelerometer sensor of a mobile device located on the same surface. Therefore, information can be encoded into low-frequency sounds played by a speaker and received on a mobile device by analyzing the produced vibrations. Note that access to the accelerometer on modern mobile devices does not require any user permissions, making the transmission completely unnoticeable. We evaluate the presented covert channel for different speakers, apply it to several application scenarios, and give an overview of possible countermeasures.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在本文中，我们研究了如何利用次低音和次声范围内的声学信号在配有扬声器的计算机和移动设备之间建立振动隐蔽通道。我们发现，典型的消费类扬声器能够发出人类无法感知的低频声音。同时，我们表明扬声器的低音扬声器产生这样的声音不可避免地会产生扬声器及其所在表面的轻微振动。由于人们不会注意到，这种振动可以被位于同一表面上的移动设备的加速度计传感器捕获。因此，通过分析产生的振动，可以将信息编码成由扬声器播放的低频声音，并在移动设备上接收。请注意，在现代移动设备上访问加速度计不需要任何用户许可，因此传输完全不会被注意到。我们针对不同的说话者评估了所提出的隐蔽信道，将其应用于几个应用场景，并给出了可能的对策的概述。",
                    "title_zh": "使用低频声信号的振动隐蔽通道"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335713",
                    "title": "Towards Automatic Embedding Cost Learning for JPEG Steganography",
                    "authors": "Jianhua Yang, Danyang Ruan, Xiangui Kang, Yun-Qing Shi",
                    "abstract": "Current mainstream methods for digital image steganography are content adaptive. That is, the secret messages are embedded in the complicated region in the cover image while minimizing the embedding distortion so as to suppress statistical detectability. Since there is already a practical encoding scheme for data embedding near the payload-distortion bound, the design of the embedding cost function becomes a deterministic part in steganography. Unlike the traditional heuristic hand-crafted method, this paper proposes a novel generative adversarial network based framework to automatically learn the embedding cost function for JPEG steganography. The proposed framework consists of a generator, a gradient-descent friendly inverse discrete cosine transformation module, an embedding simulator and a discriminator for steganalysis. Through training the generator and discriminator in alternation, the embedding cost function can finally be obtained by the trained generator. Experimental results demonstrate that our method can automatically learn a reasonable embedding cost function and achieve a satisfying performance.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "当前主流的数字图像隐写方法是内容自适应的。也就是说，秘密消息被嵌入到载体图像的复杂区域中，同时最小化嵌入失真，以便抑制统计可检测性。由于在有效载荷-失真界限附近已经有一种实用的数据嵌入编码方案，嵌入代价函数的设计成为隐写术中决定性的部分。与传统的启发式手工方法不同，本文提出了一种新的基于生成对抗网络的框架来自动学习JPEG隐写的嵌入代价函数。该框架由生成器、梯度下降友好的反离散余弦变换模块、嵌入模拟器和隐写分析鉴别器组成。通过交替训练生成器和鉴别器，由训练好的生成器最终得到嵌入代价函数。实验结果表明，该方法能够自动学习合理的嵌入代价函数，取得令人满意的性能。",
                    "title_zh": "面向JPEG隐写术的自动嵌入代价学习"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335714",
                    "title": "Effect of JPEG Quality on Steganographic Security",
                    "authors": "Jan Butora, Jessica J. Fridrich",
                    "abstract": "Abstract This work investigates both theoretically and experimentally the security of JPEG steganography as a function of the quality factor. For a fixed relative payload, modern embedding schemes, such as J-UNIWARD and UED-JC, exhibit surprising non-monotone trends due to rounding and clipping of quantization steps. Their security generally increases with increasing quality factor but starts decreasing for qualities above 95. In contrast, old-fashion steganography, such as Jsteg, OutGuess, and model-based steganography, exhibit complementary trends. The results of empirical detectors closely match the trends exhibited by the KL divergence computed between models of cover and stego DCT modes. In particular, our analysis shows that the main reason for the complementary trends is the way modern schemes attenuate embedding change rates with increasing spatial frequency. Our model also provides guidance on how to adjust the embedding algorithm J-UNIWARD to substantially improve its security for high quality factors.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3335203.3335714"
                    },
                    "abstract_zh": "摘要本文从理论和实验两方面研究了JPEG隐写术的安全性与质量因子的关系。对于固定的相对有效载荷，现代嵌入方案，如J-UNIWARD和UED-JC，由于量化步长的舍入和限幅，表现出令人惊讶的非单调趋势。它们的安全性通常随着品质因数的增加而增加，但是当品质因数超过95时开始降低。相比之下，旧式隐写术，如Jsteg、OutGuess和基于模型的隐写术，表现出互补的趋势。经验检测器的结果与在覆盖和隐写DCT模式之间计算的KL散度所展示的趋势非常匹配。特别是，我们的分析表明，互补趋势的主要原因是现代方案随着空间频率的增加而衰减嵌入变化率的方式。我们的模型还提供了关于如何调整嵌入算法J-UNIWARD的指导，以充分提高其对高品质因子的安全性。",
                    "title_zh": "JPEG质量对隐写安全性的影响"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335715",
                    "title": "Computing Dependencies between DCT Coefficients for Natural Steganography in JPEG Domain",
                    "authors": "Théo Taburet, Patrick Bas, Jessica J. Fridrich, Wadih Sawaya",
                    "abstract": "This short paper is an extension of a family of embedding schemes called Natural Steganography, which embeds a message by mimicking heteroscedastic sensor noise in the JPEG domain. Under the assumption that the development from RAW uses linear de- mosaicking, we derive a closed-form for the covariance matrix of DCT coefficients from 3 × 3 JPEG blocks. This computation relies on a matrix formulation of all steps involved in the development pipeline, which includes demosaicking, conversion to luminance, DCT transform, and reordering. This matrix is then used for pseudo-embedding in the JPEG domain on four lattices of 8 × 8 DCT blocks. The results obtained with the computed covariance matrix are contrasted with the results previously obtained with the covariance matrix estimated using Monte Carlo sampling and scaling. The empirical security using DCTR features at JPEG quality 100 increased from PE = 14% using covariance estimation and scaling to PE = 43% using the newly derived analytic form.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-02165866v2/file/Camera_Ready_V1.pdf"
                    },
                    "abstract_zh": "这篇短文是一个称为自然隐写术的嵌入方案家族的扩展，它通过模仿JPEG域中的异方差传感器噪声来嵌入消息。在从RAW开发使用线性去马赛克的假设下，我们从3 × 3 JPEG块导出DCT系数的协方差矩阵的封闭形式。该计算依赖于开发管道中涉及的所有步骤的矩阵公式，包括去马赛克、亮度转换、DCT变换和重新排序。这个矩阵然后被用于在JPEG域中的4个8 × 8 DCT块的格子上的伪嵌入。使用计算的协方差矩阵获得的结果与先前使用蒙特卡罗采样和缩放估计的协方差矩阵获得的结果形成对比。使用JPEG质量100的DCTR特征的经验安全性从使用协方差估计和缩放的PE = 14%增加到使用新导出的解析形式的PE = 43%。",
                    "title_zh": "JPEG域自然隐写的DCT系数相关性计算"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335716",
                    "title": "Enhancing Steganography via Stego Post-processing by Reducing Image Residual Difference",
                    "authors": "Bolin Chen, Weiqi Luo, Peijia Zheng",
                    "abstract": "Most modern steganography methods focus on designing an effective cost function. To our best knowledge, there is no related works concerned about modifying stego to enhance steganography security. In this paper, therefore, we propose a novel post-processing for stego image in the spatial domain. To ensure the correct extraction of hidden message, our method restricts the modification amplitude of each pixel according to the characteristics of STCs (Syndrome-Trellis Codes). To enhance steganography security, our method traverses the stego image pixel by pixel, and modifies those pixels that can reduce the image residual difference between cover and stego under some criterion. Experimental results show that the proposed method can improve the security of current steganography especially for large payloads, e.g. larger than 0.3 bpp. In addition, the post-modification rate is rather low, for instance less than 8 \\textpertenthousand \\ pixels have been changed in the enhanced stego image for the five existing steganography methods for payload as large as 0.5 bpp.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "大多数现代隐写术方法集中于设计有效的成本函数。据我们所知，目前还没有关于修改隐写术以增强隐写术安全性的相关工作。因此，本文提出了一种新的空域隐写图像后处理方法。为了保证隐藏信息的正确提取，该方法根据STCs (Syndrome-Trellis Codes)的特性来限制每个像素的修改幅度。为了增强隐写术的安全性，该方法对隐写图像进行逐像素遍历，并在一定准则下对那些能够减小载体和隐写图像残差的像素进行修改。实验结果表明，该方法可以提高现有隐写术的安全性，特别是对于大载荷，例如大于0.3 bpp。此外，后修改率相当低，例如，对于大至0.5 bpp的有效载荷，在五种现有隐写术方法的增强隐写图像中，改变的像素少于8个。",
                    "title_zh": "通过降低图像残差的隐写后处理增强隐写术"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335717",
                    "title": "Camera Identification from HDR Images",
                    "authors": "Morteza Darvish Morshedi Hosseini, Miroslav Goljan",
                    "abstract": "Performance of camera identification methods based on PRNU is very sensitive to geometric operations applied to images during acquisition and processing. Handling images that have been geometrically transformed, such as rotated, downsampled, and/or cropped requires overcoming pixel desynchronization problem. This work expands applicability of camera identification methods based on PRNU to the class of HDR images. Geometric transformations in HDR images revealed in this work are reversed in a series of steps involving block-wise PRNU matching. Efficiency of this method is then tested on HDR images from publicly available UNIFI dataset spanning 26 cameras of mobile devices.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "基于PRNU的相机识别方法的性能对采集和处理期间应用于图像的几何操作非常敏感。处理经过几何变换的图像，例如旋转、下采样和/或裁剪，需要克服像素去同步问题。这项工作将基于PRNU的相机识别方法的适用性扩展到HDR图像类别。这项工作揭示的HDR图像中的几何变换在一系列涉及块式PRNU匹配的步骤中被逆转。然后，在跨越移动设备的26个相机的公开可用的UNIFI数据集的HDR图像上测试该方法的效率。",
                    "title_zh": "从HDR图像中识别摄像机"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335718",
                    "title": "Estimation of Copy-sensitive Codes Using a Neural Approach",
                    "authors": "Rohit Yadav, Iuliia Tkachenko, Alain Trémeau, Thierry Fournel",
                    "abstract": "Copy sensitive graphical codes are used as anti-counterfeiting solution in packaging and document protection. Their security is funded on a design hard-to-predict after print and scan. In practice there exist different designs. Here random codes printed at the printer resolution are considered. We suggest an estimation of such codes by using neural networks, an in-trend approach which has however not been studied yet in the present context. In this paper, we test a state-of-the-art architecture efficient in the binarization of handwritten characters. The results show that such an approach can be successfully used by an attacker to provide a valid counterfeited code so fool an authentication system.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-02330988/file/IH_MMSec_2019_final.pdf"
                    },
                    "abstract_zh": "复制敏感图形代码被用作包装和文件保护中的防伪解决方案。他们的安全是建立在打印和扫描后难以预测的设计上的。实际上存在不同的设计。这里考虑以打印机分辨率打印的随机码。我们建议通过使用神经网络来估计这种代码，这是一种趋势方法，但是在当前上下文中还没有研究过。在这篇论文中，我们测试了一种在手写字符二值化中有效的最新架构。结果表明，攻击者可以成功地使用这种方法来提供有效的伪造代码，从而欺骗认证系统。",
                    "title_zh": "用神经方法估计复制敏感码"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335719",
                    "title": "De-identification Without Losing Faces",
                    "authors": "Yuezun Li, Siwei Lyu",
                    "abstract": "Training of deep learning models for computer vision requires large image or video datasets from real world. Often, in collecting such datasets, we also need to protect the privacy of the people captured in the images or videos, while still preserve useful attributes such as facial expressions. In this work, we describe a new face de-identification method to achieve this, which is based on a face attribute transfer model (FATM). FATM is a deep neural network model trained to map non-identity related facial attributes to the face of donors, who are a small number of consented subjects. Using the donors' faces ensures the natural appearance of the synthesized faces, and FATM blends the donors' facial attributes to those of the original faces to diversify the appearance of the synthesized faces. Experimental results on several sets of images and videos demonstrate the effectiveness of our face de-ID algorithm.",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/1902.04202"
                    },
                    "abstract_zh": "用于计算机视觉的深度学习模型的训练需要来自真实世界的大型图像或视频数据集。通常，在收集这样的数据集时，我们还需要保护图像或视频中捕捉到的人的隐私，同时仍然保留有用的属性，如面部表情。在这项工作中，我们描述了一种新的人脸去识别方法来实现这一点，这是基于人脸属性转移模型(FATM)。FATM是一个深度神经网络模型，被训练为将非身份相关的面部属性映射到捐献者的面部，捐献者是少数同意的受试者。使用捐赠者的面部确保了合成面部的自然外观，FATM将捐赠者的面部属性与原始面部的属性相融合，以使合成面部的外观多样化。在几组图像和视频上的实验结果证明了该算法的有效性。",
                    "title_zh": "不丢面子的去身份化"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335720",
                    "title": "Impact of Spatial Constraints when Signing in Uncontrolled Mobile Conditions",
                    "authors": "Majd Abazid, Nesma Houmani, Sonia Garcia-Salicetti",
                    "abstract": "In this paper, we study the impact of uncontrolled mobile conditions on signatures, when signing on two touch-screen devices of different size. 74 persons captured their signatures on both an iPad and an iPhone with different signing areas. The study exploited several quantitative indicators: an intra-personal variability measure, a statistical quality measure, and verification performance with two classifiers. We show that for 69% of writers, their signatures are more complex when made on the large surface of the iPad. This result reflects that the majority of writers are less comfortable with signing on a small handheld device with a very constrained signing surface; besides, the used rubber-tipped stylus does not offer an optimal writing precision regarding the constrained signing surface. For the remaining writers, results show that when signing on the iPad, writers tend to fill out, spontaneously, the whole available space; this leads to information loss reflected by a reduced signature complexity and stability. Performance assessment shows a significant degradation when test and reference signatures are not captured on the same platform. When test and reference signatures are captured on the same platform, performance is better when the writer signs on the small surface available on the iPhone.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在本文中，我们研究了在两个不同大小的触摸屏设备上签名时，不受控制的移动条件对签名的影响。74个人用不同的签名区在iPad和iPhone上抓拍他们的签名。该研究利用了几个定量指标:一个个人内部的可变性测量，一个统计质量测量，以及两个分类器的验证性能。我们发现，对于69%的作者来说，他们的签名在iPad的大表面上更复杂。该结果反映了大多数书写者不太习惯于在具有非常受限的签名表面的小型手持设备上签名；此外，所使用的橡胶尖触针不能提供关于受限的签名表面的最佳书写精度。对于其余的写手，结果显示，当在iPad上签名时，写手倾向于自发地填满整个可用空间；这导致信息丢失，表现为签名复杂性和稳定性降低。性能评估显示，当测试和参考签名不在同一平台上捕获时，性能会显著下降。当测试签名和参考签名在同一个平台上被捕获时，当作者在iPhone上可用的小表面上签名时，性能会更好。",
                    "title_zh": "在不受控制的移动条件下签名时空间限制的影响"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335721",
                    "title": "A Face Morphing Detection Concept with a Frequency and a Spatial Domain Feature Space for Images on eMRTD",
                    "authors": "Tom Neubert, Christian Kraetzer, Jana Dittmann",
                    "abstract": "Since the face morphing attack was introduced by Ferrara et al. in 2014, the detection of face morphings has become a wide spread topic in image forensics. By now, the community is very active and has reported diverse detection approaches. So far, the evaluations are mostly performed on images without post-processing. Face images stored within electronic machine readable documents (eMRTD) are ICAO-passport-scaled to a resolution of 413x531 and a JPG or JP2 lesize of 15 kilobytes. This paper introduces a face morphing detection concept with 3 modules (ICAO-aligned pre- processing module, feature extraction module and classi cation module), tailored for such images on eMRTD. In this work we exemplary design and evaluate two feature spaces for the feature extraction module, a frequency domain and a spatial domain feature space. Our evaluation will compare both feature spaces and is carried out with 66,229 passport-scaled images (64,363 morphed face images and 1,866 authentic face images) which are completly independent from training and include all images provided for the IHMMSEC'19 special session: \"Media Forensics - Fake or Real?\". Furthermore, we investigate the in uence of di erent morph gen- eration pipelines to the detection accuracies of the concept and we analyse the impact of neutral and smiling genuine faces to the morph detector performance. The evaluation determines a detection rate of 86.0% for passport-scaled morphed images with a false alarm rate of 4.4% for genuine images for the spatial domain feature space",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3335203.3335721"
                    },
                    "abstract_zh": "自从Ferrara等人在2014年提出人脸变形攻击以来，人脸变形的检测已经成为图像取证领域的一个热门话题。到目前为止，该团体非常活跃，并报告了多种检测方法。到目前为止，评价大多是在没有后处理的图像上进行的。存储在电子机器可读文档(eMRTD)中的面部图像被ICAO-passport缩放到413x531的分辨率和15千字节的JPG或JP2大小。本文介绍了一个人脸变形检测概念，它有三个模块(ICAO-aligned预处理模块、特征提取模块和分类模块),适用于eMRTD上的这类图像。在这项工作中，我们为特征提取模块示例性地设计和评估了两个特征空间，频域和空间域特征空间。我们的评估将比较这两个特征空间，并使用66，229个护照比例的图像(64，363个变形人脸图像和1，866个真实人脸图像)进行，这些图像完全独立于训练，并包括为IHMMSEC'19特别会议提供的所有图像:“媒体取证-假还是真？”。此外，我们研究了不同的形态生成管道对该概念的检测精度的影响，并分析了中性和微笑的真实人脸对形态检测器性能的影响。对于空间域特征空间，评估确定护照缩放的变形图像的检测率为86.0%，而真实图像的虚警率为4.4%",
                    "title_zh": "eMRTD上基于频域和空域特征空间的人脸变形检测概念"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335722",
                    "title": "Image Forensics from Chroma Subsampling of High-Quality JPEG Images",
                    "authors": "Benedikt Lorch, Christian Riess",
                    "abstract": "The JPEG compression format provides a rich source of forensic traces that include quantization artifacts, fingerprints of the container format, and numerical particularities of JPEG compressors. Such a diverse set of cues serves as the basis for a forensic examiner to determine origin and authenticity of an image. In this work, we present a novel artifact that can be used to fingerprint the JPEG compression library. The artifact arises from chroma subsampling in one of the most popular JPEG implementations. Due to integer rounding, every second column of the compressed chroma channel appears on average slightly brighter than its neighboring columns, which is why we call the artifact a \"chroma wrinkle\". We theoretically derive the chroma wrinkle footprint in DCT domain, and use this footprint for detecting chroma wrinkles. The artifact is detected with more than 90% accuracy on images of JPEG quality 75 and above. Our experiments indicate that the artifact can also be used for manipulation localization, and that it is robust to several global postprocessing operations.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "JPEG压缩格式提供了丰富的取证痕迹来源，包括量化伪像、容器格式的指纹和JPEG压缩器的数字特征。这样一组不同的线索作为法医检验者确定图像来源和真实性的基础。在这项工作中，我们提出了一个新的工件，可用于指纹JPEG压缩库。伪像来自最流行的JPEG实现之一的色度二次采样。由于整数舍入，压缩色度通道的每第二列平均起来比其相邻列稍微亮一些，这就是为什么我们将伪像称为“色度褶皱”。我们从理论上推导出DCT域中的色度皱纹足迹，并使用该足迹来检测色度皱纹。在JPEG质量75及以上的图像上，伪像检测的准确率超过90%。我们的实验表明，该伪影还可以用于操纵定位，并且对于若干全局后处理操作是鲁棒的。",
                    "title_zh": "高质量JPEG图像色度子采样的图像取证"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335723",
                    "title": "A Simple and Effective Initialization of CNN for Forensics of Image Processing Operations",
                    "authors": "Ivan Castillo Camacho, Kai Wang",
                    "abstract": "In this paper we present a simple yet effective initialization method for convolutional neural networks (CNNs). The proposed method extends the well-known Xavier initialization and can cope well with CNNs used for forensic detection of image processing operations. Our initialization inherits the simplicity and advantages of the Xavier initialization, and the difference is that our method generates a set of high-pass filters for the initialization of CNN's first layer. This allows us to better identify forensic traces which usually lie towards the high-frequency part of the image. We test the proposed method with two CNNs for two forensic problems, i.e., a multiclass classification problem of a group of image processing operations and a median filtering forensic problem with JPEG post-processing. Experimental results show the utility of our initialization.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "本文提出了一种简单而有效的卷积神经网络初始化方法。所提出的方法扩展了众所周知的Xavier初始化，并且可以很好地处理用于图像处理操作的法医检测的CNN。我们的初始化继承了Xavier初始化的简单性和优点，不同的是我们的方法为CNN的第一层初始化生成了一组高通滤波器。这使我们能够更好地识别通常位于图像高频部分的法医痕迹。我们用两个CNN对两个取证问题进行了测试，即一组图像处理操作的多类分类问题和JPEG后处理的中值滤波取证问题。实验结果显示了我们的初始化的效用。",
                    "title_zh": "一种用于图像处理操作取证的简单有效的CNN初始化方法"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335724",
                    "title": "Exposing GAN-synthesized Faces Using Landmark Locations",
                    "authors": "Xin Yang, Yuezun Li, Honggang Qi, Siwei Lyu",
                    "abstract": "Generative adversary networks (GANs) have recently led to highly realistic image synthesis results. In this work, we describe a new method to expose GAN-synthesized images using the locations of the facial landmark points. Our method is based on the observations that the facial parts configuration generated by GAN models are different from those of the real faces, due to the lack of global constraints. We perform experiments demonstrating this phenomenon, and show that an SVM classifier trained using the locations of facial landmark points is sufficient to achieve good classification performance for GAN-synthesized faces.",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/1904.00167"
                    },
                    "abstract_zh": "生成对手网络(GANs)最近已经产生了高度逼真的图像合成结果。在这项工作中，我们描述了一种新的方法来曝光氮化镓合成图像使用的位置面部标志点。我们的方法是基于这样的观察:由于缺乏全局约束，由GAN模型生成的面部部分配置不同于真实面部的配置。我们进行了演示这一现象的实验，并表明使用面部标志点的位置训练的SVM分类器足以实现对GAN合成的面部的良好分类性能。",
                    "title_zh": "使用标志点位置暴露GAN合成的面"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335725",
                    "title": "CNN-based Rescaling Factor Estimation",
                    "authors": "Chang Liu, Matthias Kirchner",
                    "abstract": "We demonstrate the estimation of image resampling parameters in a deep learning framework by regressing the rescaling factor of 64x64-sized patches on features learned directly from grayscale intensities by a convolutional neural network (CNN). Our end-to-end network design comprises a simple concatenation of 25 convolutional layers with small 3x3 receptive fields and largely abstains from the use of pooling. We report experimental results on a large set of rescaled patches, for which the proposed CNN outperforms state-of-the-art frequency-domain estimators particularly in the case of downscaling. A critical discussion of sensitivities to mismatch between training and testing data points to failure cases.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "我们演示了在深度学习框架中对图像重采样参数的估计，方法是通过卷积神经网络(CNN)对直接从灰度强度学习的特征回归64x64大小的补丁的重缩放因子。我们的端到端网络设计包括25个卷积层与小的3×3感受域的简单级联，并且很大程度上避免使用池。我们报告了在一个大的重缩放面片集合上的实验结果，对于该集合，所提出的CNN优于最先进的频域估计器，特别是在向下缩放的情况下。对训练和测试数据之间不匹配的敏感性的关键讨论指向失败案例。",
                    "title_zh": "基于CNN的重标度因子估计"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335726",
                    "title": "The ALASKA Steganalysis Challenge: A First Step Towards Steganalysis",
                    "authors": "Rémi Cogranne, Quentin Giboulot, Patrick Bas",
                    "abstract": "This paper presents ins and outs of the ALASKA challenge, a steganalysis challenge built to reflect the constraints of a forensic steganalyst. We motivate and explain the main differences w.r.t. the BOSS challenge (2010), specifically the use of a ranking metric prescribing high false positive rates, the analysis of a large diversity of different image sources and the use of a collection of steganographic schemes adapted to handle color JPEGs. The core of the challenge is also described, this includes the RAW image data-set, the implementations used to generate cover images and the specificities of the embedding schemes. The very first outcomes of the challenge are then presented, and the impacts of different parameters such as demosaicking, filtering, image size, JPEG quality factors and cover-source mismatch are analyzed. Eventually, conclusions are presented, highlighting positive and negative points together with future directions for the next challenges in practical steganalysis.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-02147763/file/ALASKA_lesson_learn_Vsubmitted.pdf"
                    },
                    "abstract_zh": "本文介绍了阿拉斯加挑战的来龙去脉，这是一项隐写分析挑战，旨在反映法医隐写分析师的局限性。我们激发并解释了与BOSS challenge (2010)的主要区别，特别是使用了一个规定高假阳性率的排名指标，分析了大量不同的图像来源，并使用了一组适合处理彩色JPEGs的隐写方案。还描述了挑战的核心，这包括原始图像数据集、用于生成封面图像的实现以及嵌入方案的特性。然后介绍了挑战的最初结果，并分析了不同参数的影响，如去马赛克、滤波、图像大小、JPEG质量因子和覆盖源失配。最后，给出了结论，强调了积极和消极的观点，以及在实际隐写分析中未来挑战的方向。",
                    "title_zh": "阿拉斯加隐写分析挑战:隐写分析的第一步"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335727",
                    "title": "Breaking ALASKA: Color Separation for Steganalysis in JPEG Domain",
                    "authors": "Yassine Yousfi, Jan Butora, Jessica J. Fridrich, Quentin Giboulot",
                    "abstract": "This paper describes the architecture and training of detectors developed for the ALASKA steganalysis challenge. For each quality factor in the range 60-98, several multi-class tile detectors implemented as SRNets were trained on various combinations of three input channels: luminance and two chrominance channels. To accept images of arbitrary size, the detector for each quality factor was a multi-class multi-layered perceptron trained on features extracted by the tile detectors. For quality 99 and 100, a new \"reverse JPEG compatibility attack\" was developed and also implemented using the SRNet via the tile detector. Throughout the paper, we explain various improvements we discovered during the course of the competition and discuss the challenges we encountered and trade offs that had to be adopted in order to build a detector capable of detecting steganographic content in a stego source of great diversity.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3335203.3335727"
                    },
                    "abstract_zh": "本文描述了为阿拉斯加隐写分析挑战赛开发的检测器的架构和训练。对于范围60-98中的每个质量因子，实现为SRNets的几个多类瓦片检测器在三个输入通道的各种组合上被训练:亮度和两个色度通道。为了接受任意大小的图像，每个质量因子的检测器是一个多类多层感知器，它根据瓦片检测器提取的特征进行训练。对于quality 99和100，开发了一种新的“反向JPEG兼容性攻击”,并且也通过瓦片检测器使用SRNet来实现。在整篇论文中，我们解释了我们在比赛过程中发现的各种改进，并讨论了我们遇到的挑战和必须采取的权衡，以便构建一种能够检测各种隐写源中的隐写内容的检测器。",
                    "title_zh": "打破阿拉斯加:JPEG域隐写分析的颜色分离"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335728",
                    "title": "Trusting Machine Learning: Privacy, Robustness, and Transparency Challenges",
                    "authors": "Reza Shokri",
                    "abstract": "Machine learning algorithms have shown an unprecedented predictive power for many complex learning tasks. As they are increasingly being deployed in large scale critical applications for processing various types of data, new questions related to their trustworthiness would arise. Can machine learning algorithms be trusted to have access to individuals’ sensitive data? Can they be robust against noisy or adversarially perturbed data? Can we reliably interpret their learning process, and explain their predictions? In this talk, I will go over the challenges of building trustworthy machine learning algorithms in centralized and distributed (federated) settings, and will discuss the inter-relation between privacy, robustness, and interpretability.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "机器学习算法对许多复杂的学习任务显示出前所未有的预测能力。随着它们越来越多地被部署在用于处理各种类型数据的大规模关键应用中，与它们的可信度相关的新问题将会出现。机器学习算法可以被信任访问个人的敏感数据吗？它们能抵抗噪音或恶意干扰的数据吗？我们能可靠地解释他们的学习过程，并解释他们的预测吗？在这次演讲中，我将回顾在集中式和分布式(联邦)环境中构建可信的机器学习算法所面临的挑战，并将讨论隐私性、健壮性和可解释性之间的相互关系。",
                    "title_zh": "信任机器学习:隐私、健壮性和透明性挑战"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335729",
                    "title": "Proving Multimedia Integrity using Sanitizable Signatures Recorded on Blockchain",
                    "authors": "Karthik Nandakumar, Nalini K. Ratha, Sharathchandra Pankanti",
                    "abstract": "While significant advancements have been made in the field of multimedia forensics to detect altered content, existing techniques mostly focus on enabling the content recipient to verify the content integrity without any inputs from the content creator. In many application scenarios, the creator has a strong incentive to establish the provenance and integrity of the multimedia data created and released by him. Hence, there is a strong need for mechanisms that allow the content creator to prove the authenticity of the released content. Since blockchain technology provides an immutable distributed database, it is an ideal solution for reliably time-stamping content with its creation time and storing an irrefutable signature of the content at the time of its creation. However, a simple digital signature scheme does not allow modification of the content after the initial commitment. Authorized multimedia content alteration by its creator is often necessary (e.g., redaction of faces to protect the privacy of individuals in a video, redaction of sensitive fields in a text document) before the content is distributed. The main contributions of this paper are: (i) a novel sanitizable signature scheme that enables the content creator to prove the integrity of the redacted content, while preventing the recipients from reconstructing the redacted segments based on the published commitment, and (ii) a blockchain-based solution for securely managing the sanitizable signature. The proposed solution employs a robust hashing scheme using chameleon hash function and Merkle tree to generate the initial signature, which is stored on the blockchain. The auxiliary data required for the integrity verification step is retained by the content creator and only a signature of this auxiliary data is stored on the blockchain. Any modifications to the multimedia content requires only updating the signature of the auxiliary data, which is securely recorded on the blockchain. We demonstrate that the proposed approach enables verification of integrity of redacted multimedia content without compromising the content privacy requirements.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "虽然在多媒体取证领域中已经取得了显著的进步来检测更改的内容，但是现有技术主要集中在使内容接收者能够在没有来自内容创建者的任何输入的情况下验证内容完整性。在许多应用场景中，创建者有强烈的动机来建立由他创建和发布的多媒体数据的出处和完整性。因此，非常需要允许内容创建者证明发布内容的真实性的机制。由于区块链技术提供了一个不可变的分布式数据库，因此它是一个理想的解决方案，可以可靠地用内容的创建时间为内容打上时间戳，并在内容创建时存储无可辩驳的内容签名。然而，简单的数字签名方案不允许在初始提交后修改内容。在分发内容之前，通常需要由创建者对多媒体内容进行授权更改(例如，编辑面孔以保护视频中个人的隐私，编辑文本文档中的敏感字段)。本文的主要贡献是:(1)一种新颖的可净化签名方案，它使内容创建者能够证明编辑内容的完整性，同时防止接收者基于发布的承诺重构编辑的片段；以及(2)一种基于区块链的安全管理可净化签名的解决方案。所提出的解决方案采用了一种健壮的散列方案，该方案使用变色龙散列函数和Merkle树来生成初始签名，该签名存储在区块链上。完整性验证步骤所需的辅助数据由内容创建者保留，并且只有该辅助数据的签名存储在区块链上。对多媒体内容的任何修改只需要更新安全地记录在区块链上的辅助数据的签名。我们证明了所提出的方法能够在不损害内容隐私要求的情况下验证编辑的多媒体内容的完整性。",
                    "title_zh": "使用区块链上记录的可清理签名证明多媒体完整性"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335730",
                    "title": "Revisiting Multivariate Lattices for Encrypted Signal Processing",
                    "authors": "Alberto Pedrouzo-Ulloa, Juan Ramón Troncoso-Pastoriza, Fernando Pérez-González",
                    "abstract": "Multimedia contents are inherently sensitive signals that must be protected when processed in untrusted environments. The field of Secure Signal Processing addresses this challenge by developing methods which enable operating with sensitive signals in a privacy-conscious way. Recently, we introduced a hard lattice problem called m-RLWE (multivariate Ring Learning with Errors) which gives support to efficient encrypted processing of multidimensional signals. Afterwards, Bootland et al. presented an attack to m-RLWE that reduces the security of the underlying scheme from a lattice with dimension \\prod_in_i to \\max\\n_i\\ _i . Our work introduces a new pre-/post-coding block that addresses this attack and achieves the efficient results of our initial approach while basing its security directly on RLWE with dimension \\prod_in_i, hence preserving the security and efficiency originally claimed. Additionally, this work provides a detailed comparison between a conventional use of RLWE, m-RLWE and our new pre-/post-coding procedure, which we denote \"packed''-RLWE. Finally, we discuss a set of encrypted signal processing applications which clearly benefit from the proposed framework, either alone or in a combination of baseline RLWE, m-RLWE and \"packed''-RLWE.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "多媒体内容是固有的敏感信号，在不受信任的环境中处理时必须加以保护。安全信号处理领域通过开发能够以保护隐私的方式处理敏感信号的方法来应对这一挑战。最近，我们引入了一个称为m-RLWE(带误差的多元环学习)的硬格问题，它为多维信号的有效加密处理提供了支持。随后，Bootland等人提出了对m-RLWE的攻击，将基础方案的安全性从维数为\\prod_in_i的格降低到\\max\\n_i\\ _i。我们的工作引入了一种新的预/后编码块来解决这种攻击，并实现了我们最初方法的有效结果，同时其安全性直接基于维数为\\prod_in_i的RLWE，因此保持了最初声称的安全性和效率。此外，这项工作提供了一个详细的比较之间的传统使用RLWE，m-RLWE和我们的新的前/后编码程序，我们称之为“压缩”& RLWE。最后，我们讨论了一组加密信号处理应用，它们明显受益于所提出的框架，或者单独使用，或者与基线RLWE、m-RLWE和“打包”& RLWE组合使用。",
                    "title_zh": "再访用于加密信号处理的多元格"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335731",
                    "title": "Watermarking Error Exponents in the Presence of Noise: The Case of the Dual Hypercone Detector",
                    "authors": "Teddy Furon",
                    "abstract": "The study of the error exponents of zero-bit watermarking is addressed in the article by Comesana, Merhav, and Barni, under the assumption that the detector relies solely on second order joint empirical statistics of the received signal and the watermark. This restriction leads to the well-known dual hypercone detector, whose score function is the absolute value of the normalized correlation. They derive the false negative error exponent and the optimum embedding rule. However, they only focus on high SNR regime, i.e. the noiseless scenario. This paper extends this theoretical study to the noisy scenario. It introduces a new definition of watermarking robustness based on the false negative error exponent, derives this quantity for the dual hypercone detector, and shows that its performances is almost equal to Costa's lower bound.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-02122206/file/ErrorExponents.pdf"
                    },
                    "abstract_zh": "在Comesana、Merhav和Barni的文章中，在检测器仅依赖于接收信号和水印的二阶联合经验统计的假设下，对零比特水印的误差指数进行了研究。这种限制导致了著名的双超圆锥检测器，其得分函数是归一化相关性的绝对值。他们导出了假负误差指数和最佳嵌入规则。然而，它们只关注高SNR区域，即无噪声场景。本文将这一理论研究扩展到噪声场景。引入了一种新的基于假负误差指数的水印鲁棒性定义，推导了对偶超锥检测器的假负误差指数，并证明了它的性能几乎等于Costa的下界。",
                    "title_zh": "噪声存在下的水印误差指数:双超圆锥检测器的情况"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335732",
                    "title": "Nearest Neighbor Decoding for Tardos Fingerprinting Codes",
                    "authors": "Thijs Laarhoven",
                    "abstract": "Over the past decade, various improvements have been made to Tardos' collusion-resistant fingerprinting scheme [Tardos, STOC 2003], ultimately resulting in a good understanding of what is the minimum code length required to achieve collusion-resistance. In contrast, decreasing the cost of the actual decoding algorithm for identifying the potential colluders has received less attention, even though previous results have shown that using joint decoding strategies, deemed too expensive for decoding, may lead to better code lengths. Moreover, in dynamic settings a fast decoder may be required to provide answers in real-time, further raising the question whether the decoding costs of score-based fingerprinting schemes can be decreased with a smarter decoding algorithm. In this paper we show how to model the decoding step of score-based fingerprinting as a nearest neighbor search problem, and how this relation allows us to apply techniques from the field of (approximate) nearest neighbor searching to obtain decoding times which are sublinear in the total number of users. As this does not affect the encoding and embedding steps, this decoding mechanism can easily be deployed within existing fingerprinting schemes, and this may bring a truly efficient joint decoder closer to reality. Besides the application to fingerprinting, similar techniques can potentially be used to decrease the decoding costs of group testing methods, which may be of independent interest.",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/1902.06196"
                    },
                    "abstract_zh": "在过去的十年中，对Tardos的抗共谋指纹方案进行了各种改进[Tardos，STOC 2003]，最终导致对实现抗共谋所需的最小码长的良好理解。相比之下，降低用于识别潜在共谋的实际解码算法的成本受到的关注较少，尽管先前的结果已经表明，使用被认为解码成本太高的联合解码策略可以导致更好的码长。此外，在动态设置中，可能需要快速解码器来实时提供答案，这进一步提出了基于分数的指纹方案的解码成本是否可以通过更智能的解码算法来降低的问题。在本文中，我们展示了如何将基于分数的指纹识别的解码步骤建模为最近邻搜索问题，以及这种关系如何允许我们应用来自(近似)最近邻搜索领域的技术来获得在用户总数中次线性的解码时间。由于这不影响编码和嵌入步骤，这种解码机制可以容易地部署在现有的指纹方案中，并且这可以使真正有效的联合解码器更接近现实。除了指纹识别的应用之外，类似的技术可以潜在地用于降低分组测试方法的解码成本，这可能是独立的兴趣所在。",
                    "title_zh": "Tardos指纹码的最近邻解码"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335733",
                    "title": "Reference Channels for Steganalysis of Images with Convolutional Neural Networks",
                    "authors": "Mo Chen, Mehdi Boroumand, Jessica J. Fridrich",
                    "abstract": "When available, reference signals may dramatically improve the accuracy of steganalysis. Particularly powerful reference signals are embedding invariants that exist when the steganographic algorithm swaps values from small disjoint subsets of the cover elements' dynamic range, such as, but not limited to, embedding schemes utilizing least significant bit replacement. This paper describes a general method how to prepare such reference signals for a certain type of embedding operations, and incorporate them in detectors built as convolutional networks to improve their detection accuracy. The beneficial effect of reference signals is shown experimentally in both the spatial and especially JPEG domain, on model-based steganography and a generic LSB flipper with and without stochastic restoration of the histogram (OutGuess).",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3335203.3335733"
                    },
                    "abstract_zh": "当参考信号可用时，可以显著提高隐写分析的准确性。特别强大的参考信号是当隐写算法交换来自覆盖元素动态范围的小的不相交子集的值时存在的嵌入不变量，例如但不限于利用最低有效位替换的嵌入方案。本文描述了一种通用方法，即如何为某种类型的嵌入操作准备这样的参考信号，并将它们合并到作为卷积网络构建的检测器中，以提高它们的检测精度。在基于模型的隐写术和具有和不具有直方图随机恢复(OutGuess)的一般LSB翻转上，参考信号的有益效果在空间域和特别是JPEG域中被实验性地示出。",
                    "title_zh": "卷积神经网络图像隐写分析的参考通道"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335734",
                    "title": "A Customized Convolutional Neural Network with Low Model Complexity for JPEG Steganalysis",
                    "authors": "Junwen Huang, Jiangqun Ni, Linhong Wan, Jingwen Yan",
                    "abstract": "Nowadays, convolutional neural network (CNN) is appied to different types of image classification tasks and outperforms almost all traditional methods. However, one may find it difficult to apply CNN to JPEG steganalysis because of the extremely low SNR (embedding messages to image contents) in the task. In this paper, a selection-channel-aware CNN for JPEG steganalysis is proposed by incorporating domain knowledge. Specifically, instead of random strategy, kernels of the first convolutional layer are initialized with hand-crafted filters to suppress the image content. Then, truncated linear unit (TLU), a heuristically-designed activation function, is adopted in the first layer as the activation function to better adapt to the distribution of feature maps. Finally, we use a generalized residual learning block to incorporate the knowledge of selection channel in the proposed CNN to further boost its performance. J-UNIWARD, a state-of-the-art JPEG steganographic scheme, is used to evaluate the performance of the proposed CNN and other competing JPEG steganalysis methods. Experiment results show that the proposed CNN steganalyzer outperforms other feature-based methods and rivals the state-of-the-art CNN-based methods with much reduced model complexity, at different payloads.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "目前，卷积神经网络(CNN)已被应用于不同类型的图像分类任务，其性能优于几乎所有的传统方法。然而，人们可能会发现很难将CNN应用于JPEG隐写分析，因为该任务中的SNR(将消息嵌入到图像内容中)极低。结合领域知识，提出了一种选择通道感知的CNN用于JPEG隐写分析。具体地说，代替随机策略，第一卷积层的内核用手工制作的滤波器初始化，以抑制图像内容。然后，在第一层采用启发式设计的激活函数——截断线性单元(TLU)作为激活函数，以更好地适应特征图的分布。最后，我们使用一个广义剩余学习块将选择信道的知识加入到所提出的细胞神经网络中，以进一步提高其性能。J-UNIWARD是一种最新的JPEG隐写方案，用于评估所提出的CNN和其他竞争JPEG隐写分析方法的性能。实验结果表明，在不同的有效载荷下，所提出的CNN隐写分析器的性能优于其他基于特征的方法，并且可以与最先进的基于CNN的方法相媲美，同时大大降低了模型复杂度。",
                    "title_zh": "用于JPEG隐写分析的低模型复杂度定制卷积神经网络"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335735",
                    "title": "Steganalysis of VoIP Streams with CNN-LSTM Network",
                    "authors": "Hao Yang, Zhongliang Yang, Yongfeng Huang",
                    "abstract": "Steganalysis of the Quantization Index Modulation (QIM) steganography in VoIP (Voice-over IP) stream is conducted in this research. VoIP is a popular media streaming and communication service on the Internet. QIM steganography makes it possible to hide secret information in VoIP streams. Detecting short and low embedding rates of QIM steganography samples remains an unsolved challenge. Recently, neural network models have been demonstrated to be capable of achieving remarkable performances and be successfully applied to many different tasks. The mainstream architectures of neural network include Convolution Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), which adopt totally different ways to understand various signals. In this paper, we first indicate a proper way to combine the strengths of these two architectures and then construct a novel and unified model called CNN-LSTM network to detect QIM-based steganography. In our model, Bidirectional Long Short-Term Memory Recurrent Neural Network (Bi-LSTM) is utilized to capture long time contextual information in carriers and CNN was used subsequently to capture both local features and global ones as well as temporal carrier features. Experiments showed that our model can achieve the state-of-art result in detecting QIM-based steganography in VoIP streams.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "本研究对VoIP (Voice-over IP)流中的量化索引调制(QIM)隐写术进行隐写分析。VoIP是互联网上流行的媒体流和通信服务。QIM隐写术使得在VoIP流中隐藏秘密信息成为可能。检测短和低嵌入率的QIM隐写样本仍然是一个未解决的挑战。最近，神经网络模型已被证明能够实现显著的性能，并成功地应用于许多不同的任务。神经网络的主流架构包括卷积神经网络(CNN)和递归神经网络(RNNs)，它们采用完全不同的方式来理解各种信号。在本文中，我们首先指出了一种结合这两种架构优势的合适方法，然后构建了一个新的统一模型，称为CNN-LSTM网络，用于检测基于QIM的隐写术。在我们的模型中，双向长短时记忆递归神经网络(Bi-LSTM)用于捕获载体中的长时间上下文信息，CNN随后用于捕获局部特征和全局特征以及时间载体特征。实验表明，我们的模型在检测VoIP流中基于QIM的隐写术方面可以达到最先进的效果。",
                    "title_zh": "基于CNN-LSTM网络的VoIP流隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335736",
                    "title": "Audio Steganalysis with Improved Convolutional Neural Network",
                    "authors": "Yuzhen Lin, Rangding Wang, Diqun Yan, Li Dong, Xueyuan Zhang",
                    "abstract": "Deep learning, especially the convolutional neural network (CNN), has enjoyed significant success in many fields, e.g., image recognition. Recently, CNN has successfully applied to multimedia steganalysis. However, the detection performance is still unsatisfactory. In this work, we propose an improved CNN-based method for audio steganalysis. Specifically, a special convolutional layer is first carefully designed, which could capture the minor steganographic noise. Then, a truncated linear unit is adapted to activate the output of shallow convolutional layer. In addition, we employ the average pooling to minimize the over-fitting risk. Finally, a parameter transfer strategy is adopted, aiming to boost the detection performance for the low embedding-rate cases. The experimental results evaluated on 30,000 audio clips verify the effectiveness of our method for a variety of embedding rates. Compared with the existing CNN-based steganalysis methods, our proposed method could achieve superior performance. To facilitate the reproducible research, the source code will be released at GitHub.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "深度学习，尤其是卷积神经网络(CNN)，已经在许多领域，例如图像识别，取得了重大成功。最近，CNN已经成功应用于多媒体隐写分析。然而，检测性能仍然不能令人满意。本文提出了一种改进的基于CNN的音频隐写分析方法。具体来说，首先仔细设计一个特殊的卷积层，它可以捕获微小的隐写噪声。然后，截断线性单元适于激活浅卷积层的输出。此外，我们采用平均池来最小化过拟合风险。最后，采用一种参数转移策略，旨在提高低嵌入率情况下的检测性能。在30，000个音频片段上评估的实验结果验证了我们的方法对于各种嵌入率的有效性。与现有的基于CNN的隐写分析方法相比，我们提出的方法具有更好的性能。为了促进可重复的研究，源代码将在GitHub上发布。",
                    "title_zh": "基于改进卷积神经网络的音频隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335737",
                    "title": "Exploiting Adversarial Embeddings for Better Steganography",
                    "authors": "Solène Bernard, Tomás Pevný, Patrick Bas, John Klein",
                    "abstract": "This work proposes a protocol to iteratively build a distortion function for adaptive steganography while increasing its practical security after each iteration. It relies on prior art on targeted attacks and iterative design of steganalysis schemes. It combines targeted attacks on a given detector with a \\min\\max strategy, which dynamically selects the most difficult stego content associated with the best classifier at each iteration. We theoretically prove the convergence, which is confirmed by the practical results. Applied on J-Uniward this new protocol increases \\perr from 7% to 20% estimated by Xu-Net, and from 10% to 23% for a non-targeted steganalysis by a linear classifier with GFR features.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-02177259/file/main_final_HAL.pdf"
                    },
                    "abstract_zh": "该工作提出了一种协议来迭代地建立自适应隐写术的失真函数，同时在每次迭代后增加其实际安全性。它依赖于目标攻击的现有技术和隐写分析方案的迭代设计。它将对给定检测器的定向攻击与最小最大策略相结合，在每次迭代中动态选择与最佳分类器相关的最困难的隐写内容。我们从理论上证明了收敛性，实际结果也证实了这一点。在J-Uniward上应用该新协议时，根据Xu-Net的估计，perr从7%增加到20%,对于通过具有GFR特征的线性分类器进行的非目标隐写分析，perr从10%增加到23%。",
                    "title_zh": "利用对抗性嵌入实现更好的隐写术"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335738",
                    "title": "Detection of Classifier Inconsistencies in Image Steganalysis",
                    "authors": "Daniel Lerch-Hostalot, David Megías",
                    "abstract": "In this paper, a methodology to detect inconsistencies in classification-based image steganalysis is presented. The proposed approach uses two classifiers: the usual one, trained with a set formed by cover and stego images, and a second classifier trained with the set obtained after embedding additional random messages into theoriginal training set. When the decisions of these two classifiers are not consistent, we know that the prediction is not reliable. The number of inconsistencies in the predictions of a testing set may indicate that the classifier is not performing correctly in the testing scenario. This occurs, for example, in case of cover source mismatch,or when we are trying to detect a steganographic method that theclassifier is no capable of modelling accurately. We also show how the number of inconsistencies can be used to predict the reliability of the classifier (classification errors).",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/1909.10278"
                    },
                    "abstract_zh": "提出了一种检测基于分类的图像隐写分析中不一致性的方法。该方法使用两个分类器:一个是通常的分类器，用载体图像和隐写图像组成的集合进行训练；另一个分类器用在原始训练集中嵌入额外的随机信息后得到的集合进行训练。当这两个分类器的决策不一致时，我们知道预测不可靠。测试集的预测中不一致的数量可以指示分类器在测试场景中没有正确执行。例如，在覆盖源不匹配的情况下，或者当我们试图检测分类器不能准确建模的隐写方法时，就会发生这种情况。我们还展示了不一致性的数量如何用于预测分类器的可靠性(分类错误)。",
                    "title_zh": "图像隐写分析中分类器不一致性的检测"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335739",
                    "title": "Fast and Effective Global Covariance Pooling Network for Image Steganalysis",
                    "authors": "Xiaoqing Deng, Bolin Chen, Weiqi Luo, Da Luo",
                    "abstract": "Recently, deep learning based methods have achieved superior performance compared to conventional methods based on hand-crafted features in image steganalysis. However, most modern methods are usually quite time consuming. For instance, it takes over 3 days to train a state-of-the-art neural network, i.e. SRNet [3] in our experiments. In this paper, therefore, we propose a fast yet very effective convolutional neural network (CNN) for image steganalysis in spatial domain. To make a good tradeoff between training time and performance, we carefully design the architecture of the proposed network according to our extensive experiments. In addition, we first introduce the global covariance pooling into steganalysis to exploit the second-order statistic of high-level features for further improving the performance. Experimental results show that the proposed network can outperform the current best one, while its training time is significantly reduced.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "最近，基于深度学习的方法在图像隐写分析中取得了优于基于手工制作特征的传统方法的性能。然而，大多数现代方法通常相当耗时。例如，在我们的实验中，训练一个最先进的神经网络，即SRNet [3]需要3天以上的时间。因此，在本文中，我们提出了一种快速而有效的卷积神经网络(CNN)用于空域图像隐写分析。为了在训练时间和性能之间取得良好的平衡，我们根据大量的实验仔细设计了所提出的网络的体系结构。此外，我们首次将全局协方差池引入隐写分析，利用高层特征的二阶统计量来进一步提高性能。实验结果表明，该网络的性能优于目前最好的网络，同时训练时间显著减少。",
                    "title_zh": "快速有效的全局协方差池网络图像隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/3335203.3335740",
                    "title": "Linear Hash Functions as a Means of Distortion-Rate Optimization in Data Embedding",
                    "authors": "Boris Ryabko, Andrey Fionov",
                    "abstract": "Embedding hidden data is usually performed by introducing some distortions (errors) in cover objects. If the distortions exceed a certain bound, steganalysis can detect the presence of hidden data. So the problem is to embed as much data as possible and not exceed a permissible distortion level to ensure indetectability. We describe a general class of stegosystems that solves the problem by employing linear hash functions. The suggested stegosystems allow to transmit hidden information of the amount asymptotically close to the maximum possible under the given distortion.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "嵌入隐藏数据通常通过在覆盖对象中引入一些失真(错误)来执行。如果失真超过一定界限，隐写分析可以检测隐藏数据的存在。因此，问题是嵌入尽可能多的数据，并且不超过允许的失真水平，以确保不可检测性。我们描述了一类通过使用线性散列函数来解决问题的隐写系统。所建议的隐写系统允许在给定失真下传输接近最大可能量的隐藏信息。",
                    "title_zh": "作为数据嵌入中失真率优化手段的线性散列函数"
                }
            ]
        }
    ],
    "2017": [
        {
            "dblp_url": "https://dblp.uni-trier.de/db/conf/ih/ihmmsec2017.html",
            "conf_title": "5. IH&MMSec 2017: Philadelphia, PA, USA",
            "conf_url": "https://doi.org/10.1145/3082031",
            "papers": [
                {
                    "url": "https://doi.org/10.1145/3082031.3092568",
                    "title": "Every Move You Make: Tracking Smartphone Users through Motion Sensors",
                    "authors": "Anupam Das",
                    "abstract": "Online users are increasingly being subjected to privacy-invasive tracking across the web for advertisement and surveillance purposes, using IP addresses, cookies, and browser fingerprinting. As web browsing activity shifts to mobile platforms such as smartphones, traditional browser fingerprinting techniques become less effective due to ephemeral IP addresses and uniform software-base. However, device fingerprinting using built-in sensors offers a new avenue for attack. In this talk, I will describe how motion sensors such as accelerometer and gyroscope, embedded in smartphones, can be exploited to track users online. Next, I will discuss the practical aspects of this attack and how it can be used to track users across different sessions under natural web browsing settings. Finally, I will talk about usable countermeasures that we have developed to protect users against such fingerprinting techniques.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "出于广告和监视目的，在线用户越来越多地受到侵犯隐私的跟踪，使用IP地址、cookies和浏览器指纹。随着网络浏览活动转移到智能手机等移动平台，由于短暂的IP地址和统一的软件基础，传统的浏览器指纹技术变得不那么有效。然而，使用内置传感器的设备指纹为攻击提供了新的途径。在本次演讲中，我将介绍如何利用智能手机中嵌入的加速度计和陀螺仪等运动传感器来跟踪在线用户。接下来，我将讨论这种攻击的实际方面，以及如何使用它在自然的web浏览设置下跨不同会话跟踪用户。最后，我将谈一谈我们为保护用户免受这种指纹技术攻击而开发的可用对策。",
                    "title_zh": "你的一举一动:通过运动传感器追踪智能手机用户"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083233",
                    "title": "Information-theoretic Bounds of Resampling Forensics: New Evidence for Traces Beyond Cyclostationarity",
                    "authors": "Cecilia Pasquini, Rainer Böhme",
                    "abstract": "Although several methods have been proposed for the detection of resampling operations in multimedia signals and the estimation of the resampling factor, the fundamental limits for this forensic task leave open research questions. In this work, we explore the effects that a downsampling operation introduces in the statistics of a 1D signal as a function of the parameters used. We quantify the statistical distance between an original signal and its downsampled version by means of the Kullback-Leibler Divergence (KLD) in case of a wide-sense stationary 1st-order autoregressive signal model. Values of the KLD are derived for different signal parameters, resampling factors and interpolation kernels, thus predicting the achievable hypothesis distinguishability in each case. Our analysis reveals unexpected detectability in case of strong downsampling due to the local correlation structure of the original signal. Moreover, since existing detection methods generally leverage the cyclostationarity of resampled signals, we also address the case where the autocovariance values are estimated directly by means of the sample autocovariance from the signal under investigation. Under the considered assumptions, the Wishart distribution models the sample covariance matrix of a signal segment and the KLD under different hypotheses is derived.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "虽然已经提出了几种方法来检测多媒体信号中的重采样操作和估计重采样因子，但是这种取证任务的基本限制留下了未解决的研究问题。在这项工作中，我们探索了下采样操作在1D信号的统计中引入的效应，作为所用参数的函数。在广义平稳一阶自回归信号模型的情况下，我们通过Kullback-Leibler散度(KLD)来量化原始信号与其下采样版本之间的统计距离。对于不同的信号参数、重采样因子和插值核，导出KLD的值，从而预测在每种情况下可实现的假设可分辨性。由于原始信号的局部相关结构，我们的分析揭示了在强下采样的情况下意外的可检测性。此外，由于现有的检测方法通常利用重采样信号的循环平稳性，所以我们还解决了通过来自被研究信号的样本自协方差来直接估计自协方差值的情况。在所考虑的假设下，Wishart分布模拟信号段的样本协方差矩阵，并且导出不同假设下的KLD。",
                    "title_zh": "重采样取证的信息论界限:超越循环平稳的痕迹的新证据"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083242",
                    "title": "Countering Anti-Forensics of Lateral Chromatic Aberration",
                    "authors": "Owen Mayer, Matthew C. Stamm",
                    "abstract": "Research has shown that lateral chromatic aberrations (LCA), an imaging fingerprint, can be anti-forensically modified to hide evidence of cut-and-paste forgery. In this paper, we propose a new technique for securing digital images against anti-forensic manipulation of LCA. To do this, we exploit resizing differences between color channels, which are induced by LCA anti-forensics, and define a feature vector to quantitatively capture these differences. Furthermore, we propose a detection method that exposes anti-forensically manipulated image patches. The technique algorithm is validated through experimental procedure, showing dependence on forgery patch size as well as anti-forensic scaling factor.",
                    "files": {
                        "openAccessPdf": "http://dl.acm.org/ft_gateway.cfm?id=3083242&type=pdf"
                    },
                    "abstract_zh": "研究表明，横向色差(LCA)，一种成像指纹，可以被反法医修改，以隐藏剪切粘贴伪造的证据。在本文中，我们提出了一种新的技术来保护数字图像免受反取证操纵的生命周期分析。为此，我们利用由LCA反取证引起的颜色通道之间的尺寸调整差异，并定义一个特征向量来定量捕捉这些差异。此外，我们提出了一种检测方法，暴露反法医操纵的图像补丁。该技术算法通过实验程序进行验证，显示出对伪造补丁大小以及反取证比例因子的依赖性。",
                    "title_zh": "应对横向色差的反取证"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083244",
                    "title": "Modeling Attacks on Photo-ID Documents and Applying Media Forensics for the Detection of Facial Morphing",
                    "authors": "Christian Kraetzer, Andrey Makrushin, Tom Neubert, Mario Hildebrandt, Jana Dittmann",
                    "abstract": "Since 2014, a novel approach to attack face image based person verification designated as face morphing attack has been actively discussed in the biometric and media forensics communities. Up until that point, modern travel documents were considered to be extremely hard to forge or to successfully manipulate. In the case of template-targeting attacks like facial morphing, the face verification process becomes vulnerable, making it a necessity to design protection mechanisms. In this paper, a new modeling approach for face morphing attacks is introduced. We start with a life-cycle model for photo-ID documents. We extend this model by an image editing history model, allowing for a precise description of attack realizations as a foundation for performing media forensics as well as training and testing scenarios for the attack detectors. On the basis of these modeling approaches, two different realizations of the face morphing attack as well as a forensic morphing detector are implemented and evaluated. The design of the feature space for the detector is based on the idea that the blending operation in the morphing pipeline causes the reduction of face details. To quantify this reduction, we adopt features implemented in the OpenCV image processing library, namely the number of SIFT, SURF, ORB, FAST and AGAST keypoints in the face region as well as the loss of edge-information with Canny and Sobel edge operators. Our morphing detector is trained with 2000 self-acquired authentic and 2000 morphed images captured with three camera types (Canon EOS 1200D, Nikon D 3300, Nikon Coolpix A100) and tested with authentic and morphed face images from a public database. Morphing detection accuracies of a decision tree classifier vary from 81.3% to 98% for different training and test scenarios.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "自2014年以来，一种被称为人脸变形攻击的基于人脸图像的个人验证的新方法一直在生物识别和媒体取证社区中积极讨论。在此之前，现代旅行证件被认为极难伪造或成功篡改。在像面部变形这样的模板目标攻击的情况下，面部验证过程变得脆弱，这使得设计保护机制成为必要。介绍了一种新的人脸变形攻击建模方法。我们从带有照片的身份证件的生命周期模型开始。我们通过图像编辑历史模型扩展了该模型，允许精确描述攻击实现，作为执行媒体取证以及攻击检测器的训练和测试场景的基础。在这些建模方法的基础上，实现并评估了人脸变形攻击的两种不同实现以及取证变形检测器。检测器的特征空间的设计基于变形流水线中的混合操作导致人脸细节减少的思想。为了量化这种减少，我们采用OpenCV图像处理库中实现的特征，即面部区域中SIFT、SURF、ORB、FAST和AGAST关键点的数量，以及Canny和Sobel边缘算子的边缘信息损失。我们的变形检测器使用三种相机类型(佳能EOS 1200D、尼康D 3300、尼康Coolpix A100)捕获的2000幅自我获取的真实和2000幅变形图像进行训练，并使用来自公共数据库的真实和变形人脸图像进行测试。对于不同的训练和测试场景，决策树分类器的变形检测准确率从81.3%到98%不等。",
                    "title_zh": "对照片身份文档的攻击建模和应用媒体取证检测面部变形"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083235",
                    "title": "The Square Root Law of Steganography: Bringing Theory Closer to Practice",
                    "authors": "Andrew D. Ker",
                    "abstract": "There are two interpretations of the term \"square root law of steganography\". As a rule of thumb, that the secure capacity of an imperfect stegosystem scales only with the square root of the cover size (not linearly as for perfect stegosystems), it acts as a robust guide in multiple steganographic domains. As a mathematical theorem, it is unfortunately limited to artificial models of covers that are a long way from real digital media objects: independent pixels or first-order stationary Markov chains. It is also limited to models of embedding where the changes are uniformly distributed and, for the most part, independent. This paper brings the theoretical square root law closer to the practice of digital media steganography, by extending it to cases where the covers are Markov Random Fields, including inhomogeneous Markov chains and Ising models. New proof techniques are required. We also consider what a square root law should say about adaptive embedding, where the changes are not uniformly located, and state a conjecture.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "“隐写术的平方根定律”这个术语有两种解释。根据经验，不完美的隐写系统的安全容量仅与覆盖大小的平方根成比例(与完美的隐写系统不同，不是线性的)，它在多个隐写域中充当稳健的向导。作为一个数学定理，它不幸地局限于与真实数字媒体对象相距甚远的封面的人工模型:独立像素或一阶平稳马尔可夫链。它还局限于嵌入模型，其中变化是均匀分布的，并且在很大程度上是独立的。通过将平方根定律扩展到覆盖为马尔可夫随机场的情况，包括非齐次马尔可夫链和伊辛模型，使理论平方根定律更接近数字媒体隐写术的实践。需要新的证明技术。我们还考虑了平方根定律对自适应嵌入应该说些什么，其中变化不是均匀分布的，并陈述了一个猜想。",
                    "title_zh": "隐写术的平方根定律:使理论更接近实践"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083239",
                    "title": "Nonlinear Feature Normalization in Steganalysis",
                    "authors": "Mehdi Boroumand, Jessica J. Fridrich",
                    "abstract": "In this paper, we propose a method for normalization of rich feature sets to improve detection accuracy of simple classifiers in steganalysis. It consists of two steps: 1) replacing random subsets of empirical joint probability mass functions (co-occurrences) by their conditional probabilities and 2) applying a non-linear normalization to each element of the feature vector by forcing its marginal distribution over covers to be uniform. We call the first step random conditioning and the second step feature uniformization. When applied to maxSRMd2 features in combination with simple classifiers, we observe a gain in detection accuracy across all tested stego algorithms and payloads. For better insight, we investigate the gain for two image formats. The proposed normalization has a very low computational complexity and does not require any feedback from the stego class.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "本文提出了一种丰富特征集的归一化方法，以提高简单分类器在隐写分析中的检测精度。它包括两个步骤:1)用它们的条件概率替换经验联合概率质量函数(共现)的随机子集，以及2)通过强制其在覆盖上的边缘分布是均匀的，对特征向量的每个元素应用非线性归一化。我们称第一步为随机调节，第二步为特征均匀化。当结合简单的分类器应用于maxSRMd2特征时，我们观察到所有测试的隐写算法和有效载荷的检测精度都有所提高。为了更好地理解，我们研究了两种图像格式的增益。所提出的归一化具有非常低的计算复杂度，并且不需要来自隐写类的任何反馈。",
                    "title_zh": "隐写分析中的非线性特征归一化"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083243",
                    "title": "Improving GFR Steganalysis Features by Using Gabor Symmetry and Weighted Histograms",
                    "authors": "Chao Xia, Qingxiao Guan, Xianfeng Zhao, Zhoujun Xu, Yi Ma",
                    "abstract": "The GFR (Gabor Filter Residual) features, built as histograms of quantized residuals obtained with 2D Gabor filters, can achieve competitive detection performance against adaptive JPEG steganography. In this paper, an improved version of the GFR is proposed. First, a novel histogram merging method is proposed according to the symmetries between different Gabor filters, thus making the features more compact and robust. Second, a new weighted histogram method is proposed by considering the position of the residual value in a quantization interval, making the features more sensitive to the slight changes in residual values. The experiments are given to demonstrate the effectiveness of our proposed methods.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "GFR (Gabor滤波器残差)特征被构建为用2D Gabor滤波器获得的量化残差的直方图，可以实现与自适应JPEG隐写术竞争的检测性能。本文提出了GFR的一个改进版本。首先，根据不同Gabor滤波器之间的对称性，提出了一种新的直方图合并方法，使得特征更加紧凑和鲁棒。其次，提出了一种新的加权直方图方法，该方法考虑了残差在量化区间中的位置，使得特征对残差的微小变化更加敏感。实验证明了我们提出的方法的有效性。",
                    "title_zh": "利用Gabor对称性和加权直方图改进GFR隐写分析特征"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083236",
                    "title": "Deep Convolutional Neural Network to Detect J-UNIWARD",
                    "authors": "Guanshuo Xu",
                    "abstract": "This paper presents an empirical study on applying convolutional neural networks (CNNs) to detecting J-UNIWARD -- one of the most secure JPEG steganographic method. Experiments guiding the architectural design of the CNNs have been conducted on the JPEG compressed BOSSBase containing 10,000 covers of size 512×512. Results have verified that both the pooling method and the depth of the CNNs are critical for performance. Results have also proved that a 20-layer CNN, in general, outperforms the most sophisticated feature-based methods, but its advantage gradually diminishes on hard-to-detect cases. To show that the performance generalizes to large-scale databases and to different cover sizes, one experiment has been conducted on the CLS-LOC dataset of ImageNet containing more than one million covers cropped to unified size of 256×256. The proposed 20-layer CNN has cut the error achieved by a CNN recently proposed for large-scale JPEG steganalysis by 35%. Source code is available via GitHub: https://github.com/GuanshuoXu/deep_cnn_jpeg_steganalysis",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/1704.08378"
                    },
                    "abstract_zh": "本文提出了一个应用卷积神经网络(CNN)检测J-UNIWARD -最安全的JPEG隐写方法之一的实证研究。在包含10，000个大小为512×512的封面的JPEG压缩BOSSBase上进行了指导CNN架构设计的实验。结果已经证实，合并方法和CNN的深度对于性能都是至关重要的。结果也证明了20层的CNN通常优于最复杂的基于特征的方法，但是在难以检测的情况下，它的优势逐渐减弱。为了表明该性能适用于大规模数据库和不同的封面大小，在ImageNet的CLS-洛克数据集上进行了一个实验，该数据集包含超过一百万个裁剪为256×256的统一大小的封面。提出的20层CNN将最近提出的用于大规模JPEG隐写分析的CNN所实现的错误减少了35%。源代码可以通过GitHub:https://github.com/GuanshuoXu/deep_cnn_jpeg_steganalysis获得",
                    "title_zh": "检测J单向的深度卷积神经网络"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083248",
                    "title": "JPEG-Phase-Aware Convolutional Neural Network for Steganalysis of JPEG Images",
                    "authors": "Mo Chen, Vahid Sedighi, Mehdi Boroumand, Jessica J. Fridrich",
                    "abstract": "Detection of modern JPEG steganographic algorithms has traditionally relied on features aware of the JPEG phase. In this paper, we port JPEG-phase awareness into the architecture of a convolutional neural network to boost the detection accuracy of such detectors. Another innovative concept introduced into the detector is the \"catalyst kernel\" that, together with traditional high-pass filters used to pre-process images allows the network to learn kernels more relevant for detection of stego signal introduced by JPEG steganography. Experiments with J-UNIWARD and UED-JC embedding algorithms are used to demonstrate the merit of the proposed design.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "现代JPEG隐写算法的检测传统上依赖于感知JPEG阶段的特征。在本文中，我们将JPEG相位感知移植到卷积神经网络的架构中，以提高这种检测器的检测精度。检测器中引入的另一个创新概念是“催化剂内核”，它与用于预处理图像的传统高通滤波器一起，允许网络学习与JPEG隐写术引入的隐写信号检测更相关的内核。用J-UNIWARD和UED-JC嵌入算法进行的实验证明了所提设计的优点。",
                    "title_zh": "JPEG相位感知卷积神经网络在JPEG图像隐写分析中的应用"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083234",
                    "title": "Audio Steganalysis with Convolutional Neural Network",
                    "authors": "Bolin Chen, Weiqi Luo, Haodong Li",
                    "abstract": "In recent years, deep learning has achieved breakthrough results in various areas, such as computer vision, audio recognition, and natural language processing. However, just several related works have been investigated for digital multimedia forensics and steganalysis. In this paper, we design a novel CNN (convolutional neural networks) to detect audio steganography in the time domain. Unlike most existing CNN based methods which try to capture media contents, we carefully design the network layers to suppress audio content and adaptively capture the minor modifications introduced by ±1 LSB based steganography. Besides, we use a mix of convolutional layer and max pooling to perform subsampling to achieve good abstraction and prevent over-fitting. In our experiments, we compared our network with six similar network architectures and two traditional methods using handcrafted features. Extensive experimental results evaluated on 40,000 speech audio clips have shown the effectiveness of the proposed convolutional network.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "近年来，深度学习在计算机视觉、音频识别、自然语言处理等多个领域取得了突破性的成果。然而，只有几个相关的工作进行了调查，为数字多媒体取证和隐写分析。本文设计了一种新颖的CNN(卷积神经网络)来检测时域音频隐写术。与大多数现有的试图捕获媒体内容的基于CNN的方法不同，我们仔细设计了网络层来抑制音频内容，并自适应地捕获由基于1 LSB的隐写术引入的微小修改。此外，我们使用卷积层和最大池的混合来执行子采样，以实现良好的抽象并防止过拟合。在我们的实验中，我们将我们的网络与六个类似的网络架构和两个使用手工制作功能的传统方法进行了比较。在40，000个语音音频片段上评估的大量实验结果显示了所提出的卷积网络的有效性。",
                    "title_zh": "基于卷积神经网络的音频隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3092567",
                    "title": "Using Stylometry to Attribute Programmers and Writers",
                    "authors": "Rachel Greenstadt",
                    "abstract": "In this talk, I will discuss my lab's work in the emerging field of adversarial stylometry and machine learning. Machine learning algorithms are increasingly being used in security and privacy domains, in areas that go beyond intrusion or spam detection. For example, in digital forensics, questions often arise about the authors of documents: their identity, demographic background, and whether they can be linked to other documents. The field of stylometry uses linguistic features and machine learning techniques to answer these questions. We have applied stylometry to difficult domains such as underground hacker forums, open source projects (code), and tweets. I will discuss our Doppelgnger Finder algorithm, which enables us to group Sybil accounts on underground forums and detect blogs from Twitter feeds and reddit comments. In addition, I will discuss our work attributing unknown source code and binaries.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在这次演讲中，我将讨论我的实验室在对抗性风格测量和机器学习这一新兴领域的工作。机器学习算法越来越多地用于安全和隐私领域，超出了入侵或垃圾邮件检测的范围。例如，在数字取证中，经常会出现关于文档作者的问题:他们的身份、人口背景，以及他们是否可以与其他文档相关联。风格测量领域使用语言特征和机器学习技术来回答这些问题。我们已经将风格学应用于困难的领域，如地下黑客论坛、开源项目(代码)和推文。我将讨论我们的Doppelgnger Finder算法，它使我们能够在地下论坛上对Sybil帐户进行分组，并从Twitter feeds和reddit评论中检测博客。此外，我将讨论我们的工作归属未知的源代码和二进制文件。",
                    "title_zh": "使用笔法来区分程序员和作者"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3084682",
                    "title": "Towards Imperceptible Natural Language Watermarking for German",
                    "authors": "Oren Halvani, Martin Steinebach, Lukas Graner",
                    "abstract": "Watermarking natural language is still a challenge in the domain of digital watermarking. Here, only the textual information must be used as a cover. No format changes or modified illustrations are accepted. Still, natural language watermarking (NLW) has some important applications, especially in leakage tracking, where a small set of individually marked copies of a confidently text is distributed. Properties of watermarking schemes such as imperceptibility, blindness or adaptability to non-English languages are of importance here. In order to address these three simultaneously, we present a blind NLW scheme, consisting of four independent embedding methods, which operate on the phonetical, morphological, lexical and syntactical layer of German texts. An evaluation based on 1,645 assessments provided by 131 test persons reveals promising results.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "自然语言水印仍然是数字水印领域的一个挑战。这里，只有文本信息必须用作封面。不接受任何格式更改或修改的插图。然而，自然语言水印(NLW)有一些重要的应用，特别是在泄漏跟踪中，在泄漏跟踪中，一小组单独标记的可信文本副本被分发。水印方案的属性，例如不可感知性、盲目性或对非英语语言的适应性，在这里是很重要的。为了同时解决这三个问题，我们提出了一个盲NLW方案，它由四个独立的嵌入方法组成，分别在德语文本的语音、词法、词汇和句法层上运行。基于131名测试人员提供的1，645项评估的评估揭示了有希望的结果。",
                    "title_zh": "面向德语的不可感知自然语言水印"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083240",
                    "title": "Text Steganography with High Embedding Rate: Using Recurrent Neural Networks to Generate Chinese Classic Poetry",
                    "authors": "Yubo Luo, Yongfeng Huang",
                    "abstract": "We propose a novel text steganography method using RNN Encoder-Decoder structure to generate quatrains, one genre of Chinese poetry. Compared to other text-generation based steganography methods which have either very low embedding rate or flaws in the naturalness of generated texts, our method has higher embedding rate and better text quality. In this paper, we use the LSTM Encoder-Decoder model to generate the first line of a quatrain with a keyword and then generate the following lines one by one. RNN has proved effective in generating poetry, but when applied to steganograpy, poetry quality decreases sharply, because of the redundancy we create to hide information. To overcome this problem, we propose a template-constrained generation method and develop a word-choosing approach using inner-word mutual information. Through a series of experiments, it is proven that our approach outperforms other poetry steganography methods in both embedding rate and poetry quality.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "我们提出了一种新的文本隐写方法，该方法利用RNN编解码结构生成中国诗歌的绝句。与其他基于文本生成的隐写方法相比，我们的方法具有更高的嵌入率和更好的文本质量。在本文中，我们使用LSTM编码器-解码器模型来生成一个带有关键字的四行诗的第一行，然后逐个生成后面的行。RNN已被证明在生成诗歌方面是有效的，但当应用于隐写术时，诗歌质量会急剧下降，因为我们创建了冗余来隐藏信息。为了解决这个问题，我们提出了一种模板约束的生成方法，并开发了一种使用词内互信息的选词方法。通过一系列的实验，我们的方法在嵌入率和诗歌质量上都优于其他的诗歌隐写方法。",
                    "title_zh": "高嵌入率的文本隐写术:用递归神经网络生成中国古典诗词"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083246",
                    "title": "Audio Reversible Watermarking Scheme in the intDCT Domain with Modified Prediction Error Expansion",
                    "authors": "Alejandra Menendez-Ortiz, Claudia Feregrino Uribe, Jose Juan Garcia-Hernandez",
                    "abstract": "Reversible watermarking schemes (RWS) allow the restoration of the original signals after the watermarks are extracted. Most RWS for audio signals use time-domain for information hiding, although their transparency is hard to maintain for high embedding capacities. Some audio RWS use the frequency domain to improve transparency; however, their embedding capacity is lower than that of time-domain schemes. In this manuscript a RWS for audio signals is proposed, it differs from other schemes that work with the intDCT domain in the use of auditory masking properties, which are exploited to improve transparency, and the increase on embedding capacity is explored through a modified prediction error expansion (PEE). The payload capacity is 27.5 kbps with a degradation over -2 ODG, which are adequate results for practical audio applications. A generalized multi-bit expansion is proposed and experimental results suggest that higher expansion factors improve transparency.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "可逆水印方案(RWS)允许在提取水印后恢复原始信号。大多数音频信号的RWS使用时域进行信息隐藏，尽管它们的透明性很难保持高嵌入容量。一些音频RWS使用频域来提高透明度；然而，它们的嵌入容量低于时域方案。在这篇文章中，提出了一种用于音频信号的RWS，它在听觉掩蔽特性的使用方面不同于与intDCT域一起工作的其他方案，听觉掩蔽特性被用来提高透明度，并且通过修改的预测误差扩展(PEE)来探索嵌入容量的增加。有效载荷容量为27.5 kbps，衰减超过-2 ODG，这对于实际音频应用来说是足够的结果。提出了一种广义的多比特扩展，并且实验结果表明较高的扩展因子提高了透明度。",
                    "title_zh": "改进预测误差扩展的intDCT域音频可逆水印方案"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083241",
                    "title": "A Minimum Distortion: High Capacity Watermarking Technique for Relational Data",
                    "authors": "Maikel L. Pérez Gort, Claudia Feregrino Uribe, Jyrki Nummenmaa",
                    "abstract": "In this paper, a new multi-attribute and high capacity image-based watermarking technique for relational data is proposed. The embedding process causes low distortion into the data considering the usability restrictions defined over the marked relation. The conducted experiments show the high resilience of the proposed technique against tuple deletion and tuple addition attacks. An interesting trend of the extracted watermark is analyzed when, within certain limits, if the number of embedded marks is small, the watermark signal far from being compromised, discretely improves in the case of tuple addition attacks. According to the results, marking 13% of the attributes and under an attack of 100% of tuples addition, 96% of the watermark is extracted. Also, while previous techniques embed up to 61% of the watermark, under the same conditions, we guarantee to embed 99.96% of the marks.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "提出了一种新的多属性大容量关系数据图像水印技术。考虑到在标记关系上定义的可用性限制，嵌入过程导致数据的低失真。实验表明，该技术对元组删除和元组添加攻击具有很强的抵御能力。当在一定限度内，如果嵌入标记的数量很小，水印信号远未被破坏，在元组添加攻击的情况下，水印信号离散地改善时，提取的水印的有趣趋势被分析。根据结果，标记13%的属性，在100%元组添加的攻击下，提取了96%的水印。此外，虽然以前的技术嵌入高达61%的水印，但在相同条件下，我们保证嵌入99.96%的标记。",
                    "title_zh": "一种最小失真的关系数据大容量水印技术"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083245",
                    "title": "A Steganalytic Algorithm to Detect DCT-based Data Hiding Methods for H.264/AVC Videos",
                    "authors": "Peipei Wang, Yun Cao, Xianfeng Zhao, Meineng Zhu",
                    "abstract": "This paper presents an effective steganalytic algorithm to detect Discrete Cosine Transform (DCT) based data hiding methods for H.264/AVC videos. These methods hide covert information into compressed video streams by manipulating quantized DCT coefficients, and usually achieve high payload and low computational complexity, which is suitable for applications with hard real-time requirements. In contrast to considerable literature grown up in JPEG domain steganalysis, so far there is few work found against DCT-based methods for compressed videos. In this paper, the embedding impacts on both spatial and temporal correlations are carefully analyzed, based on which two feature sets are designed for steganalysis. The first feature set is engineered as the histograms of noise residuals from the decompressed frames using 16 DCT kernels, in which a quantity measuring residual distortion is accumulated. The second feature set is designed as the residual histograms from the similar blocks linked by motion vectors between inter-frames. The experimental results have demonstrated that our method can effectively distinguish stego videos undergone DCT manipulations from clean ones, especially for those of high qualities.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "针对H.264/AVC视频中基于离散余弦变换(DCT)的数据隐藏方法，提出了一种有效的隐写分析算法。这些方法通过操作量化的DCT系数将隐藏信息隐藏到压缩的视频流中，并且通常实现高有效载荷和低计算复杂度，适合于具有硬实时要求的应用。与JPEG域隐写分析方面的大量文献相比，迄今为止，针对基于DCT的压缩视频方法的工作很少。本文详细分析了嵌入对空间相关性和时间相关性的影响，并在此基础上设计了两个用于隐写分析的特征集。第一特征集被设计为来自使用16个DCT核的解压缩帧的噪声残差的直方图，其中累积了测量残差失真的量。第二特征集被设计为来自通过帧间运动矢量链接的相似块的残差直方图。实验结果表明，该方法能有效区分经过DCT变换的隐写视频和干净视频，尤其是高质量的隐写视频。",
                    "title_zh": "一种检测基于DCT的H.264/AVC视频数据隐藏方法的隐写分析算法"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083237",
                    "title": "Combined and Calibrated Features for Steganalysis of Motion Vector-Based Steganography in H.264/AVC",
                    "authors": "Liming Zhai, Lina Wang, Yanzhen Ren",
                    "abstract": "This paper presents a novel feature set for steganalysis of motion vector-based steganography in H.264/AVC. First, the influence of steganographic embedding on the sum of absolute difference (SAD) and the motion vector difference (MVD) is analyzed, and then the statistical characteristics of these two aspects are combined to design features. In terms of SAD, the macroblock partition modes are used to measure the quantization distortion, and by using the optimality of SAD in neighborhood, the partition based neighborhood optimal probability features are extracted. In terms of MVD, it has been proved that MVD is better in feature construction than neighboring motion vector difference (NMVD) which has been widely used by traditional steganalyzers, and thus the inter and intra co-occurrence features are constructed based on the distribution of two components of neighboring MVDs and the distribution of two components of the same MVD. Finally, the combined features are enhanced by window optimal calibration, which utilizes the optimality of both SAD and MVD in a local window area. Experiments on various conditions demonstrate that the proposed scheme generally achieves a more accurate detection than current methods especially for videos encoded in variable block size and high quantization parameter values, and exhibits strong universality in applications.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "针对H.264/AVC中基于运动矢量的隐写术，提出了一种新的隐写分析特征集。首先分析隐写嵌入对绝对差和(SAD)和运动矢量差(MVD)的影响，然后结合这两方面的统计特性设计特征。在SAD方面，利用宏块划分模式来度量量化失真，并利用SAD在邻域内的最优性，提取基于分块的邻域最优概率特征。在MVD方面，已经证明MVD在特征构造方面优于传统隐写分析器广泛使用的相邻运动矢量差(NMVD ),因此基于相邻MVD的两个分量的分布和同一MVD的两个分量的分布来构造帧间和帧内共现特征。最后，通过窗口最优校准来增强组合特征，该校准利用了局部窗口区域中SAD和MVD的最优性。在各种条件下的实验表明，所提出的方案总体上实现了比现有方法更准确的检测，尤其是对于以可变块大小和高量化参数值编码的视频，并且在应用中表现出很强的通用性。",
                    "title_zh": "H.264/AVC中基于运动矢量隐写术的组合和标定特征分析"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083249",
                    "title": "A Generic Approach Towards Image Manipulation Parameter Estimation Using Convolutional Neural Networks",
                    "authors": "Belhassen Bayar, Matthew C. Stamm",
                    "abstract": "Estimating manipulation parameter values is an important problem in image forensics. While several algorithms have been proposed to accomplish this, their application is exclusively limited to one type of image manipulation. These existing techniques are often designed using classical approaches from estimation theory by constructing parametric models of image data. This is problematic since this process of developing a theoretical model then deriving a parameter estimator must be repeated each time a new image manipulation is derived. In this paper, we propose a new data-driven generic approach to performing manipulation parameter estimation. Our proposed approach can be adapted to operate on several different manipulations without requiring a forensic investigator to make substantial changes to the proposed method. To accomplish this, we reformulate estimation as a classification problem by partitioning the parameter space into disjoint subsets such that each parameter subset is assigned a distinct class. Subsequently, we design a constrained CNN-based classifier that is able to extract classification features directly from data as well as estimating the manipulation parameter value in a subject image. Through a set of experiments, we demonstrated the effectiveness of our approach using four different types of manipulations.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "估计操作参数值是图像取证中的一个重要问题。虽然已经提出了几种算法来实现这一点，但是它们的应用仅限于一种类型的图像处理。这些现有技术通常是通过构建图像数据的参数模型，使用来自估计理论的经典方法来设计的。这是有问题的，因为每次导出新的图像处理时，必须重复开发理论模型然后导出参数估计器的过程。在本文中，我们提出了一种新的数据驱动的一般方法来执行操纵参数估计。我们提出的方法可以适用于几种不同的操作，而不需要法医调查人员对提出的方法进行实质性的改变。为了实现这一点，我们通过将参数空间划分成不相交的子集来将估计重新表述为分类问题，使得每个参数子集被分配一个不同的类别。随后，我们设计了一个基于约束CNN的分类器，该分类器能够直接从数据中提取分类特征，并估计主题图像中的操作参数值。通过一组实验，我们使用四种不同类型的操作展示了我们的方法的有效性。",
                    "title_zh": "基于卷积神经网络的图像处理参数估计通用方法"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083247",
                    "title": "Recasting Residual-based Local Descriptors as Convolutional Neural Networks: an Application to Image Forgery Detection",
                    "authors": "Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva",
                    "abstract": "Local descriptors based on the image noise residual have proven extremely effective for a number of forensic applications, like forgery detection and localization. Nonetheless, motivated by promising results in computer vision, the focus of the research community is now shifting on deep learning. In this paper we show that a class of residual-based descriptors can be actually regarded as a simple constrained convolutional neural network (CNN). Then, by relaxing the constraints, and fine-tuning the net on a relatively small training set, we obtain a significant performance improvement with respect to the conventional detector.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "基于图像噪声残差的局部描述符已经被证明对于许多取证应用非常有效，例如伪造检测和定位。尽管如此，在计算机视觉有希望的结果的推动下，研究界的焦点现在正在转向深度学习。在本文中，我们表明，一类基于残差的描述符实际上可以被视为一个简单的约束卷积神经网络(CNN)。然后，通过放松约束，并在相对较小的训练集上对网络进行微调，我们获得了相对于传统检测器的显著性能改进。",
                    "title_zh": "将基于残差的局部描述符重铸为卷积神经网络:在图像伪造检测中的应用"
                },
                {
                    "url": "https://doi.org/10.1145/3082031.3083250",
                    "title": "Image Forensics Based on Transfer Learning and Convolutional Neural Network",
                    "authors": "Yifeng Zhan, Yifang Chen, Qiong Zhang, Xiangui Kang",
                    "abstract": "There have been a growing number of interests in using the convolutional neural network(CNN) in image forensics, where some excellent methods have been proposed. Training the randomly initialized model from scratch needs a big amount of training data and computational time. To solve this issue, we present a new method of training an image forensic model using prior knowledge transferred from the existing steganalysis model. We also find out that CNN models tend to show poor performance when tested on a different database. With knowledge transfer, we are able to easily train an excellent model for a new database with a small amount of training data from the new database. Performance of our models are evaluated on Bossbase and BOW by detecting five forensic types, including median filtering, resampling, JPEG compression, contrast enhancement and additive Gaussian noise. Through a series of experiments, we demonstrate that our proposed method is very effective in two scenario mentioned above, and our method based on transfer learning can greatly accelerate the convergence of CNN model. The results of these experiments show that our proposed method can detect five different manipulations with an average accuracy of 97.36%.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在图像取证中使用卷积神经网络(CNN)已经引起了越来越多的兴趣，并提出了一些优秀的方法。从头开始训练随机初始化的模型需要大量的训练数据和计算时间。为了解决这个问题，我们提出了一种新的方法来训练一个图像取证模型使用先验知识转移到现有的隐写分析模型。我们还发现，当在不同的数据库上测试时，CNN模型往往表现不佳。通过知识转移，我们能够利用新数据库中的少量训练数据，轻松地为新数据库训练出优秀的模型。通过检测五种取证类型，包括中值滤波、重采样、JPEG压缩、对比度增强和加性高斯噪声，在Bossbase和BOW上评估了我们的模型的性能。通过一系列的实验，我们证明了我们提出的方法在上述两种情况下是非常有效的，并且我们基于迁移学习的方法可以大大加快CNN模型的收敛速度。实验结果表明，我们提出的方法可以检测五种不同的操作，平均准确率为97.36%。",
                    "title_zh": "基于迁移学习和卷积神经网络的图像取证"
                }
            ]
        }
    ],
    "2018": [
        {
            "dblp_url": "https://dblp.uni-trier.de/db/conf/ih/ihmmsec2018.html",
            "conf_title": "6th IH&MMSec 2018: Innsbruck, Austria",
            "conf_url": "https://doi.org/10.1145/3206004",
            "papers": [
                {
                    "url": "https://doi.org/10.1145/3206004.3206023",
                    "title": "Covert and Deniable Communications",
                    "authors": "Ross Anderson",
                    "abstract": "At the first Information Hiding Workshop in 1996 we tried to clarify the models and assumptions behind information hiding. We agreed the terminology of cover text and stego text against a background of the game proposed by our keynote speaker Gus Simmons: that Alice and Bob are in jail and wish to hatch an escape plan without the fact of their communication coming to the attention of the warden, Willie. Since then there have been significant strides in developing technical mechanisms for steganography and steganalysis, with new techniques from machine learning providing ever more powerful tools for the analyst, such as the ensemble classifier. There have also been a number of conceptual advances, such as the square root law and effective key length. But there always remains the question whether we are using the right security metrics for the application. In this talk I plan to take a step backwards and look at the systems context. When can stegosystems actually be used? The deployment history is patchy, with one being Trucrypt's hidden volumes, inspired by the steganographic file system. Image forensics also find some use, and may be helpful against some adversarial machine learning attacks (or at least help us understand them). But there are other contexts in which patterns of activity have to be hidden for that activity to be effective. I will discuss a number of examples starting with deception mechanisms such as honeypots, Tor bridges and pluggable transports, which merely have to evade detection for a while; then moving on to the more challenging task of designing deniability mechanisms, from leaking secrets to a newspaper through bitcoin mixes, which have to withstand forensic examination once the participants come under suspicion. We already know that, at the system level, anonymity is hard. However the increasing quantity and richness of the data available to opponents may move a number of applications from the deception category to that of deniability. To pick up on our model of 20 years ago, Willie might not just put Alice and Bob in solitary confinement if he finds them communicating, but torture them or even execute them. Changing threat models are historically one of the great disruptive forces in security engineering. This leads me to suspect that a useful research area may be the intersection of deception and forensics, and how information hiding systems can be designed in anticipation of richer and more complex threat models. The ever-more-aggressive censorship systems deployed in some parts of the world also raise the possibility of using information hiding techniques in censorship circumvention. As an example of recent practical work, I will discuss Covertmark, a toolkit for testing pluggable transports that was partly inspired by Stirmark, a tool we presented at the second Information Hiding Workshop twenty years ago.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在1996年的第一次信息隐藏研讨会上，我们试图阐明信息隐藏背后的模型和假设。我们同意了封面文本和隐写文本的术语，以我们的主题演讲人Gus Simmons提出的游戏背景为背景:Alice和Bob在监狱中，希望在他们的通信没有引起典狱长Willie的注意的情况下策划一个越狱计划。从那时起，在开发隐写术和隐写分析的技术机制方面取得了重大进展，来自机器学习的新技术为分析师提供了越来越强大的工具，如集成分类器。还有一些概念上的进步，比如平方根定律和有效密钥长度。但是，我们是否为应用程序使用了正确的安全指标，这始终是一个问题。在这次演讲中，我打算后退一步，看看系统的背景。隐写系统什么时候可以实际使用？部署历史参差不齐，其中一个是Trucrypt的隐藏卷，灵感来自隐写文件系统。图像取证也有一些用途，可能有助于对抗一些对抗性的机器学习攻击(或至少帮助我们理解它们)。但是在其他情况下，为了使活动有效，必须隐藏活动模式。我将讨论一些例子，从欺骗机制开始，如蜜罐、Tor桥和可插拔传输，它们只需要暂时逃避检测；然后转向更具挑战性的设计否认机制的任务，从通过比特币混合物向报纸泄露秘密，一旦参与者受到怀疑，这些机制就必须经受住法医检查。我们已经知道，在系统层面，匿名是很难的。然而，对手可获得的数据的数量和丰富性的增加可能会将许多应用从欺骗类别转移到可否认类别。按照我们20年前的模式，如果威利发现爱丽丝和鲍勃在交流，他可能不只是把他们单独监禁，而是折磨他们，甚至处决他们。不断变化的威胁模型在历史上是安全工程中最具破坏性的力量之一。这让我怀疑一个有用的研究领域可能是欺骗和取证的交叉，以及如何在预期更丰富和更复杂的威胁模型中设计信息隐藏系统。在世界某些地方部署的越来越激进的审查系统也增加了在审查规避中使用信息隐藏技术的可能性。作为最近实际工作的一个例子，我将讨论Covertmark，这是一个用于测试可插拔传输的工具包，部分灵感来自于Stirmark，一个我们在20年前的第二次信息隐藏研讨会上展示的工具。",
                    "title_zh": "隐蔽和可否认的通信"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206024",
                    "title": "Deep Learning in Multimedia Forensics",
                    "authors": "Luisa Verdoliva",
                    "abstract": "With the widespread diffusion of powerful media editing tools, falsifying images and videos has become easier and easier in the last few years. Fake multimedia, often used to support fake news, represents a growing menace in many fields of life, notably in politics, journalism, and the judiciary. In response to this threat, the signal processing community has produced a major research effort. A large number of methods have been proposed for source identification, forgery detection and localization, relying on the typical signal processing tools. The advent of deep learning, however, is changing the rules of the game. On one hand, new sophisticated methods based on deep learning have been proposed to accomplish manipulations that were previously unthinkable. On the other hand, deep learning provides also the analyst with new powerful forensic tools. Given a suitably large training set, deep learning architectures ensure usually a significant performance gain with respect to conventional methods, and a much higher robustness to post-processing and evasions. In this talk after reviewing the main approaches proposed in the literature to ensure media authenticity, the most promising solutions relying on Convolutional Neural Networks will be explored with special attention to realistic scenarios, such as when manipulated images and videos are spread out over social networks. In addition, an analysis of the efficacy of adversarial attacks on such methods will be presented.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3206004.3206024"
                    },
                    "abstract_zh": "随着强大的媒体编辑工具的广泛传播，伪造图像和视频在过去几年变得越来越容易。经常被用来支持假新闻的假多媒体，在生活的许多领域，特别是在政治、新闻和司法领域，代表着一种日益增长的威胁。为了应对这一威胁，信号处理界开展了大量的研究工作。依靠典型的信号处理工具，已经提出了大量的方法用于源识别、伪造检测和定位。然而，深度学习的出现正在改变游戏规则。一方面，基于深度学习的新的复杂方法已经被提出来完成以前不可想象的操作。另一方面，深度学习也为分析师提供了新的强大的取证工具。给定适当大的训练集，深度学习架构通常确保相对于传统方法的显著性能增益，以及对后处理和规避的高得多的鲁棒性。在本次演讲中，我们回顾了文献中提出的确保媒体真实性的主要方法，并探讨了依赖卷积神经网络的最有前途的解决方案，同时特别关注现实场景，例如当经过处理的图像和视频在社交网络上传播时。此外，还将对这类方法的对抗性攻击的有效性进行分析。",
                    "title_zh": "多媒体取证中的深度学习"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206008",
                    "title": "Defining Joint Distortion for JPEG Steganography",
                    "authors": "Weixiang Li, Weiming Zhang, Kejiang Chen, Wenbo Zhou, Nenghai Yu",
                    "abstract": "Recent studies have shown that the non-additive distortion model of Decomposing Joint Distortion ($DeJoin$) can work well for spatial image steganography by defining joint distortion with the principle of Synchronizing Modification Directions (SMD). However, no principles have yet produced to instruct the definition of joint distortion for JPEG steganography. Experimental results indicate that SMD can not be directly used for JPEG images, which means that simply pursuing modification directions clustered does not help improve the steganographic security. In this paper, we inspect the embedding change from the spatial domain and propose a principle of Block Boundary Continuity (BBC) for defining JPEG joint distortion, which aims to restrain blocking artifacts caused by inter-block adjacent modifications and thus effectively preserve the spatial continuity at block boundaries. According to BBC, whether inter-block adjacent modifications should be synchronized or desynchronized is related to the DCT mode and the adjacent direction of inter-block coefficients (horizontal or vertical). When built into $DeJoin$, experiments demonstrate that BBC does help improve state-of-the-art additive distortion schemes in terms of relatively large embedding payloads against modern JPEG steganalyzers.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "最近的研究表明，分解联合失真的非加性失真模型(DeJoin$)可以很好地用于空间图像隐写，它通过同步修改方向(SMD)的原则来定义联合失真。然而，还没有产生指导JPEG隐写术的联合失真的定义的原则。实验结果表明，SMD不能直接用于JPEG图像，这意味着简单地追求修改方向聚类无助于提高隐写安全性。本文从空间域的角度考察嵌入变化，提出了一种定义JPEG联合失真的块边界连续性原则，旨在抑制块间相邻修改引起的块效应，从而有效地保持块边界的空间连续性。据BBC报道，块间相邻修改应该同步还是去同步，与DCT模式和块间系数的相邻方向(水平或垂直)有关。当内置到$DeJoin$中时，实验证明BBC确实有助于改进最先进的附加失真方案，相对于现代JPEG隐写分析器，嵌入有效载荷相对较大。",
                    "title_zh": "定义JPEG隐写术的联合失真"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206021",
                    "title": "Facing the Cover-Source Mismatch on JPHide using Training-Set Design",
                    "authors": "Dirk Borghys, Patrick Bas, Helena Bruyninckx",
                    "abstract": "This short paper investigates the influence of the image processing pipeline (IPP) on the cover-source mismatch (CSM) for the popular JPHide steganographic scheme. We propose to deal with CSM by combining a forensics and a steganalysis approach. A multi-classifier is first trained to identify the IPP, and secondly a specific training set is designed to train a targeted classifier for steganalysis purposes. We show that the forensic step is immune to the steganographic embedding. The proposed IPP-informed steganalysis outperforms classical strategies based on training on a mixture of sources and we show that it can provide results close to a detector specifically trained on the appropriate source.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-01833873/file/BB_IHMMSEC-HAL.pdf"
                    },
                    "abstract_zh": "这篇短文研究了图像处理流水线(IPP)对流行的JPHide隐写方案的覆盖源失配(CSM)的影响。我们建议通过结合取证和隐写分析方法来处理CSM。首先训练多分类器来识别IPP，然后设计特定的训练集来训练用于隐写分析目的的目标分类器。我们证明了取证步骤是免疫的隐写嵌入。提出的IPP信息隐写分析优于基于混合源训练的经典策略，并且我们表明它可以提供接近于在适当源上专门训练的检测器的结果。",
                    "title_zh": "利用训练集设计解决JPHide上的覆盖源失配问题"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206014",
                    "title": "Cover Block Decoupling for Content-Adaptive H.264 Steganography",
                    "authors": "Yun Cao, Yu Wang, Xianfeng Zhao, Meineng Zhu, Zhoujun Xu",
                    "abstract": "This paper makes the first attempt to achieve content-adaptive H.264 steganography with the quantised discrete cosine transform (QDCT) coefficients in intra-frames. Currently, state-of-the-art JPEG steganographic schemes embed their payload while minimizing a heuristically defined distortion. However, porting this concept to schemes of compressed videos remains an unsolved challenge. Because of H.264 intra prediction, the QDCT coefficient blocks are highly depended on their adjacent encoded blocks, and modifying one coefficient block will set off a chain reaction in the following cover blocks. Based on a thorough investigation into this problem, we propose two embedding strategies for cover block decoupling to inhibit the embedding interactions. With this methodology, the latest achievements in the JPEG domain are expected to be incorporated to construct H.264 steganographic schemes for better performances.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "本文首次尝试利用帧内量化离散余弦变换(QDCT)系数实现内容自适应的H.264隐写术。目前，最先进的JPEG隐写方案嵌入了它们的有效载荷，同时最小化了启发式定义的失真。然而，将这个概念移植到压缩视频的方案中仍然是一个未解决的挑战。由于H.264帧内预测，QDCT系数块高度依赖于其相邻的编码块，并且修改一个系数块将在后续的覆盖块中引起连锁反应。基于对这一问题的深入研究，我们提出了两种覆盖块解耦嵌入策略来抑制嵌入交互。通过这种方法，JPEG领域的最新成果有望被整合到H.264隐写方案中，以获得更好的性能。",
                    "title_zh": "用于内容自适应H.264隐写术的覆盖块解耦"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206006",
                    "title": "Do EEG-Biometric Templates Threaten User Privacy?",
                    "authors": "Yvonne Höller, Andreas Uhl",
                    "abstract": "The electroencephalogram (EEG) was introduced as a method for the generation of biometric templates. So far, most research focused on the optimisation of the enrolment and authentication, and it was claimed that the EEG has many advantages. However, it was never assessed whether the biometric templates obtained from the EEG contain sensitive information about the enrolled users. In this work we ask whether we can infer personal characteristics such as age, sex, or informations about neurological disorders from these templates. To this end, we extracted a set of 16 feature vectors from EEG epochs from a sample of 60 healthy subjects and neurological patients. One of these features was the classical power spectrum, while the other 15 features were derived from a multivariate autoregressive model, considering also interdependencies of EEG channels. We classified the sample by sex, neurological diagnoses, age, atrophy of the brain, and intake of neurological drugs. We obtained classification accuracies of up to .70 for sex, .86 for the classification of epilepsy vs. other populations, .81 for the differentiation of young vs. old people's templates, and .82 for the intake of medication targeted to the central nervous system. These informations represent privacy sensitive information about the users, so that our results emphasise the need to apply protective safeguards in the deployment of EEG biometric systems.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3206004.3206006"
                    },
                    "abstract_zh": "脑电图(EEG)作为一种生物模板的生成方法被引入。到目前为止，大多数研究集中在注册和认证的优化上，并声称EEG具有许多优点。然而，从未评估从EEG获得的生物特征模板是否包含关于注册用户的敏感信息。在这项工作中，我们询问是否可以从这些模板中推断出个人特征，如年龄、性别或有关神经疾病的信息。为此，我们从60名健康受试者和神经疾病患者的样本中提取了一组来自EEG时期的16个特征向量。其中一个特征是经典功率谱，而其他15个特征是从多变量自回归模型中得出的，也考虑了EEG通道的相互依赖性。我们根据性别、神经系统诊断、年龄、脑萎缩和神经系统药物的摄入对样本进行分类。我们获得了高达0.70的性别分类准确率，0.86的癫痫与其他人群的分类准确率，0.81的年轻人与老年人模板的区分准确率，以及0.82的针对中枢神经系统的药物摄入准确率。这些信息代表了用户的隐私敏感信息，因此我们的结果强调了在部署EEG生物识别系统时应用保护措施的必要性。",
                    "title_zh": "脑电生物模板会威胁用户隐私吗？"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206009",
                    "title": "Fake Faces Identification via Convolutional Neural Network",
                    "authors": "Huaxiao Mo, Bolin Chen, Weiqi Luo",
                    "abstract": "Generative Adversarial Network (GAN) is a prominent generative model that are widely used in various applications. Recent studies have indicated that it is possible to obtain fake face images with a high visual quality based on this novel model. If those fake faces are abused in image tampering, it would cause some potential moral, ethical and legal problems. In this paper, therefore, we first propose a Convolutional Neural Network (CNN) based method to identify fake face images generated by the current best method [20], and provide experimental evidences to show that the proposed method can achieve satisfactory results with an average accuracy over 99.4%. In addition, we provide comparative results evaluated on some variants of the proposed CNN architecture, including the high pass filter, the number of the layer groups and the activation function, to further verify the rationality of our method.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "生成对抗网络是一种重要的生成模型，广泛应用于各种领域。最近的研究表明，基于这种新颖的模型，有可能获得具有高视觉质量的假人脸图像。如果这些假脸被滥用于图像篡改，将会引发一些潜在的道德、伦理和法律问题。因此，在本文中，我们首先提出了一种基于卷积神经网络(CNN)的方法来识别由当前最佳方法[20]生成的伪造人脸图像，并提供实验证据来表明，所提出的方法可以达到令人满意的结果，平均准确率超过99.4%。此外，我们提供了对所提出的CNN架构的一些变体的评估的比较结果，包括高通滤波器、层组的数目和激活函数，以进一步验证我们的方法的合理性。",
                    "title_zh": "基于卷积神经网络的假脸识别"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206018",
                    "title": "Generalized Benford's Law for Blind Detection of Morphed Face Images",
                    "authors": "Andrey Makrushin, Christian Kraetzer, Tom Neubert, Jana Dittmann",
                    "abstract": "A morphed face image in a photo ID is a serious threat to image-based user verification enabling that multiple persons could be matched with the same document. The application of machine-readable travel documents (MRTD) at automated border control (ABC) gates is an example of a verification scenario that is very sensitive to this kind of fraud. Detection of morphed face images prior to face matching is, therefore, indispensable for effective border security. We introduce the face morphing detection approach based on fitting a logarithmic curve to nine Benford features extracted from quantized DCT coefficients of JPEG compressed original and morphed face images. We separately study the parameters of the logarithmic curve in face and background regions to establish the traces imposed by the morphing process. The evaluation results show that a single parameter of the logarithmic curve may be sufficient to clearly separate morphed and original images.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "照片ID中的变形人脸图像是对基于图像的用户验证的严重威胁，使得多个人可以与同一文档匹配。在自动边境控制(ABC)关口应用机器可读旅行证件(MRTD)就是对这种欺诈非常敏感的一个验证案例。因此，在面部匹配之前检测变形的面部图像对于有效的边界安全是不可或缺的。介绍了一种基于对数曲线拟合的人脸变形检测方法，该方法从JPEG压缩的原始和变形人脸图像的量化DCT系数中提取9个Benford特征。我们分别研究面部和背景区域中对数曲线的参数，以建立由变形过程强加的轨迹。评估结果表明，对数曲线的单个参数可能足以清楚地区分变形图像和原始图像。",
                    "title_zh": "变形人脸图像盲检测的广义Benford定律"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206011",
                    "title": "CNN-based Steganalysis of MP3 Steganography in the Entropy Code Domain",
                    "authors": "Yuntao Wang, Kun Yang, Xiaowei Yi, Xianfeng Zhao, Zhoujun Xu",
                    "abstract": "This paper presents an effective steganalytic scheme based on CNN for detecting MP3 steganography in the entropy code domain. These steganographic methods hide secret messages into the compressed audio stream through Huffman code substitution, which usually achieve high capacity, good security and low computational complexity. First, unlike most previous CNN based steganalytic methods, the quantified modified DCT (QMDCT) coefficients matrix is selected as the input data of the proposed network. Second, a high pass filter is used to extract the residual signal, and suppress the content itself, so that the network is more sensitive to the subtle alteration introduced by the data hiding methods. Third, the $ 1 \\times 1 $ convolutional kernel and the batch normalization layer are applied to decrease the danger of overfitting and accelerate the convergence of the back-propagation. In addition, the performance of the network is optimized via fine-tuning the architecture. The experiments demonstrate that the proposed CNN performs far better than the traditional handcrafted features. In particular, the network has a good performance for the detection of an adaptive MP3 steganography algorithm, equal length entropy codes substitution (EECS) algorithm which is hard to detect through conventional handcrafted features. The network can be applied to various bitrates and relative payloads seamlessly. Last but not the least, a sliding window method is proposed to steganalyze audios of arbitrary size.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "提出了一种有效的基于CNN的隐写分析方案，用于检测熵编码域的MP3隐写。这些隐写方法通过霍夫曼码替换将秘密消息隐藏到压缩的音频流中，通常实现高容量、良好的安全性和低计算复杂度。首先，与大多数先前的基于CNN的隐写分析方法不同，量化的修正DCT (QMDCT)系数矩阵被选择作为所提出的网络的输入数据。第二，使用高通滤波器来提取剩余信号，并抑制内容本身，使得网络对由数据隐藏方法引入的细微改变更敏感。第三，应用1乘1卷积核和批量归一化层来减少过拟合的危险和加速反向传播的收敛。此外，通过微调架构优化了网络性能。实验表明，所提出的细胞神经网络的性能远远优于传统的手工特征。特别地，该网络对于难以通过传统手工特征检测的自适应MP3隐写算法、等长熵编码替换(EECS)算法具有良好的检测性能。该网络可以无缝地应用于各种比特率和相关有效载荷。最后，提出了一种滑动窗口方法来对任意大小的音频进行隐写分析。",
                    "title_zh": "基于CNN的熵编码域MP3隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206012",
                    "title": "Adversarial Examples Against Deep Neural Network based Steganalysis",
                    "authors": "Yiwei Zhang, Weiming Zhang, Kejiang Chen, Jiayang Liu, Yujia Liu, Nenghai Yu",
                    "abstract": "Deep neural network based steganalysis has developed rapidly in recent years, which poses a challenge to the security of steganography. However, there is no steganography method that can effectively resist the neural networks for steganalysis at present. In this paper, we propose a new strategy that constructs enhanced covers against neural networks with the technique of adversarial examples. The enhanced covers and their corresponding stegos are most likely to be judged as covers by the networks. Besides, we use both deep neural network based steganalysis and high-dimensional feature classifiers to evaluate the performance of steganography and propose a new comprehensive security criterion. We also make a tradeoff between the two analysis systems and improve the comprehensive security. The effectiveness of the proposed scheme is verified with the evidence obtained from the experiments on the BOSSbase using the steganography algorithm of WOW and popular steganalyzers with rich models and three state-of-the-art neural networks.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "基于深度神经网络的隐写分析近年来发展迅速，对隐写术的安全性提出了挑战。然而，目前还没有能有效抵抗神经网络隐写分析的隐写方法。在这篇文章中，我们提出了一种新的策略，用对立例子技术来构造针对神经网络的增强覆盖。增强的覆盖和它们相应的隐写最有可能被网络判断为覆盖。此外，我们使用基于深度神经网络的隐写分析和高维特征分类器来评估隐写术的性能，并提出一种新的综合安全标准。我们还在两种分析系统之间进行了权衡，提高了综合安全性。通过在BOSSbase上使用WOW隐写算法和流行的隐写分析器(具有丰富的模型和三个最先进的神经网络)进行的实验，验证了所提方案的有效性。",
                    "title_zh": "基于深度神经网络的隐写分析对抗实例"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206005",
                    "title": "Identification of Audio Processing Operations Based on Convolutional Neural Network",
                    "authors": "Bolin Chen, Weiqi Luo, Da Luo",
                    "abstract": "To reduce the tampering artifacts and/or enhance audio quality, some audio processing operations are often applied in the resulting tampered audio. Like image forensics, the detection of various post processing operations has become very important for audio authentication. In this paper, we propose a convolutional neural network (CNN) to detect audio processing operations. In the proposed method, we carefully design the network architecture, with particular attention to the frequency representation for the audio input, the activation function and the depth of the network. In our experiments, we evaluate the proposed method on audio clips with 12 commonly used audio processing operations and of three different small sizes. The experimental results show that our method can significantly outperform related methods based on hand-crafted features and other CNN architectures, and can achieve state-of-the-art results for both binary and multiple classification.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "为了减少篡改假象和/或增强音频质量，通常在所得到的篡改音频中应用一些音频处理操作。像图像取证一样，各种后处理操作的检测对于音频认证变得非常重要。在本文中，我们提出了一个卷积神经网络(CNN)来检测音频处理操作。在所提出的方法中，我们仔细地设计了网络架构，特别注意音频输入的频率表示、激活函数和网络的深度。在我们的实验中，我们使用12种常用的音频处理操作和三种不同的小尺寸的音频片段来评估所提出的方法。实验结果表明，我们的方法明显优于基于手工特征和其他CNN架构的相关方法，并且在二分类和多分类上都可以达到最先进的结果。",
                    "title_zh": "基于卷积神经网络的音频处理操作识别"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206022",
                    "title": "Learning Unified Deep-Features for Multiple Forensic Tasks",
                    "authors": "Owen Mayer, Belhassen Bayar, Matthew C. Stamm",
                    "abstract": "Recently, deep learning researchers have developed a technique known as deep features in which feature extractors for a task are learned by a CNN. These features are then provided to another classifier, or even used to perform a different classification task. Research in deep learning suggests that in some cases, deep features generalize to seemingly unrelated tasks. In this paper, we develop techniques for learning deep features that can be used across multiple forensic tasks, namely image manipulation detection and camera model identification. To do this, we develop two approaches for building deep forensic features: a transfer learning approach and a multitask learning approach. We experimentally evaluate the performance of both approaches in several scenarios and find that: 1) features learned for camera model identification generalize well to manipulation detection tasks but manipulation detection features do not generalize well to camera model identification, suggesting a task asymmetry, 2) deeper features are more task specific while shallower features generalize well across tasks, suggesting a feature hierarchy, and 3) a single, unified feature extractor can be learned that is highly discriminative for multiple forensic tasks. Furthermore, we find that when there is limited training data, a unified feature extractor can significantly outperform a targeted CNN.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3206004.3206022"
                    },
                    "abstract_zh": "最近，深度学习研究人员开发了一种称为深度特征的技术，其中一项任务的特征提取器由CNN学习。这些特征然后被提供给另一个分类器，或者甚至用于执行不同的分类任务。深度学习的研究表明，在某些情况下，深度特征会推广到看似不相关的任务。在本文中，我们开发了用于学习深度特征的技术，这些技术可用于多种取证任务，即图像操纵检测和相机型号识别。为此，我们开发了两种构建深度取证特征的方法:迁移学习方法和多任务学习方法。我们在几个场景中实验性地评估了这两种方法的性能，并且发现:1)为相机模型识别学习的特征很好地推广到操纵检测任务，但是操纵检测特征不能很好地推广到相机模型识别，这表明任务不对称；2)较深的特征更特定于任务，而较浅的特征跨任务推广得很好，这表明特征层级；以及3)可以学习单个统一的特征提取器，该特征提取器对于多个取证任务具有很高的区分度。此外，我们发现，当训练数据有限时，统一的特征提取器可以明显优于目标CNN。",
                    "title_zh": "为多个取证任务学习统一的深度特征"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206010",
                    "title": "Image Forgery Localization based on Multi-Scale Convolutional Neural Networks",
                    "authors": "Yaqi Liu, Qingxiao Guan, Xianfeng Zhao, Yun Cao",
                    "abstract": "In this paper, we propose to utilize Convolutional Neural Networks (CNNs) and the segmentation-based multi-scale analysis to locate tampered areas in digital images. First, to deal with color input sliding windows of different scales, we adopt a unified CNN architecture. Then, we elaborately design the training procedures of CNNs on sampled training patches. With a set of tampering detectors based on CNNs for different scales, a series of complementary tampering possibility maps can be generated. Last but not least, a segmentation-based method is proposed to fuse these maps and generate the final decision map. By exploiting the benefits of both the small-scale and large-scale analyses, the segmentation-based multi-scale analysis can lead to a performance leap in forgery localization of CNNs. Numerous experiments are conducted to demonstrate the effectiveness and efficiency of our method.",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/1706.07842"
                    },
                    "abstract_zh": "本文提出利用卷积神经网络和基于分割的多尺度分析来定位数字图像中的篡改区域。首先，为了处理不同尺度的颜色输入滑动窗口，我们采用了统一的CNN架构。然后，我们精心设计了细胞神经网络在样本训练块上的训练过程。利用一组基于不同尺度的细胞神经网络的篡改检测器，可以生成一系列互补的篡改可能性图。最后，提出了一种基于分割的方法来融合这些图并生成最终的决策图。通过利用小尺度和大尺度分析的优点，基于分割的多尺度分析可以导致CNN伪造定位的性能飞跃。大量实验证明了该方法的有效性和高效性。",
                    "title_zh": "基于多尺度卷积神经网络的图像伪造定位"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206013",
                    "title": "Densely Connected Convolutional Neural Network for Multi-purpose Image Forensics under Anti-forensic Attacks",
                    "authors": "Yifang Chen, Xiangui Kang, Z. Jane Wang, Qiong Zhang",
                    "abstract": "Multiple-purpose forensics has been attracting increasing attention worldwide. However, most of the existing methods based on hand-crafted features often require domain knowledge and expensive human labour and their performances can be affected by factors such as image size and JPEG compression. Furthermore, many anti-forensic techniques have been applied in practice, making image authentication more difficult. Therefore, it is of great importance to develop methods that can automatically learn general and robust features for image operation detectors with the capability of countering anti-forensics. In this paper, we propose a new convolutional neural network (CNN) approach for multi-purpose detection of image manipulations under anti-forensic attacks. The dense connectivity pattern, which has better parameter efficiency than the traditional pattern, is explored to strengthen the propagation of general features related to image manipulation detection. When compared with three state-of-the-art methods, experiments demonstrate that the proposed CNN architecture can achieve a better performance (i.e., with a 11% improvement in terms of detection accuracy under anti-forensic attacks). The proposed method can also achieve better robustness against JPEG compression with maximum improvement of 13% on accuracy under low-quality JPEG compression.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "多用途法医学在世界范围内越来越受到关注。然而，大多数基于手工制作特征的现有方法通常需要领域知识和昂贵的人工劳动，并且它们的性能会受到诸如图像大小和JPEG压缩等因素的影响。此外，许多反取证技术已经应用于实践，使得图像认证更加困难。因此，研究能够自动学习通用和鲁棒特征的方法，对于具有反取证能力的图像操作检测器具有重要意义。在本文中，我们提出了一种新的卷积神经网络(CNN)方法，用于在反取证攻击下对图像操作的多用途检测。探索比传统模式具有更好的参数效率的密集连接模式，以加强与图像操作检测相关的一般特征的传播。当与三种最先进的方法进行比较时，实验表明，所提出的CNN架构可以实现更好的性能(即，在抗取证攻击下，检测精度提高了11%)。该方法对JPEG压缩也有较好的鲁棒性，在低质量JPEG压缩下准确率最高提高了13%。",
                    "title_zh": "抗取证攻击下多用途图像取证的密集连接卷积神经网络"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206020",
                    "title": "Maintaining Rate-Distortion Optimization for IPM-Based Video Steganography by Constructing Isolated Channels in HEVC",
                    "authors": "Yu Wang, Yun Cao, Xianfeng Zhao, Zhoujun Xu, Meineng Zhu",
                    "abstract": "This paper proposes an effective intra-frame prediction mode (IPM)-based video steganography in HEVC to maintain rate-distortion optimization as well as improve empirical security. The unique aspect of this work and one that distinguishes it from prior art is that we capture the embedding impacts on neighboring prediction units, called inter prediction unit (inter-PU) embedding impacts caused by the predictive coding widespread employed in video coding standards, using a distortion measure. To avoid the emergence of neighboring IPMs mutually affecting each other within the same channel, three-layered isolated channels are established in terms of the property of IPM coding. According to theoretical analysis for embedding impacts on the current prediction unit, called intra prediction unit (intra-PU) embedding impacts on coding efficiency (both visual quality and compression efficiency), a novel distortion function purposely designed to discourage the embedding changes with impacts on adjacent channels is proposed to express the multi-level embedding impacts. Based on the defined distortion function, two-layered syndrome-trellis codes (STCs) are utilized in practical embedding implementation alternatively. Experimental results demonstrate that the proposed scheme outperforms other existing IPM-based video steganography in terms of rate-distortion optimization and empirical security.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "提出了一种有效的基于帧内预测模式(IPM)的HEVC视频隐写算法，以保持率失真优化并提高经验安全性。这项工作的独特方面以及与现有技术的区别在于，我们使用失真度量来捕获对相邻预测单元的嵌入影响，称为帧间预测单元(帧间预测单元)嵌入影响，该嵌入影响是由视频编码标准中广泛采用的预测编码引起的。为了避免在同一信道中出现相邻IPM相互影响的情况，根据IPM编码的特性建立了三层隔离信道。根据嵌入对当前预测单元的影响(称为帧内预测单元(intra-PU)嵌入对编码效率(视觉质量和压缩效率)的影响)的理论分析，提出了一种新的失真函数来表达多级嵌入影响，该函数旨在抑制嵌入变化对相邻通道的影响。基于定义的失真函数，在实际嵌入实现中交替使用两层校正子网格码(STCs)。实验结果表明，该方案在率失真优化和经验安全性方面优于其他现有的基于IPM的视频隐写算法。",
                    "title_zh": "通过在HEVC构建隔离信道保持基于IPM的视频隐写的率失真优化"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206015",
                    "title": "Exploring Non-Additive Distortion in Steganography",
                    "authors": "Tomás Pevný, Andrew D. Ker",
                    "abstract": "Leading steganography systems make use of the Syndrome-Trellis Code (STC) algorithm to minimize a distortion function while encoding the desired payload, but this constrains the distortion function to be additive. The Gibbs Embedding algorithm works for a certain class of non-additive distortion functions, but has its own limitations and is highly complex. In this short paper we show that it is possible to modify the STC algorithm in a simple way, to minimize a non-additive distortion function suboptimally. We use it for two examples. First, applying it to the S-UNIWARD distortion function, we show that it does indeed reduce distortion, compared with minimizing the additive approximation currently used in image steganography, but that it makes the payload more -- not less -- detectable. This parallels research attempting to use Gibbs Embedding for the same task. Second, we apply it to distortion defined by the output of a specific detector, as a counter-move in the steganography game. However, unless the Warden is forced to move first (by fixing the detector) this is highly detectable.",
                    "files": {
                        "openAccessPdf": "https://ora.ox.ac.uk/objects/uuid:ee03a351-669e-46a5-8727-c0e1fab6db89/download_file?safe_filename=PevnyKer-IHMMSec18.pdf&file_format=application%2Fpdf&type_of_work=Conference+item"
                    },
                    "abstract_zh": "领先的隐写术系统利用校正子网格码(STC)算法来最小化失真函数，同时对期望的有效载荷进行编码，但是这将失真函数约束为相加的。Gibbs嵌入算法适用于某一类非加性失真函数，但有其自身的局限性，并且非常复杂。在这篇简短的论文中，我们证明了以简单的方式修改STC算法，次优地最小化非加性失真函数是可能的。我们用它来举两个例子。首先，将其应用于s -单向失真函数，我们表明，与最小化当前图像隐写术中使用的加法近似相比，它确实减少了失真，但它使有效载荷更容易检测。这项parallels研究试图使用Gibbs嵌入来完成相同的任务。第二，我们将它应用于由特定检测器的输出定义的失真，作为隐写术游戏中的反棋。然而，除非管理员被迫首先移动(通过固定检测器)，否则这是高度可检测的。",
                    "title_zh": "探索隐写术中的非附加失真"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206017",
                    "title": "On the Relationship Between Embedding Costs and Steganographic Capacity",
                    "authors": "Andrew D. Ker",
                    "abstract": "Contemporary steganography in digital media is dominated by the framework of additive distortion minimization: every possible change is given a cost, and the embedder minimizes total cost using some variant of the Syndrome-Trellis Code algorithm. One can derive the relationship between the cost of each change c_i and the probability that it should be made pi_i, but the literature has not examined the relationship between the costs and the total capacity (secure payload size) of the cover. In this paper we attempt to uncover such a relationship, asymptotically, for a simple independent pixel model of covers. We consider a 'knowing' detector who is aware of the embedding costs, in which case sum pi_i^2 c_i should be optimized. It is shown that the total of the inverse costs, sum c_i^-1, along with the embedder's desired security against an optimal opponent, determines the asymptotic capacity. This result also recovers a Square Root Law. Some simple simulations confirm the relationship between costs and capacity in this ideal model.",
                    "files": {
                        "openAccessPdf": "https://ora.ox.ac.uk/objects/uuid:8d406ed0-948f-4422-9df0-dfe58332052f/download_file?safe_filename=Ker-IHMMSec18.pdf&file_format=application%2Fpdf&type_of_work=Conference+item"
                    },
                    "abstract_zh": "数字媒体中的当代隐写术受附加失真最小化框架的支配:每个可能的改变都给定了成本，并且嵌入器使用校正子网格码算法的某种变体来最小化总成本。人们可以推导出每次改变的成本c_i和它应该成为pi_i的概率之间的关系，但是文献没有检查成本和覆盖的总容量(安全有效载荷大小)之间的关系。在本文中，我们试图揭示这种关系，渐近，一个简单的独立像素模型的覆盖。我们考虑一个“知道”检测器，它知道嵌入成本，在这种情况下，和pi_i^2 c_i应该被优化。研究表明，总的逆成本，和c_i^-1，以及嵌入者对最优对手的期望安全性，决定了渐近容量。这个结果也恢复了平方根定律。一些简单的模拟证实了这个理想模型中成本和容量之间的关系。",
                    "title_zh": "嵌入成本与隐写容量的关系"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206016",
                    "title": "Real or Fake: Mobile Device Drug Packaging Authentication",
                    "authors": "Rudolf Schraml, Luca Debiasi, Andreas Uhl",
                    "abstract": "Shortly, within the member states of the European Union a serialization-based anti-counterfeiting system for pharmaceutical products will be introduced. This system requires a third party enabling to track serialized and enrolled instances of each product from the manufacturer to the consumer. An alternative to serialization is authentication of a product by classifying it as being real or fake using intrinsic or extrinsic features of the product. Thereby, one approach is packaging material classification using images of the packaging textures. While the basic feasibility has been proven recently, it is not clear if such an authentication system works with images captured with mobile devices. Thus, in this work mobile drug packaging authentication is investigated. The experimental evaluation provides results on single- and cross-sensor scenarios. Results indicate the principal feasibility and acknowledge open issues for a mobile device drug packaging authentication system.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "不久，欧盟成员国将引入一种基于电子监管的药品防伪系统。该系统要求第三方能够从制造商到消费者跟踪每个产品的序列化和注册实例。序列化的替代方法是通过使用产品的内在或外在特征将产品分类为真或假来认证产品。因此，一种方法是使用包装纹理的图像对包装材料进行分类。虽然最近已经证明了基本的可行性，但尚不清楚这种认证系统是否适用于移动设备拍摄的图像。因此，本文对移动药品包装认证进行了研究。实验评估提供了单传感器和交叉传感器场景的结果。结果表明了主要的可行性，并承认移动设备药品包装认证系统的开放问题。",
                    "title_zh": "真假:移动设备药品包装认证"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206019",
                    "title": "Forensic Analysis and Anonymisation of Printed Documents",
                    "authors": "Timo Richter, Stephan Escher, Dagmar Schönfeld, Thorsten Strufe",
                    "abstract": "Contrary to popular belief, the paperless office has not yet established itself. Printer forensics is therefore still an important field today to protect the reliability of printed documents or to track criminals. An important task of this is to identify the source device of a printed document. There are many forensic approaches that try to determine the source device automatically and with commercially available recording devices. However, it is difficult to find intrinsic signatures that are robust against a variety of influences of the printing process and at the same time can identify the specific source device. In most cases, the identification rate only reaches up to the printer model. For this reason we reviewed document colour tracking dots, an extrinsic signature embedded in nearly all modern colour laser printers. We developed a refined and generic extraction algorithm, found a new tracking dot pattern and decoded pattern information. Through out we propose to reuse document colour tracking dots, in combination with passive printer forensic methods. From privacy perspective we additional investigated anonymization approaches to defeat arbitrary tracking. Finally we propose our toolkitdeda which implements the entire workflow of extracting, analysing and anonymisation of a tracking dot pattern.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "与普遍的看法相反，无纸办公室还没有建立起来。因此，打印机取证仍然是当今保护打印文档可靠性或追踪罪犯的重要领域。其中的一项重要任务是识别打印文档的源设备。有许多取证方法试图自动确定源设备，并使用市场上可买到的记录设备。然而，很难找到既能抵抗打印过程的各种影响又能识别特定源设备的固有签名。在大多数情况下，识别率仅达到打印机型号。为此，我们回顾了文档彩色跟踪点，这是几乎所有现代彩色激光打印机中都嵌入的外部签名。我们开发了一个精炼和通用的提取算法，发现了一个新的跟踪点模式和解码模式信息。我们建议结合被动打印机取证方法，重新使用文档颜色跟踪点。从隐私的角度来看，我们额外调查了匿名化方法，以挫败任意跟踪。最后，我们提出了我们的toolkitdeda，它实现了跟踪点模式的提取、分析和匿名化的整个工作流程。",
                    "title_zh": "打印文档的取证分析和匿名化"
                },
                {
                    "url": "https://doi.org/10.1145/3206004.3206007",
                    "title": "Applicability of No-Reference Visual Quality Indices for Visual Security Assessment",
                    "authors": "Heinz Hofbauer, Andreas Uhl",
                    "abstract": "From literature it is known that full-reference visual quality indices are a poor fit for the estimation of visual security for selective encryption. The question remains whether no-reference visual quality indices can perform where full reference indices falter. Furthermore, no-reference visual quality indices frequently use machine learning to train a model of natural scene statistics. It would be of interest to be able to gauge the impact of learning statistics from selectively encrypted images on performance as quality estimators for encryption. In the following we will answer these two questions.",
                    "files": {
                        "openAccessPdf": "https://dl.acm.org/doi/pdf/10.1145/3206004.3206007"
                    },
                    "abstract_zh": "从文献可知，全参考视觉质量指数不太适合用于估计选择性加密的视觉安全性。问题是无参考视觉质量指数是否能在完全参考指数失效的情况下发挥作用。此外，无参考视觉质量指数经常使用机器学习来训练自然场景统计的模型。感兴趣的是能够测量从选择性加密的图像中学习统计对作为加密质量评估器的性能的影响。下面我们将回答这两个问题。",
                    "title_zh": "无参考视觉质量指数在视觉安全评估中的适用性"
                }
            ]
        }
    ],
    "2015": [
        {
            "dblp_url": "https://dblp.uni-trier.de/db/conf/ih/ihmmsec2015.html",
            "conf_title": "IH&MMSec 2015: Portland, OR, USA",
            "conf_url": "http://dl.acm.org/citation.cfm?id=2756601",
            "papers": [
                {
                    "url": "https://doi.org/10.1145/2756601.2756622",
                    "title": "Implications of Cyber Warfare",
                    "authors": "David Aucsmith",
                    "abstract": "",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "",
                    "title_zh": "网络战的含义"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756623",
                    "title": "IoT Privacy: Can We Regain Control?",
                    "authors": "Richard Chow",
                    "abstract": "Privacy is part of the Internet of Things (IoT) discussion because of the increased potential for sensitive data collection. In the vision for IoT, sensors penetrate ubiquitously into our physical lives and are funneled into big data systems for analysis. IoT data allows new benefits to end users - but also allows new inferences that erode privacy. The usual privacy mechanisms employed by users no longer work in the context of IoT. Users can no longer turn off a service (e.g., GPS), nor can they even turn off a device and expect to be safe from tracking. IoT means the monitoring and data collection is continuing even in the physical world. On a computer, we have at least a semblance of control and can in principle determine what applications are running and what data they are collecting. For example, on a traditional computer, we do have malware defenses - even if imperfect. Such defenses are strikingly absent for IoT, and it is unclear how traditional defenses can be applied to IoT. The issue of control is the main privacy problem in the context of IoT. Users generally don't know about all the sensors in the environment (with the potential exception of sensors in the user's own home). Present-day examples are WiFi MAC trackers and Google Glass, of course, but systems in the future will become even less discernible. In one sense, this is a security problem - detecting malicious devices or \"environmental malware.\" But it is also a privacy problem - many sensor devices in fact want to be transparent to users (for instance, by adopting a traditional notice-and-consent model), but are blocked by the lack of a natural communication channel to the user. Even assuming communication mechanisms, we have complex usability problems. For instance, we need to understand what sensors a person might be worried about and in what contexts. Audio capture at home is different from audio capture in a lecture hall. What processing is done on the sensor data may also be important. A camera capturing video for purposes of gesture recognition may be less worrisome than for purposes of facial recognition (and, of course, the user needs assurance on the proclaimed processing). Finally, given the large number of \"things\", the problem of notice fatigue must be dealt with, or notifications will become no more useful than browser security warnings. In this talk, we discuss all these problems in detail, together with potential solutions.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "隐私是物联网(IoT)讨论的一部分，因为敏感数据收集的可能性越来越大。在物联网的愿景中，传感器无处不在地渗透到我们的物理生活中，并被纳入大数据系统进行分析。物联网数据为最终用户带来了新的好处，但也带来了侵蚀隐私的新推论。用户通常采用的隐私机制在物联网环境中不再有效。用户不能再关闭服务(例如GPS)，他们甚至也不能关闭设备并期望免受跟踪。物联网意味着即使在现实世界中，监控和数据收集仍在继续。在电脑上，我们至少有表面上的控制权，原则上可以决定哪些应用程序在运行，哪些数据在收集。例如，在传统计算机上，我们确实有恶意软件防御——即使不完美。对于物联网来说，这种防御明显缺乏，而且不清楚如何将传统防御应用于物联网。控制问题是物联网环境中的主要隐私问题。用户通常不知道环境中的所有传感器(除了用户自己家中的传感器)。今天的例子是WiFi MAC追踪器和谷歌眼镜，当然，但未来的系统将变得更加难以辨别。在某种意义上，这是一个安全问题——检测恶意设备或“环境恶意软件”但这也是一个隐私问题——许多传感器设备实际上希望对用户透明(例如，通过采用传统的通知和同意模式)，但由于缺乏与用户的自然通信渠道而受阻。即使假设通信机制，我们也有复杂的可用性问题。例如，我们需要了解一个人可能会担心什么传感器，以及在什么情况下会担心。家中的音频捕获不同于演讲厅中的音频捕获。对传感器数据进行什么处理也可能是重要的。用于手势识别的摄像机捕捉视频可能没有用于面部识别的摄像机那么令人担忧(当然，用户需要对所宣称的处理有所保证)。最后，考虑到大量的“事情”，必须解决通知疲劳的问题，否则通知将变得比浏览器安全警告更有用。在本次讲座中，我们将详细讨论所有这些问题，以及潜在的解决方案。",
                    "title_zh": "物联网隐私:我们能重获控制权吗？"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756620",
                    "title": "Improving Steganographic Security by Synchronizing the Selection Channel",
                    "authors": "Tomás Denemark, Jessica J. Fridrich",
                    "abstract": "This paper describes a general method for increasing the security of additive steganographic schemes for digital images represented in the spatial domain. Additive embedding schemes first assign costs to individual pixels and then embed the desired payload by minimizing the sum of costs of all changed pixels. The proposed framework can be applied to any such scheme -- it starts with the cost assignment and forms a non-additive distortion function that forces adjacent embedding changes to synchronize. Since the distortion function is purposely designed as a sum of locally supported potentials, one can use the Gibbs construction to realize the embedding in practice. The beneficial impact of synchronizing the embedding changes is linked to the fact that modern steganalysis detectors use higher-order statistics of noise residuals obtained by filters with sign-changing kernels and to the fundamental difficulty of accurately estimating the selection channel of a non-additive embedding scheme implemented with several Gibbs sweeps. Both decrease the accuracy of detectors built using rich media models, including their selection-channel-aware versions.",
                    "files": {
                        "openAccessPdf": "http://dde.binghamton.edu/tomasD/pdf/ih-acm-2015.pdf"
                    },
                    "abstract_zh": "本文描述了一种提高空间域数字图像附加隐写方案安全性的通用方法。加法嵌入方案首先将成本分配给各个像素，然后通过最小化所有变化像素的成本总和来嵌入期望的有效载荷。所提出的框架可以应用于任何这样的方案——它从成本分配开始，并形成非加性失真函数，该函数迫使相邻的嵌入变化同步。由于畸变函数被有意设计为局部支持势的和，因此在实践中可以使用吉布斯构造来实现嵌入。同步嵌入变化的有益影响与以下事实有关:现代隐写分析检测器使用由具有符号变化核的滤波器获得的噪声残差的高阶统计量，以及准确估计用几次吉布斯扫描实现的非加法嵌入方案的选择信道的基本困难。两者都降低了使用富媒体模型构建的检测器的准确性，包括它们的选择通道感知版本。",
                    "title_zh": "通过同步选择通道提高隐写安全性"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756608",
                    "title": "Steganalysis of Adaptive JPEG Steganography Using 2D Gabor Filters",
                    "authors": "Xiaofeng Song, Fenlin Liu, Chunfang Yang, Xiangyang Luo, Yi Zhang",
                    "abstract": "Adaptive JPEG steganographic schemes are difficult to preserve the image texture features in all scales and orientations when the embedding changes are constrained to the complicated texture regions, then a steganalysis feature extraction method is proposed based on 2 dimensional (2D) Gabor filters. The 2D Gabor filters have certain optimal joint localization properties in the spatial domain and in the spatial frequency domain. They can describe the image texture features from different scales and orientations, therefore the changes of image statistical characteristics caused by steganography embedding can be captured more effectively. For the proposed feature extraction method, the decompressed JPEG image is filtered by 2D Gabor filters with different scales and orientations firstly. Then, the histogram features are extracted from all the filtered images.Lastly, the ensemble classifier is used to assemble the proposed steganalysis feature as well as the final steganalyzer. The experimental results show that the proposed steganalysis feature can achieve a competitive performance by comparing with the other steganalysis features when they are used for the detection performance of adaptive JPEG steganography such as UED, JUNIWARD and SI-UNIWARD.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "针对自适应JPEG隐写算法难以在复杂纹理区域保持图像纹理特征的问题，提出了一种基于二维2D Gabor滤波器的隐写分析特征提取方法。2D Gabor滤波器在空间域和空间频域中具有某些最佳的联合定位特性。它们可以从不同的尺度和方向描述图像的纹理特征，因此可以更有效地捕捉到因隐写嵌入引起的图像统计特征的变化。对于所提出的特征提取方法，首先用不同尺度和方向的2D Gabor滤波器对解压后的JPEG图像进行滤波。然后，从所有滤波图像中提取直方图特征。最后，使用集成分类器来组合所提出的隐写分析特征以及最终的隐写分析器。实验结果表明，与其他隐写分析特征相比，该隐写分析特征在用于自适应JPEG隐写检测性能方面具有优势，如UED、朱尼沃德和SI-UNIWARD。",
                    "title_zh": "基于2D Gabor滤波器的自适应JPEG隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756609",
                    "title": "Video Steganography Based on Optimized Motion Estimation Perturbation",
                    "authors": "Yun Cao, Hong Zhang, Xianfeng Zhao, Haibo Yu",
                    "abstract": "In this paper, a novel motion vector-based video steganographic scheme is proposed, which is capable of withstanding the current best statistical detection method. With this scheme, secret message bits are embedded into motion vector (MV) values by slightly perturbing their motion estimation (ME) processes. In general, two measures are taken for steganographic security (statistical undetectability) enhancement. First, the ME perturbations are optimized ensuring the modified MVs are still local optimal, which essentially makes targeted detectors ineffective. Secondly, to minimize the overall embedding impact under a given relative payload, a double-layered coding structure is used to control the ME perturbations. Experimental results demonstrate that the proposed scheme achieves a much higher level of security compared with other existing MV-based approaches. Meanwhile, the reconstructed visual quality and the coding efficiency are slightly affected as well.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "本文提出了一种新的基于运动矢量的视频隐写方案，能够抵抗目前最好的统计检测方法。在这种方案中，秘密信息比特通过轻微扰动它们的运动估计(me)过程被嵌入到运动矢量(MV)值中。通常，对于隐写术安全性(统计不可检测性)增强，采取两种措施。首先，优化ME扰动，确保修改的mv仍然是局部最优的，这实质上使得目标检测器无效。其次，为了在给定的相对有效载荷下最小化整体嵌入影响，使用双层编码结构来控制ME扰动。实验结果表明，与现有的基于MV的方法相比，该方案具有更高的安全性。同时，重建的视觉质量和编码效率也受到轻微影响。",
                    "title_zh": "基于优化运动估计扰动的视频隐写"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756621",
                    "title": "Effect of Imprecise Knowledge of the Selection Channel on Steganalysis",
                    "authors": "Vahid Sedighi, Jessica J. Fridrich",
                    "abstract": "It has recently been shown that steganalysis of content-adaptive steganography can be improved when the Warden incorporates in her detector the knowledge of the selection channel -- the probabilities with which the individual cover elements were modified during embedding. Such attacks implicitly assume that the Warden knows at least approximately the payload size. In this paper, we study the loss of detection accuracy when the Warden uses a selection channel that was imprecisely determined either due to lack of information or the stego changes themselves. The loss is investigated for two types of qualitatively different detectors -- binary classifiers equipped with selection-channel-aware rich models and optimal detectors derived using the theory of hypothesis testing from a cover model. Two different embedding paradigms are addressed -- steganography based on minimizing distortion and embedding that minimizes the detectability of an optimal detector within a chosen cover model. Remarkably, the experimental and theoretical evidence are qualitatively in agreement across different embedding methods, and both point out that inaccuracies in the selection channel do not have a strong effect on steganalysis detection errors. It pays off to use imprecise selection channel rather than none. Our findings validate the use of selection-channel-aware detectors in practice.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "最近的研究表明，当管理员在她的检测器中加入选择信道的知识时，内容自适应隐写术的隐写分析可以得到改善，选择信道是在嵌入期间各个覆盖元素被修改的概率。这种攻击隐含地假设管理员至少知道大约的有效载荷大小。在本文中，我们研究了当监管者使用一个选择信道时检测精度的损失，该选择信道是由于缺乏信息或隐写本身的变化而不精确确定的。研究了两种性质不同的检测器的损失-配备有选择信道感知丰富模型的二元分类器和使用假设检验理论从覆盖模型导出的最佳检测器。提出了两种不同的嵌入范例——基于最小化失真的隐写术和最小化选择的覆盖模型内的最佳检测器的可检测性的嵌入。值得注意的是，在不同的嵌入方法中，实验和理论证据在定性上是一致的，并且都指出选择通道中的不准确性对隐写分析检测错误没有强烈的影响。使用不精确的选择通道总比没有通道好。我们的发现验证了选择通道感知检测器在实践中的应用。",
                    "title_zh": "选择通道的不精确知识对隐写分析的影响"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756604",
                    "title": "On Characterizing and Measuring Out-of-Band Covert Channels",
                    "authors": "Brent C. Carrara, Carlisle M. Adams",
                    "abstract": "A methodology for characterizing and measuring out-of-band covert channels (OOB-CCs) is proposed and used to evaluate covert-acoustic channels (i.e., covert channels established using speakers and microphones). OOB-CCs are low-probability of detection/low-probability of interception channels established using commodity devices that are not traditionally used for communication (e.g., speaker and microphone, display and FM radio, etc.). To date, OOB-CCs have been declared \"covert\" if the signals used to establish these channels could not be perceived by a human adversary. This work examines OOB-CCs from the perspective of a passive adversary and argues that a different methodology is required in order to effectively assess OOB-CCs. Traditional communication systems are measured by their capacity and bit error rate; while important parameters, they do not capture the key measures of OOB-CCs: namely, the probability of an adversary detecting the channel and the amount of data that two covertly communicating parties can exchange without being detected. As a result, the adoption of the measure steganographic capacity is proposed and used to measure the amount of data (in bits) that can be transferred through an OOB-CC before a passive adversary's probability of detecting the channel reaches a given threshold. The theoretical steganographic capacity for discrete memoryless channels as well as additive white Gaussian noise channels is calculated in this paper and a case study is performed to measure the steganographic capacity of OOB covert-acoustic channels, when a passive adversary uses an energy detector to detect the covert communication. The case study reveals the conditions under which the covertly communicating parties can achieve perfect steganography (i.e., conditions under which data can be communicated without risk of detection).",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "提出了一种用于表征和测量带外隐蔽信道(OOB-CCs)的方法，并用于评估隐蔽声学信道(即使用扬声器和麦克风建立的隐蔽信道)。OOB-CCs是使用传统上不用于通信的商用设备(例如扬声器和麦克风、显示器和调频收音机等)建立的低检测概率/低截获概率信道。).迄今为止，如果用来建立这些信道的信号不能被人类对手察觉，OOB-CCs就被宣布为“秘密的”。这项工作从被动对手的角度研究了OOB-CCs，并认为需要不同的方法来有效地评估OOB-CCs。传统的通信系统是以其容量和误码率来衡量的；虽然这些参数很重要，但它们并没有抓住OOB-CCs的关键指标:也就是说，对手检测到信道的概率，以及两个秘密通信方可以在不被检测到的情况下交换的数据量。因此，提出采用测量隐写容量，并用于测量在被动对手检测到信道的概率达到给定阈值之前，可以通过OOB-CC传输的数据量(以比特为单位)。本文计算了离散无记忆信道和加性高斯白噪声信道的理论隐写容量，并通过一个实例研究了被动敌手使用能量检测器检测隐蔽通信时OOB隐蔽声信道的隐写容量。该案例研究揭示了秘密通信方可以实现完美隐写术的条件(即，可以在没有检测风险的情况下传输数据的条件)。",
                    "title_zh": "带外隐蔽信道的表征和测量"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756605",
                    "title": "LiHB: Lost in HTTP Behaviors - A Behavior-Based Covert Channel in HTTP",
                    "authors": "Yao Shen, Liusheng Huang, Fei Wang, Xiaorong Lu, Wei Yang, Lu Li",
                    "abstract": "The application-layer covert channels have been extensively studied in recent years. Information-hiding in ubiquitous application packets can significantly improve the capacity of covert channels. However, the undetectability is still a knotty problem, because the existing covert channels are all frustrated by proper detection schemes. In this paper, we propose LiHB, a behavior-based covert channel in HTTP. When a client is browsing a website and downloading webpage objects, we can reveal some fluctuation behaviors that the distribution relationship between the ports opening and HTTP requests are flexible. Based on combinatorial nature of distributing N HTTP requests over M HTTP flows, such fluctuation can be exploited by LiHB channel to encode covert messages, which can obtain high stealthiness. Besides, LiHB achieves a considerable and controllable capacity by setting the number of webpage objects and HTTP flows. Compared with existing techniques, LiHB is the first covert channel implemented based on the unsuspicious behavior of browsers, the most important application-layer software. Because most HTTP proxies are using NAPT techniques, LiHB can also operate well even when a proxy is equipped, which poses a serious threat to individual privacy. Experimental results show that LiHB covert channel achieves a good capacity, reliability and high undetectability.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "近年来，应用层隐蔽通道得到了广泛的研究。在无处不在的应用包中隐藏信息可以显著提高隐蔽信道的容量。然而，不可检测性仍然是一个棘手的问题，因为现有的隐蔽信道都被适当的检测方案所挫败。本文提出了一种基于行为的HTTP隐蔽通道LiHB。当客户端浏览网站和下载网页对象时，我们可以发现一些波动行为，即端口开放和HTTP请求之间的分配关系是灵活的。基于将N个HTTP请求分布在M个HTTP流上的组合性质，LiHB信道可以利用这种波动性对隐蔽消息进行编码，从而获得较高的隐蔽性。此外，LiHB通过设置网页对象和HTTP流的数量，实现了可观且可控的容量。与现有技术相比，LiHB是第一个基于浏览器(最重要的应用层软件)的可疑行为实现的隐蔽通道。因为大多数HTTP代理都使用NAPT技术，所以即使配备了代理，LiHB也可以很好地运行，这对个人隐私构成了严重威胁。实验结果表明，LiHB隐蔽信道具有良好的容量、可靠性和较高的不可检测性。",
                    "title_zh": "LiHB:迷失在HTTP行为中——HTTP中基于行为的隐蔽通道"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756606",
                    "title": "Visual Honey Encryption: Application to Steganography",
                    "authors": "Ji Won Yoon, Hyoungshick Kim, Hyun-Ju Jo, Hyelim Lee, Kwangsu Lee",
                    "abstract": "Honey encryption (HE) is a new technique to overcome the weakness of conventional password-based encryption (PBE). However, conventional honey encryption still has the limitation that it works only for binary bit streams or integer sequences because it uses a fixed distribution-transforming encoder (DTE). In this paper, we propose a variant of honey encryption called visual honey encryption which employs an adaptive DTE in a Bayesian framework so that the proposed approach can be applied to more complex domains including images and videos. We applied this method to create a new steganography scheme which significantly improves the security level of traditional steganography.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "蜂蜜加密(HE)是一种克服传统的基于口令的加密(PBE)缺点的新技术。然而，传统的honey加密仍然有局限性，因为它使用固定的分布变换编码器(DTE ),所以它只对二进制比特流或整数序列有效。在本文中，我们提出了一种称为视觉蜂蜜加密的蜂蜜加密变体，它在贝叶斯框架中采用了自适应DTE，因此所提出的方法可以应用于更复杂的领域，包括图像和视频。我们应用这种方法创建了一种新的隐写术方案，显著提高了传统隐写术的安全性。",
                    "title_zh": "视觉蜜糖加密:在隐写术中的应用"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756612",
                    "title": "Flicker Forensics for Pirate Device Identification",
                    "authors": "Adi Hajj-Ahmad, Séverine Baudry, Bertrand Chupeau, Gwenaël J. Doërr",
                    "abstract": "Cryptography-based content protection is an efficient means to protect multimedia content during transport. Nevertheless, content is eventually decrypted at rendering time, leaving it vulnerable to piracy e.g. using a camcorder to record movies displayed on an LCD screen. Such type of piracy naturally imprints a visible flicker signal in the pirate video due to the interplay between the rendering and acquisition devices. The parameters of such flicker are inherently tied to the characteristics of the pirate devices such as the back-light of the LCD screen and the read-out time of the camcorder. In this article, we introduce a forensic methodology to estimate such parameters by analyzing the flicker signal present in pirate recordings. Experimental results clearly showcase that the accuracy of these estimation techniques offers efficient means to tell-tale which devices have been used for piracy thanks to the variety of factory settings used by consumer electronics manufacturers.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "基于密码术的内容保护是在传输期间保护多媒体内容的有效手段。然而，内容最终会在呈现时被解密，这使得它容易受到盗版的攻击，例如使用便携式摄像机来记录在LCD屏幕上显示的电影。由于呈现和获取设备之间的相互作用，这种类型的盗版自然会在盗版视频中留下可见的闪烁信号。这种闪烁的参数固有地依赖于盗版设备的特性，例如LCD屏幕的背光和摄像机的读出时间。在本文中，我们介绍了一种取证方法，通过分析盗版录音中存在的闪烁信号来估计这些参数。实验结果清楚地表明，由于消费电子产品制造商所使用的各种工厂设置，这些估计技术的准确性提供了有效的手段来辨别哪些设备已经被用于盗版。",
                    "title_zh": "用于盗版设备识别的闪烁取证"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756614",
                    "title": "Enhancing Sensor Pattern Noise for Source Camera Identification: An Empirical Evaluation",
                    "authors": "Bei-Bei Liu, Xingjie Wei, Jeff Yan",
                    "abstract": "The sensor pattern noise (SPN) based source camera identification technique has been well established. The common practice is to subtract a denoised image from the original one to get an estimate of the SPN. Various techniques to improve SPN's reliability have previously been proposed. Identifying the most effective technique is important, for both researchers and forensic investigators in law enforcement agencies. Unfortunately, the results from previous studies have proven to be irreproducible and incomparable dash there is no consensus on which technique works the best. Here, we extensively evaluate various ways of enhancing the SPN by using the public Dresden database. We identify which enhancing methods are more effective and offer some insights into the behavior of SPN. For example, we find that the most effective enhancing methods share a common strategy of spectrum flattening. We also show that methods that only aim at reducing the contamination from image content do not lead to satisfying results, since the non-unique artifacts (NUA) among different cameras are the major troublemaker to the identification performance. While there is a trend of employing sophisticate methods to predict the impact of image content, our results suggest that more effort should be invested to tame the NUAs.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "已经很好地建立了基于传感器模式噪声(SPN)的源相机识别技术。通常的做法是从原始图像中减去去噪后的图像，以获得SPN的估计值。先前已经提出了各种提高SPN可靠性的技术。对于执法机构的研究人员和法医调查人员来说，确定最有效的技术非常重要。不幸的是，以前的研究结果已被证明是不可重复的和无与伦比的dash。对于哪种技术效果最好还没有达成共识。在这里，我们通过使用公共的Dresden数据库广泛地评估了增强SPN的各种方法。我们确定了哪些增强方法更有效，并对SPN的行为提供了一些见解。例如，我们发现最有效的增强方法共享频谱平坦化的共同策略。我们还表明，仅旨在减少图像内容污染的方法不会导致令人满意的结果，因为不同相机之间的非唯一伪影(NUA)是识别性能的主要麻烦制造者。虽然有一种趋势是采用复杂的方法来预测图像内容的影响，但我们的结果表明，应该投入更多的努力来驯服nua。",
                    "title_zh": "增强源摄像机识别的传感器模式噪声:经验评估"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756615",
                    "title": "ForeMan, a Versatile and Extensible Database System for Digitized Forensics Based on Benchmarking Properties",
                    "authors": "Christian Arndt, Stefan Kiltz, Jana Dittmann, Robert Fischer",
                    "abstract": "To benefit from new opportunities offered by the digitalization of forensic disciplines, the challenges especially w.r.t. comprehensibility and searchability have to be met. Important tools in this forensic process are databases containing digitized representations of physical crime scene traces. We present ForeMan, an extensible database system for digitized forensics handling separate databases and enabling intra and inter trace type searches. It now contains 762 fiber data sets and 27 fingerprint data sets (anonymized time series). Requirements of the digitized forensic process model are mapped to design aspects and conceptually modeled around benchmarking properties. A fiber categorization scheme is used to structure fiber data according to forensic use case identification. Our research extends the benchmarking properties by fiber fold shape derived from the application field of fibers (part of micro traces) and sequence number derived from the application field of time series analysis for fingerprint aging research. We identify matching data subsets from both digitized trace types and introduce the terms of entity-centered and spatial-centered information. We show how combining two types of digitized crime scene traces (fiber and fingerprint data) can give new insights for research and casework and discuss requirements for other trace types such as firearm and toolmarks.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "为了从法医学科数字化提供的新机遇中受益，必须应对挑战，尤其是可理解性和可搜索性。这一法医过程中的重要工具是包含犯罪现场痕迹数字化表示的数据库。我们介绍ForeMan，这是一个可扩展的数据库系统，用于数字化取证，处理独立的数据库，并支持内部和内部跟踪类型的搜索。它现在包含762个纤维数据集和27个指纹数据集(匿名时间序列)。数字化取证过程模型的需求被映射到设计方面，并围绕基准属性进行概念建模。纤程分类方案用于根据法庭用例识别来构造纤程数据。我们的研究通过从纤维(部分微痕迹)的应用领域得到的纤维褶皱形状和从用于指纹老化研究的时间序列分析的应用领域得到的序列号来扩展基准性能。我们从两种数字化轨迹类型中识别匹配的数据子集，并引入以实体为中心和以空间为中心的信息这两个术语。我们展示了如何结合两种类型的数字化犯罪现场痕迹(纤维和指纹数据)可以为研究和办案提供新的见解，并讨论了对其他痕迹类型的要求，如枪支和工具痕迹。",
                    "title_zh": "ForeMan是一个通用的可扩展数据库系统，用于基于基准属性的数字化取证"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756603",
                    "title": "Optimal Sequential Fingerprinting: Wald vs. Tardos",
                    "authors": "Thijs Laarhoven",
                    "abstract": "We study sequential collusion-resistant fingerprinting, where the fingerprinting code is generated in advance but accusations may be made between rounds, and show that in this setting both the dynamic Tardos scheme and schemes building upon Wald's sequential probability ratio test (SPRT) are asymptotically optimal. We further compare these two approaches to sequential fingerprinting, highlighting differences between the two schemes. Based on these differences, we argue that Wald's scheme should in general be preferred over the dynamic Tardos scheme, even though both schemes have their merits. As a side result, we derive an optimal sequential group testing method for the classical model, which can easily be generalized to different group testing models.",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/1502.03722"
                    },
                    "abstract_zh": "我们研究了顺序抗共谋指纹识别，其中指纹码是预先生成的，但可以在两轮之间进行指控，并表明在这种情况下，动态Tardos方案和基于Wald的顺序概率比检验(SPRT)的方案都是渐近最优的。我们进一步比较了这两种方法的序列指纹，突出了两种方案之间的差异。基于这些差异，我们认为Wald方案通常应该优于动态Tardos方案，尽管这两种方案都有其优点。作为一个附带结果，我们导出了经典模型的最优序贯分组检验方法，该方法可以很容易地推广到不同的分组检验模型。",
                    "title_zh": "最优序列指纹:Wald与Tardos"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756611",
                    "title": "Universal Threshold Calculation for Fingerprinting Decoders using Mixture Models",
                    "authors": "Marcel Schäfer, Sebastian Mair, Waldemar Berchtold, Martin Steinebach",
                    "abstract": "Collusion attacks on watermarked media copies are commonly countered by probabilistically generated fingerprinting codes and appropriate tracing algorithms. The latter calculates accusation scores representing the suspiciousness of the fingerprints. In a 'detect many' scenario a threshold decides which scores are associated to the colluders. This work proposes a universal method to calculate thresholds for different decoders solely with knowledge of the accusation scores from the actual attack. Applying mixture models on the scores, the threshold is set up satisfying the selected error probabilities. It is independent from the fingerprint generation and can be applied at any decoder. Also no knowledge about the number of attackers or their strategy is needed.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "对水印媒体拷贝的共谋攻击通常通过概率生成的指纹代码和适当的追踪算法来对抗。后者计算代表指纹可疑性的指控分数。在“检测多个”场景中，阈值决定哪些分数与共谋相关联。这项工作提出了一个通用的方法来计算不同的解码器的阈值，只有知识的指控分数从实际的攻击。对分数应用混合模型，设置满足所选错误概率的阈值。它独立于指纹生成，可以应用于任何解码器。也不需要知道攻击者的数量或他们的策略。",
                    "title_zh": "使用混合模型的指纹解码器的通用阈值计算"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756607",
                    "title": "3D Print-Scan Resilient Watermarking Using a Histogram-Based Circular Shift Coding Structure",
                    "authors": "Jong-Uk Hou, Do-Gon Kim, Sunghee Choi, Heung-Kyu Lee",
                    "abstract": "3D printing content is a new form of content being distributed in digital as well as analog domains. Therefore, its security is the biggest technical challenge of the content distribution service. In this paper, we analyze the 3D print-scan process, and we organize possible distortions according to the processes with respect to 3D mesh watermarking. Based on the analysis, we propose a circular shift coding structure for the 3D model. When the rotating disks of the coding structure are aligned in parallel to the layers of the 3D printing, the structure preserves a statistical feature of each disk from the layer dividing process. Based on the circular shift coding structure, we achieve a 3D print-scan resilient watermarking scheme. In experimental tests, the proposed scheme is robust against such signal processing, and cropping attacks. Furthermore, the embedded information is not lost after 3D print-scan process.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "3D打印内容是在数字和模拟领域分发的一种新的内容形式。因此，其安全性是内容分发服务的最大技术挑战。在本文中，我们分析了3D打印-扫描过程，并根据与3D网格水印相关的过程来组织可能的失真。在此基础上，我们提出了一种三维模型的循环移位编码结构。当编码结构的旋转盘平行于3D打印的层对齐时，该结构保留了来自层划分过程的每个盘的统计特征。基于循环移位编码结构，我们实现了一种3D打印扫描弹性水印方案。在实验测试中，所提出的方案对这种信号处理和剪切攻击是鲁棒的。此外，在3D打印-扫描过程之后，嵌入的信息不会丢失。",
                    "title_zh": "使用基于直方图的循环移位编码结构的3D打印扫描弹性水印"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756613",
                    "title": "End-to-Display Encryption: A Pixel-Domain Encryption with Security Benefit",
                    "authors": "Sebastian Matthias Burg, Dustin Peterson, Oliver Bringmann",
                    "abstract": "Providing secure access to confidential information is extremely difficult, notably when regarding weak endpoints and users. With the increasing number of corporate espionage cases and data leaks, a usable approach enhancing the security of data on endpoints is needed. In this paper we present our implementation for providing a new level of security for confidential documents that are viewed on a display. We call this End-to-Display Encryption (E2DE). E2DE encrypts images in the pixel-domain before transmitting them to the user. These images can then be displayed by arbitrary image viewers and are sent to the display. On the way to the display, the data stream is analyzed and the encrypted pixels are decrypted depending on a private key stored on a chip card inserted in the receiver, creating a viewable representation of the confidential data on the display, without decrypting the information on the computer itself. We implemented a prototype on a Digilent Atlys FPGA Board supporting resolutions up to Full HD.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "提供对机密信息的安全访问是极其困难的，尤其是在涉及脆弱的端点和用户时。随着企业间谍案件和数据泄露数量的增加，需要一种可用的方法来增强终端数据的安全性。在本文中，我们展示了为在显示器上查看的机密文档提供新的安全级别的实现。我们称之为端到端显示加密(E2DE)。E2DE在将图像传输给用户之前，先在像素域对图像进行加密。然后，这些图像可以由任意图像查看器显示，并被发送到显示器。在到达显示器的途中，数据流被分析，加密的像素根据存储在插入接收器中的芯片卡上的私钥被解密，在显示器上创建机密数据的可视表示，而不解密计算机本身上的信息。我们在Digilent Atlys FPGA板上实现了一个原型，支持最高全高清分辨率。",
                    "title_zh": "端到端显示加密:具有安全优势的像素域加密"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756602",
                    "title": "Touch-based Static Authentication Using a Virtual Grid",
                    "authors": "William F. Bond, Ahmed Awad E. A.",
                    "abstract": "Keystroke dynamics is a subfield of computer security in which the cadence of the typist's keystrokes are used to determine authenticity. The static variety of keystroke dynamics uses typing patterns observed during the typing of a password or passphrase. This paper presents a technique for static authentication on mobile tablet devices using neural networks for analysis of keystroke metrics. Metrics used in the analysis of typing are monographs, digraphs, and trigraphs. Monographs as we define them consist of the time between the press and release of a single key, coupled with the discretized x-y location of the keystroke on the tablet. A digraph is the duration between the presses of two consecutively pressed keys, and a trigraph is the duration between the press of a key and the press of a key two keys later. Our technique combines the analysis of monographs, digraphs, and trigraphs to produce a confidence measure. Our best equal error rate for distinguishing users from impostors is 9.3% for text typing, and 9.0% for a custom experiment setup that is discussed in detail in the paper.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "击键动力学是计算机安全的一个子领域，其中打字员击键的节奏被用来确定真实性。静态种类的击键动力学使用在键入密码或短语期间观察到的键入模式。提出了一种在移动平板设备上使用神经网络分析击键度量进行静态认证的技术。在分型分析中使用的度量标准是专论、二图和三图。我们定义的专论包括按下和释放单个键之间的时间，以及输入板上击键的离散x-y位置。有向图是连续按下两个键之间的持续时间，三向图是按下一个键和两个键之后再按下一个键之间的持续时间。我们的技术结合了专论、有向图和三向图的分析来产生一个置信度。我们区分用户和冒名顶替者的最佳等错误率，对于文本输入是9.3%，对于本文详细讨论的自定义实验设置是9.0%。",
                    "title_zh": "使用虚拟网格的基于触摸的静态认证"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756616",
                    "title": "SATTVA: SpArsiTy inspired classificaTion of malware VAriants",
                    "authors": "Lakshmanan Nataraj, S. Karthikeyan, B. S. Manjunath",
                    "abstract": "There is an alarming increase in the amount of malware that is generated today. However, several studies have shown that most of these new malware are just variants of existing ones. Fast detection of these variants plays an effective role in thwarting new attacks. In this paper, we propose a novel approach to detect malware variants using a sparse representation framework. Exploiting the fact that most malware variants have small differences in their structure, we model a new/unknown malware sample as a sparse linear combination of other malware in the training set. The class with the least residual error is assigned to the unknown malware. Experiments on two standard malware datasets, Malheur dataset and Malimg dataset, show that our method outperforms current state of the art approaches and achieves a classification accuracy of 98.55\\% and 92.83\\% respectively. Further, by using a confidence measure to reject outliers, we obtain 100\\% accuracy on both datasets, at the expense of throwing away a small percentage of outliers. Finally, we evaluate our technique on two large scale malware datasets: Offensive Computing dataset (2,124 classes, 42,480 malware) and Anubis dataset (209 classes, 36,784 samples). On both datasets our method obtained an average classification accuracy of 77\\%, thus making it applicable to real world malware classification.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "如今，恶意软件的数量正以惊人的速度增长。然而，几项研究表明，这些新的恶意软件大多只是现有恶意软件的变种。快速检测这些变种在阻止新的攻击中起着有效的作用。在本文中，我们提出了一种新的方法来检测使用稀疏表示框架的恶意软件变种。利用大多数恶意软件变体在结构上存在微小差异的事实，我们将新的/未知的恶意软件样本建模为训练集中其他恶意软件的稀疏线性组合。具有最小残差的类被分配给未知恶意软件。在Malheur数据集和Malimg数据集上的实验表明，该方法的分类准确率分别达到98.55%和92.83%，优于现有的方法。此外，通过使用置信度度量来拒绝异常值，我们在两个数据集上都获得了100%的准确性，代价是丢弃了一小部分异常值。最后，我们在两个大规模恶意软件数据集上评估了我们的技术:攻击性计算数据集(2，124个类别，42，480个恶意软件)和Anubis数据集(209个类别，36，784个样本)。在两个数据集上，我们的方法获得了77%的平均分类准确率，从而使其适用于真实世界的恶意软件分类。",
                    "title_zh": "SATTVA:稀疏性启发的恶意软件变种分类"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756618",
                    "title": "Thumbnail-Preserving Encryption for JPEG",
                    "authors": "Charles V. Wright, Wu-chi Feng, Feng Liu",
                    "abstract": "With more and more data being stored in the cloud, securing multimedia data is becoming increasingly important. Use of existing encryption methods with cloud services is possible, but makes many web-based applications difficult or impossible to use. In this paper, we propose a new image encryption scheme specially designed to protect JPEG images in cloud photo storage services. Our technique allows efficient reconstruction of an accurate low-resolution thumbnail from the ciphertext image, but aims to prevent the extraction of any more detailed information. This will allow efficient storage and retrieval of image data in the cloud but protect its contents from outside hackers or snooping cloud administrators. Experiments of the proposed approach using an online selfie database show that it can achieve a good balance of privacy, utility, image quality, and file size.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "随着越来越多的数据存储在云中，保护多媒体数据变得越来越重要。将现有的加密方法用于云服务是可能的，但这使得许多基于网络的应用程序难以或无法使用。在本文中，我们提出了一种新的图像加密方案，专门用于保护云照片存储服务中的JPEG图像。我们的技术允许从密文图像中有效地重建精确的低分辨率缩略图，但目的是防止提取任何更详细的信息。这将允许在云中有效地存储和检索图像数据，但保护其内容免受外部黑客或窥探的云管理员的攻击。使用在线自拍数据库的实验表明，该方法可以在隐私、实用性、图像质量和文件大小之间取得良好的平衡。",
                    "title_zh": "JPEG的缩略图保留加密"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756610",
                    "title": "On Elliptic Curve Based Untraceable RFID Authentication Protocols",
                    "authors": "Eun-Kyung Ryu, Dae-Soo Kim, Kee-Young Yoo",
                    "abstract": "An untraceable RFID authentication scheme allows a legitimate reader to authenticate a tag, and at the same time it assures the privacy of the tag against unauthorized tracing. In this paper, we revisit three elliptic-curve based untraceable RFID authentication protocols recently published and show they are not secure against active attacks and do not support the untraceability for tags. We also provide a new construction to solve such problems using the elliptic-curved based Schnorr signature technique. Our construction satisfies all requirements for RFID security and privacy including replay protection, impersonation resistance, untraceability, and forward privacy. It requires only two point scalar multiplications and two hash operations with two messages exchanges. Compared to previous works, our construction has better security and efficiency.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "不可追踪的RFID认证方案允许合法的读取器认证标签，同时它确保标签的私密性，防止未授权的追踪。在本文中，我们回顾了最近发表的三个基于椭圆曲线的不可追踪RFID认证协议，并指出它们不能抵抗主动攻击，并且不支持标签的不可追踪性。我们还提供了一种新的构造来解决这类问题，使用基于椭圆曲线的Schnorr签名技术。我们的结构满足RFID安全和隐私的所有要求，包括重放保护、防假冒、不可追踪性和前向隐私。它只需要两次点标量乘法和两次散列运算以及两次消息交换。与以前的工程相比，我们的施工具有更好的安全性和效率。",
                    "title_zh": "基于椭圆曲线的不可追踪RFID认证协议研究"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756617",
                    "title": "A Negative Number Vulnerability for Histogram-based Face Recognition Systems",
                    "authors": "Alireza Farrokh Baroughi, Scott Craver, Mohammed Faizan Mohsin",
                    "abstract": "A popular method of face identification is the use of local binary pattern (LBP) histograms. In this method, a face image is partitioned into regions, and a histogram of features is produced for each region; faces are compared by measuring the similarity of their histograms through statistics such as chi-square score or K-L divergence. Comparison of histograms, however, is particularly prone to exploitation via a negative-number bug if coded naively. This allows a surprisingly precise and powerful attack: if an adversary can alter a histogram to change a single zero to a negative number of appropriate magnitude, the change will induce a negligible difference in matching under ordinary use, but match an attacker to an intended victim if the attacker briefly displays a printed striped pattern to a camera. This tampering is minor and can be inflicted long before the attack, allowing the insertion of a back door in a face recognition system that will behave normally until the moment of exploitation. We exhibit an example of this bug in the wild, in the OpenCV computer vision library, and illustrate the effectiveness of this attack in impersonating multiple victims.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "一种流行的人脸识别方法是使用局部二进制模式(LBP)直方图。在该方法中，人脸图像被分割成多个区域，并且为每个区域产生特征直方图；通过诸如卡方得分或K-L散度之类的统计测量直方图的相似性来比较面部。但是，直方图的比较特别容易被利用，如果编码简单的话，会出现负数错误。这允许进行惊人精确和强大的攻击:如果对手可以改变直方图，将单个零改变为适当大小的负数，这种改变在正常使用下将导致匹配中的可忽略差异，但是如果攻击者简单地向摄像机显示打印的条纹图案，则将攻击者与预期的受害者匹配。这种篡改是轻微的，可以在攻击之前很久就造成，允许在人脸识别系统中插入后门，该系统将正常运行，直到被利用。我们在OpenCV计算机视觉库中展示了这种缺陷的一个例子，并说明了这种攻击在模拟多个受害者方面的有效性。",
                    "title_zh": "基于直方图的人脸识别系统的负数漏洞"
                },
                {
                    "url": "https://doi.org/10.1145/2756601.2756619",
                    "title": "Automated Firearm Identification: On using a novel Multiple-Slice-Shape (MSS) Approach for Comparison and Matching of Firing Pin Impression Topography",
                    "authors": "Robert Fischer, Claus Vielhauer",
                    "abstract": "The examination of firearm related toolmarks impressed to cartridges and bullets is a well known forensic discipline. The application of three dimensional imaging systems and pattern recognition techniques for automatic comparison and matching of topographic data is a central field of research in the domain of digital crime scene analysis. In this work, we introduce and evaluate a novel Multiple-Slice-Shape (MSS) approach with the objective to closer link the preprocessing and feature extraction stages and improve the automated examinations of firearm toolmark surface data. We employ two existing features which are applied to the topography of firing pin impressions and aim at an automatic matching of the shapes based on multiple line-profile measurement. We suggest several modifications of the original Multiple-Angle-Path (MAP) and Multiple-Circle-Path (MCP) features to achieve an optimal integration into the proposed processing pipeline. Our evaluation approach is three-fold. First, we aim at the determination of an initial parameterization for MSS processing and feature extraction. Second, we evaluate the accuracy of discrimination for two firearms of the same mark and model. Third, we evaluate the accuracy using six different weapons. The test set contains 72 cartridge samples including six guns and three ammunition manufactures. Regarding the first evaluation, the results indicate an improvement of the accuracy for both features. Regarding the second evaluation, the achieved accuracy ranges between 67% and 100% for the MAP feature, and between 92% and 100% for the MCP feature. With respect to the third evaluation, the best result is achieved for MAP32 with 73% and for MCP15 with 92% compared to 56% and 82% correct classification rate regarding the original versions. It is supposed that various 3D spatial features can be combined and maybe improved by using the proposed MSS approach. We motivate the evaluation of this question for future work.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "对印在弹药筒和子弹上的与火器相关的工具痕迹的检验是一项众所周知的法医学科。将三维成像系统和模式识别技术应用于地形数据的自动比较和匹配是数字犯罪现场分析领域的中心研究领域。在这项工作中，我们介绍并评估了一种新的多切片形状(MSS)方法，其目标是将预处理和特征提取阶段更紧密地联系起来，并改进火器工具痕迹表面数据的自动化检查。我们采用两个现有的特征，这两个特征被应用于撞针印痕的形貌，并且旨在基于多线轮廓测量的形状的自动匹配。我们建议对原始的多角度路径(MAP)和多圆形路径(MCP)特征进行一些修改，以实现与所提出的处理流水线的最佳集成。我们的评估方法有三个方面。首先，我们旨在为MSS处理和特征提取确定初始参数化。第二，我们评估相同标记和型号的两种火器的识别准确度。第三，我们使用六种不同的武器评估精度。该测试集包含72个弹壳样本，包括6支枪和3个弹药制造商。关于第一次评估，结果表明两个特征的准确度都有所提高。关于第二次评估，地图功能实现的精度范围在67%和100%之间，MCP功能实现的精度范围在92%和100%之间。关于第三次评估，与原始版本的56%和82%的正确分类率相比，MAP32和MCP15获得了73%和92%的最佳结果。假设可以通过使用所提出的MSS方法来组合并可能改进各种3D空间特征。我们激励对这个问题的评价，为今后的工作服务。",
                    "title_zh": "自动火器识别:用一种新的多切片形状(MSS)方法比较和匹配撞针印痕形貌"
                }
            ]
        }
    ],
    "2016": [
        {
            "dblp_url": "https://dblp.uni-trier.de/db/conf/ih/ihmmsec2016.html",
            "conf_title": "4. IH&MMSec 2016: Vigo, Galicia, Spain",
            "conf_url": "https://doi.org/10.1145/2909827",
            "papers": [
                {
                    "url": "https://doi.org/10.1145/2909827.2930784",
                    "title": "Machine Learning under Attack: Vulnerability Exploitation and Security Measures",
                    "authors": "Battista Biggio",
                    "abstract": "Learning to discriminate between secure and hostile patterns is a crucial problem for species to survive in nature. Mimetism and camouflage are well-known examples of evolving weapons and defenses in the arms race between predators and preys. It is thus clear that all of the information acquired by our senses should not be considered necessarily secure or reliable. In machine learning and pattern recognition systems, however, we have started investigating these issues only recently. This phenomenon has been especially observed in the context of adversarial settings like malware detection and spam filtering, in which data can be purposely manipulated by humans to undermine the outcome of an automatic analysis. As current pattern recognition methods are not natively designed to deal with the intrinsic, adversarial nature of these problems, they exhibit specific vulnerabilities that an attacker may exploit either to mislead learning or to evade detection. Identifying these vulnerabilities and analyzing the impact of the corresponding attacks on learning algorithms has thus been one of the main open issues in the novel research field of adversarial machine learning, along with the design of more secure learning algorithms. In the first part of this talk, I introduce a general framework that encompasses and unifies previous work in the field, allowing one to systematically evaluate classifier security against different, potential attacks. As an example of application of this framework, in the second part of the talk, I discuss evasion attacks, where malicious samples are manipulated at test time to evade detection. I then show how carefully-designed poisoning attacks can mislead some learning algorithms by manipulating only a small fraction of their training data. In addition, I discuss some defense mechanisms against both attacks in the context of real-world applications, including biometric identity recognition and computer security. Finally, I briefly discuss our ongoing work on attacks against clustering algorithms, and sketch some promising future research directions.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "学会区分安全模式和敌对模式是物种在自然界生存的关键问题。模仿和伪装是捕食者和猎物之间军备竞赛中进化武器和防御的著名例子。因此，很明显，我们感官获得的所有信息都不应该被认为是必然安全或可靠的。然而，在机器学习和模式识别系统中，我们只是最近才开始研究这些问题。这种现象在恶意软件检测和垃圾邮件过滤等敌对环境中尤为明显，在这些环境中，数据可能会被人故意操纵，以破坏自动分析的结果。由于当前的模式识别方法不是为处理这些问题的内在的、敌对的本质而设计的，所以它们表现出特定的弱点，攻击者可以利用这些弱点来误导学习或逃避检测。因此，识别这些漏洞并分析相应攻击对学习算法的影响，以及设计更安全的学习算法，已经成为对抗性机器学习的新研究领域中的主要公开问题之一。在这个演讲的第一部分，我介绍了一个通用框架，它包含并统一了该领域以前的工作，允许人们系统地评估分类器对不同的潜在攻击的安全性。作为这个框架应用的一个例子，在演讲的第二部分，我将讨论规避攻击，恶意样本在测试时被操纵以规避检测。然后，我展示了精心设计的中毒攻击如何通过只操纵一小部分训练数据来误导一些学习算法。此外，我还讨论了在实际应用环境中针对这两种攻击的一些防御机制，包括生物特征身份识别和计算机安全。最后，我简要讨论了我们正在进行的对聚类算法的攻击的工作，并概述了一些有前途的未来研究方向。",
                    "title_zh": "攻击下的机器学习:漏洞利用与安全措施"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930785",
                    "title": "Bringing Multimedia Security from the Research Lab to the Forensic Lab",
                    "authors": "Martino Jerian",
                    "abstract": "The work done by researchers in the field, on multimedia security, is laying the foundations for increasing the capabilities of the community of forensic practitioners. However, very often there is a big gap between what is studied at the research level and what is available to the law enforcement labs in their everyday work. Often researchers achieve a very high level of expertise in their specific fields, but miss out on the context in which the technologies must be used, while the end users are often not aware of such technologies or don't have the skills or the time to work with them. Having worked on both sides of the fence, Martino Jerian, CEO and founder of Amped Software, will tell the story of how a project initially started as a master thesis at university, became one of the standard tools for image and video forensics, used by law enforcement and intelligence agencies in more than 50 countries worldwide. Martino Jerian will show how some technological partnerships with universities managed to yield very good practical results both on the business and research level. It will show how expert researchers are actually amazed and excited when they are directed and pushed to face real problems out of the lab and see the result of their research in a finished product which helps to solve cases and, sometimes, saves lives. Martino Jerian will then showcase the software Amped Authenticate, which is the world leading tool for image forgery detection, as an example of a success story of building a bridge between the researchers and the forensic community.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "研究人员在多媒体安全领域所做的工作为提高法医从业者的能力奠定了基础。然而，在研究水平上所研究的东西和执法实验室日常工作中所能获得的东西之间往往有很大的差距。通常，研究人员在其特定领域获得了非常高水平的专业知识，但却忽略了必须使用这些技术的背景，而最终用户通常不了解这些技术，或者没有技能或时间来使用它们。Amped Software的首席执行官兼创始人Martino Jerian在双方都工作过，他将讲述一个项目如何从大学的硕士论文开始，成为图像和视频取证的标准工具之一，被全球50多个国家的执法机构和情报机构使用。Martino Jerian将展示一些与大学的技术合作如何在商业和研究层面产生非常好的实际结果。它将展示当专家研究人员被指导和推动去面对实验室之外的真实问题，并看到他们的研究成果最终有助于解决案件，有时还能拯救生命时，他们是如何感到惊讶和兴奋的。Martino Jerian将展示Amped Authenticate软件，这是世界领先的图像伪造检测工具，作为在研究人员和法医界之间建立桥梁的成功故事的一个例子。",
                    "title_zh": "将多媒体安全从研究实验室带到法医实验室"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930786",
                    "title": "A Deep Learning Approach to Universal Image Manipulation Detection Using a New Convolutional Layer",
                    "authors": "Belhassen Bayar, Matthew C. Stamm",
                    "abstract": "When creating a forgery, a forger can modify an image using many different image editing operations. Since a forensic examiner must test for each of these, significant interest has arisen in the development of universal forensic algorithms capable of detecting many different image editing operations and manipulations. In this paper, we propose a universal forensic approach to performing manipulation detection using deep learning. Specifically, we propose a new convolutional network architecture capable of automatically learning manipulation detection features directly from training data. In their current form, convolutional neural networks will learn features that capture an image's content as opposed to manipulation detection features. To overcome this issue, we develop a new form of convolutional layer that is specifically designed to suppress an image's content and adaptively learn manipulation detection features. Through a series of experiments, we demonstrate that our proposed approach can automatically learn how to detect multiple image manipulations without relying on pre-selected features or any preprocessing. The results of these experiments show that our proposed approach can automatically detect several different manipulations with an average accuracy of 99.10%.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在制作赝品时，伪造者可以使用许多不同的图像编辑操作来修改图像。由于法医检查者必须对这些中的每一个进行测试，所以对能够检测许多不同的图像编辑操作和操纵的通用法医算法的开发产生了极大的兴趣。在本文中，我们提出了一种使用深度学习执行操纵检测的通用取证方法。具体来说，我们提出了一种新的卷积网络架构，能够直接从训练数据中自动学习操纵检测特征。在目前的形式下，卷积神经网络将学习捕捉图像内容的功能，而不是操纵检测功能。为了克服这个问题，我们开发了一种新形式的卷积层，它专门用于抑制图像的内容，并自适应地学习操纵检测功能。通过一系列实验，我们证明了我们提出的方法可以自动学习如何检测多个图像操作，而不依赖于预先选择的特征或任何预处理。实验结果表明，我们提出的方法可以自动检测几种不同的操作，平均准确率为99.10%。",
                    "title_zh": "使用新卷积层的通用图像操作检测的深度学习方法"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930787",
                    "title": "Forensics of High Quality and Nearly Identical JPEG Image Recompression",
                    "authors": "Cecilia Pasquini, Pascal Schöttle, Rainer Böhme, Giulia Boato, Fernando Pérez-González",
                    "abstract": "We address the known problem of detecting a previous compression in JPEG images, focusing on the challenging case of high and very high quality factors (>= 90) as well as repeated compression with identical or nearly identical quality factors. We first revisit the approaches based on Benford--Fourier analysis in the DCT domain and block convergence analysis in the spatial domain. Both were originally conceived for specific scenarios. Leveraging decision tree theory, we design a combined approach complementing the discriminatory capabilities. We obtain a set of novel detectors targeted to high quality grayscale JPEG images.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "我们解决了检测JPEG图像中先前压缩的已知问题，重点关注高质量因子和非常高质量因子(> = 90)的挑战性情况，以及具有相同或几乎相同质量因子的重复压缩。我们首先回顾基于DCT域中的Benford - Fourier分析和空间域中的块收敛分析的方法。两者最初都是为特定场景而设计的。利用决策树理论，我们设计了一个组合方法来补充区分能力。我们获得了一组新的检测器，目标是高质量的灰度JPEG图像。",
                    "title_zh": "高质量和几乎相同的JPEG图像再压缩的取证"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930788",
                    "title": "A Higher Order Analysis of the Joint Capacity of Digital Fingerprinting Codes against the Interleaving Attack",
                    "authors": "Hiroki Koga, Kaoru Itabashi",
                    "abstract": "Digital fingerprinting codes are embedded to licenced digital contents for preventing illegal distribution by colluders. Digital fingerprinting codes are usually required to have the ability to specify a part (or all) of the colluders with high probability who generate a pirated copy. In this paper we evaluate the joint capacity C(PINT) of the digital fingerprinting code against the interleaving attack by c colluders. Quite recently, Laarhoven shows that C(PINT) = 1.1604/(2c2 ln 2) for sufficiently large c. However, this formula includes numerical optimization and is not a good approximation of the capacity C(PINT) for small values of c. In this paper we obtain two series that yield various upper and lower bounds of C(PINT), respectively, which give fairly well approximation of C(PINT) for a wide range of c. The upper and the lower bounds are obtained by using the Taylor expansion indicated by Furon and Pérez-Freire [8] together with the analysis of the l-th moments of a binomial distribution around its mean for l≥2. In particular, the lower bounds give approximation of c2C(PINT) for various ranges of c.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "数字指纹代码被嵌入到许可的数字内容中，用于防止共谋的非法分发。通常要求数字指纹代码具有指定一部分(或全部)产生盗版拷贝的高概率共谋的能力。本文评估了数字指纹码抵抗C共谋交错攻击的联合容量C(PINT)。最近，Laarhoven证明，对于足够大的C，C(PINT) = 1.1604/(2c2 ln 2)。然而，该公式包括数值优化，并且对于小的C值，不是容量C(PINT)的良好近似。在本文中，我们获得了两个系列，分别产生C(PINT)的各种上界和下界。 其给出了C(PINT)在较宽范围内的相当好的近似值。通过使用Furon和Pérez-Freire [8]指出的Taylor展开式以及对l≥2的二项式分布在其平均值周围的l阶矩的分析，获得了上界和下界。 特别是，下限给出了不同c范围的c2C(品脱)近似值。",
                    "title_zh": "数字指纹码联合抗交织攻击能力的高阶分析"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930789",
                    "title": "Efficient HS based Reversible Data Hiding Using Multi-feature Complexity Measure and Optimized Histogram",
                    "authors": "Junxiang Wang, Jiangqun Ni, Xing Zhang",
                    "abstract": "Histogram-shifting (HS) embedding as a most successful reversible data hiding (RDH) scheme is widely investigated. How to take advantage of covers' redundancyis one of the key issues for performance improvement of RDH. Among them, sorting technique isof practically importance, which uses in prioritysmooth areas with high correlation to hide message. For conventional schemes, usually a single empirical feature in the context of local cover object, such as local variance, is utilized as complexity measure for sorting, which may not effectively exploit the covers' correlations and thus lead to limited performance improvement. In this paper, a general framework for optimal construction of complexity measure with multi-feature is developed, which includes optimal feature selection and weight parameters determination based on an optimization model. In addition, the optimal truncation point for the top of the sorted cover is determined based on the payload constraint to construct the optimal histogram for further performance improvement of HS embedding. Compared with previous approaches, experimental results demonstrate the superiority of the proposed scheme.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "直方图平移(HS)嵌入作为一种最成功的可逆数据隐藏(RDH)方案被广泛研究。如何利用覆盖冗余是提高RDH性能的关键问题之一。其中，排序技术具有重要的实用价值，它主要用于高度相关的优先平滑区域来隐藏信息。对于传统方案，通常使用局部覆盖对象上下文中的单个经验特征(如局部方差)作为排序的复杂度度量，这可能不能有效地利用覆盖的相关性，从而导致有限的性能改善。本文提出了一个多特征复杂性测度优化构造的一般框架，包括基于优化模型的最优特征选择和权重参数确定。此外，基于有效载荷约束来确定排序覆盖顶部的最佳截断点，以构建最佳直方图来进一步提高HS嵌入的性能。与以前的方法相比，实验结果证明了该方法的优越性。",
                    "title_zh": "基于多特征复杂度和优化直方图的高效可逆数据隐藏"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930790",
                    "title": "A Novel CDMA Based High Performance Reversible Data Hiding Scheme",
                    "authors": "Bin Ma, Jian Xu, Yun Q. Shi",
                    "abstract": "In this paper, based on the principle of Code Division Multiple Access (CDMA), a novel reversible data hiding scheme is presented. The to-be-embedded data are represented by different orthogonal spreading sequences and embedded into a cover image while degrading the image quality slightly. According to the feature of orthogonality, different spreading sequences are repeatedly embedded into the image without disturbing each other, and most elements of different spreading sequences are mutually cancelled in the process of multilevel data embedding. Thus, it keeps the distortion of the embedded image at a relatively low level even with a high embedding capacity. Moreover, the location-map of the proposed scheme can be highly compressed and thus the size is quite small; it further helps to obtain high net embedding capacity. Experimental results have demonstrated that the CDMA based reversible data hiding scheme can achieve higher image quality at the moderate-to-high embedding capacity than other state-of-the-art reversible data hiding works.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "基于码分多址(CDMA)原理，提出了一种新的可逆数据隐藏方案。要嵌入的数据由不同的正交扩频序列表示，并嵌入到载体图像中，同时略微降低图像质量。根据正交性的特点，将不同的扩频序列重复嵌入到图像中，互不干扰，并且在多级数据嵌入过程中，不同扩频序列的大部分元素相互抵消。因此，即使具有高嵌入容量，它也将嵌入图像的失真保持在相对较低的水平。此外，所提出的方案的位置图可以被高度压缩，因此大小相当小；这进一步有助于获得高的净嵌入容量。实验结果表明，基于CDMA的可逆数据隐藏方案可以在中高嵌入容量下获得比其他现有可逆数据隐藏方案更高的图像质量。",
                    "title_zh": "一种新的基于CDMA的高性能可逆数据隐藏方案"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930791",
                    "title": "Dynamic Privacy-Preserving Genomic Susceptibility Testing",
                    "authors": "Mina Namazi, Juan Ramón Troncoso-Pastoriza, Fernando Pérez-González",
                    "abstract": "The field of genomic research has considerably grown in the recent years due to the unprecedented advances brought about by Next Generation Sequencing (NGS) and the need and increasing widespread use of outsourced processing. But this rapid increase also poses severe privacy risks due to the inherently sensitive nature of genomic information. In this work, we address privacy-preserving genetic susceptibility tests outsourced to an untrustworthy party, enhancing previous approaches in terms of computation and communication efficiency by leveraging the use of somewhat homomorphic lattice encryption and relinearization operations to achieve more efficient constructions. Additionally, we also propose a more general construction which deals with several different medical units (such as pharmaceutical companies or hospitals), managing patients' consent to the disclosure of test results for each of these units, which may dynamically join the system. Our scheme features an attribute-based homomorphic cryptosystem which enables enforcing the patient's access policy referred to the different medical units.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "由于下一代测序(NGS)带来的前所未有的进步以及对外包处理的需求和日益广泛的使用，基因组研究领域近年来有了相当大的发展。但是，由于基因组信息固有的敏感性，这种快速增长也带来了严重的隐私风险。在这项工作中，我们解决了外包给不可信方的隐私保护遗传易感性测试，通过利用某种同态格加密和重新线性化操作来实现更有效的构建，在计算和通信效率方面增强了以前的方法。此外，我们还提出了一个更一般的结构，它处理几个不同的医疗单位(如制药公司或医院)，管理患者对这些单位中每一个的测试结果公开的同意，这些单位可以动态地加入系统。我们的方案以基于属性的同态密码系统为特征，该系统使得能够对不同的医疗单位实施患者的访问策略。",
                    "title_zh": "动态隐私保护基因组敏感性测试"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930792",
                    "title": "Group Testing for Identification with Privacy",
                    "authors": "Ahmet Iscen, Teddy Furon",
                    "abstract": "This paper describes an approach where group testing helps in enforcing security and privacy in identification. We detail a particular scheme based on embedding and group testing. We add a second layer of defense, group vectors, where each group vector represents a set of dataset vectors. Whereas the selected embedding poorly protects the data when used alone, the group testing approach makes it much harder to reconstruct the data when combined with the embedding. Even when curious server and user collude to disclose the secret parameters, they cannot accurately recover the data. Another byproduct of our approach is that it reduces the complexity of the search and the required storage space. We show the interest of our work in a benchmark biometrics dataset, where we verify our theoretical analysis with real data.",
                    "files": {
                        "openAccessPdf": "https://hal.inria.fr/hal-01309032/file/ihm05s-iscenA.pdf"
                    },
                    "abstract_zh": "本文描述了一种方法，其中组测试有助于加强身份验证的安全性和隐私性。我们详细介绍了一个基于嵌入和分组测试的特殊方案。我们添加了第二层防御，组向量，其中每个组向量代表一组数据集向量。当单独使用时，所选择的嵌入对数据的保护很差，而当与嵌入结合使用时，组测试方法使得重构数据更加困难。即使当好奇服务器和用户合谋泄露秘密参数时，也无法准确恢复数据。我们方法的另一个副产品是它减少了搜索的复杂性和所需的存储空间。我们在一个基准生物统计学数据集上展示了我们工作的兴趣，在那里我们用真实数据验证了我们的理论分析。",
                    "title_zh": "群体隐私识别测试"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930793",
                    "title": "Differentially Private Matrix Factorization using Sketching Techniques",
                    "authors": "Raghavendran Balu, Teddy Furon",
                    "abstract": "Collaborative filtering is a popular technique for recommendation system due to its domain independence and reliance on user behavior data alone. But the possibility of identification of users based on these personal data raise privacy concerns. Differential privacy aims to minimize these identification risks by adding controlled noise with known characteristics. The addition of noise impacts the utility of the system and does not add any other value to the system other than enhanced privacy. We propose using sketching techniques to implicitly provide the differential privacy guarantees by taking advantage of the inherent randomness of the data structure. In particular, we use count sketch as a storage model for matrix factorization, one of the successful collaborative filtering techniques. Our model is also compact and scales well with data, making it well suitable for large scale applications.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-01317596/file/dp-countsketch.pdf"
                    },
                    "abstract_zh": "协同过滤因其领域独立性和对用户行为数据的依赖性而成为推荐系统的常用技术。但是基于这些个人数据识别用户身份的可能性引起了隐私问题。差分隐私旨在通过添加具有已知特征的受控噪声来最小化这些识别风险。噪声的增加影响了系统的效用，并且除了增强隐私之外，没有给系统增加任何其他价值。我们建议使用草图技术，通过利用数据结构固有的随机性来隐式地提供差分隐私保证。特别是，我们使用count sketch作为矩阵分解的存储模型，这是一种成功的协同过滤技术。我们的模型也很紧凑，可以很好地根据数据进行缩放，非常适合大规模应用。",
                    "title_zh": "使用草图技术的差分私有矩阵分解"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930794",
                    "title": "Secure and Verifiable Outsourcing of Nonnegative Matrix Factorization (NMF)",
                    "authors": "Jia Duan, Jiantao Zhou, Yuanman Li",
                    "abstract": "Cloud computing platforms are becoming increasingly prevalent and readily available nowadays, providing us alternative and economic services for resource-constrained clients to perform large-scale computation. In this work, we address the problem of secure outsourcing of large-scale nonnegative matrix factorization (NMF) to a cloud in a way that the client can verify the correctness of results with small overhead. The input matrix protection is achieved by a lightweight, permutation-based encryption mechanism. By exploiting the iterative nature of NMF computation, we propose a single-round verification strategy, which can be proved to be effective. Both theoretical and experimental results are given to demonstrate the superior performance of our scheme.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "如今，云计算平台正变得越来越流行和可用，为资源受限的客户执行大规模计算提供了替代和经济的服务。在这项工作中，我们解决了将大规模非负矩阵分解(NMF)安全外包到云的问题，客户端可以以较小的开销验证结果的正确性。输入矩阵保护通过轻量级的基于置换的加密机制来实现。利用NMF计算的迭代性质，我们提出了一种单轮验证策略，该策略可以被证明是有效的。理论和实验结果都表明了该方案的优越性能。",
                    "title_zh": "非负矩阵分解的安全可验证外包(NMF)"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930795",
                    "title": "Practical and Scalable Sharing of Encrypted Data in Cloud Storage with Key Aggregation",
                    "authors": "Hung Dang, Yun Long Chong, Francois Brun, Ee-Chien Chang",
                    "abstract": "We study a sensor network setting in which samples are encrypted individually using different keys and maintained on a cloud storage. For large systems, e.g. those that generate several millions of samples per day, fine-grained sharing of encrypted samples is challenging. Existing solutions, such as Attribute-Based Encryption (ABE) and Key Aggregation Cryptosystem (KAC), can be utilized to address the challenge, but only to a certain extent. They are often computationally expensive and thus unlikely to operate at scale. We propose an algorithmic enhancement and two heuristics to improve KAC's key reconstruction cost, while preserving its provable security. The improvement is particularly significant for range and down-sampling queries -- accelerating the reconstruction cost from quadratic to linear running time. Experimental study shows that for queries of size 32k samples, the proposed fast reconstruction techniques speed-up the original KAC by at least 90 times on range and down-sampling queries, and by eight times on general (arbitrary) queries. It also shows that at the expense of splitting the query into 16 sub-queries and correspondingly issuing that number of different aggregated keys, reconstruction time can be reduced by 19 times. As such, the proposed techniques make KAC more applicable in practical scenarios such as sensor networks or the Internet of Things.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "我们研究了一个传感器网络设置，其中样本分别使用不同的密钥加密，并保存在云存储中。对于大型系统，例如那些每天生成几百万个样本的系统，加密样本的细粒度共享是具有挑战性的。现有的解决方案，如基于属性的加密(ABE)和密钥聚合密码系统(KAC)，可以用来解决这一挑战，但只能在一定程度上解决。它们通常计算量很大，因此不太可能大规模运行。我们提出了一种算法改进和两种启发式方法来提高KAC的密钥重建成本，同时保持其可证明的安全性。这种改进对于范围和下采样查询来说尤其显著——将重建成本从二次加速到线性运行时间。实验研究表明，对于大小为32k样本的查询，所提出的快速重建技术在范围和下采样查询上使原始KAC加速至少90倍，在一般(任意)查询上加速8倍。它还显示，以将查询拆分为16个子查询并相应地发出该数量的不同聚合键为代价，重构时间可以减少19倍。因此，所提出的技术使KAC更适用于传感器网络或物联网等实际场景。",
                    "title_zh": "具有密钥聚合的云存储中加密数据的实用和可扩展共享"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930796",
                    "title": "Provable Secure Universal Steganography of Optimal Rate: Provably Secure Steganography does not Necessarily Imply One-Way Functions",
                    "authors": "Sebastian Berndt, Maciej Liskiewicz",
                    "abstract": "We present the first complexity-theoretic secure steganographic protocol which, for any communication channel, is provably secure, reliable, and has nearly optimal bandwidth. Our system is unconditionally secure, i.e. our proof does not rely on any unproven complexity-theoretic assumption, like e.g. the existence of one-way functions. This disproves the claim that the existence of one-way functions and access to a communication channel oracle are both necessary and sufficient conditions for the existence of secure steganography, in the sense that secure and reliable steganography exists independently of the existence of one-way functions.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "我们提出了第一个复杂性理论安全隐写协议，它对于任何通信信道都是可证明安全、可靠的，并且具有接近最优的带宽。我们的系统是无条件安全的，即我们的证明不依赖于任何未经证明的复杂性理论假设，例如单向函数的存在。这否定了单向函数的存在和对通信信道oracle的访问是安全隐写术存在的必要和充分条件的主张，在这个意义上，安全和可靠的隐写术独立于单向函数的存在而存在。",
                    "title_zh": "最优速率的可证明安全的通用隐写术:可证明安全的隐写术不一定意味着单向函数"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930797",
                    "title": "Rethinking Optimal Embedding",
                    "authors": "Andrew D. Ker, Tomás Pevný, Patrick Bas",
                    "abstract": "At present, almost all leading steganographic techniques for still images use a distortion minimization paradigm, where each potential change is assigned a cost ci and the change probabilities πi chosen to minimize the average total cost ∑iπici. However, some detectors have exploited knowledge of this adaptivity and the embedding cannot be considered optimal. In this work we prove a theoretical result suggesting that, against a knowing attacker, the embedder should simply minimize ∑iπ2ici instead, for the same costs ci, which is the minimax and equilibrium strategy. This aligns with some special case results that have appeared in recent literature. We then test some simple steganographic methods in theoretical and real settings, showing that naive (average cost) adaptivity is exploitable, but the equilibrium probabilities cannot be exploited. However, it is essential to determine statistically well-founded costs ci.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-01360016/file/camera-ready%20%281%29.pdf"
                    },
                    "abstract_zh": "目前，几乎所有领先的静态图像隐写技术都使用失真最小化范式，其中每个潜在的变化都被分配一个成本ci，并且选择变化概率πi以最小化平均总成本∑iπici。然而，一些检测器已经利用了这种适应性的知识，并且嵌入不能被认为是最佳的。在本文中，我们证明了一个理论结果，即对于已知的攻击者，嵌入者应该简单地最小化∑iπ2ici，对于相同的成本ci，这是最小最大均衡策略。这与最近文献中出现的一些特例结果一致。然后，我们在理论和现实环境中测试了一些简单的隐写方法，表明天真(平均成本)自适应性是可利用的，但均衡概率不能被利用。然而，确定有统计依据的成本ci是至关重要的。",
                    "title_zh": "重新思考最优嵌入"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930798",
                    "title": "Ensemble of CNNs for Steganalysis: An Empirical Study",
                    "authors": "Guanshuo Xu, Han-Zhou Wu, Yun Q. Shi",
                    "abstract": "There has been growing interest in using convolutional neural networks (CNNs) in the fields of image forensics and steganalysis, and some promising results have been reported recently. These works mainly focus on the architectural design of CNNs, usually, a single CNN model is trained and then tested in experiments. It is known that, neural networks, including CNNs, are suitable to form ensembles. From this perspective, in this paper, we employ CNNs as base learners and test several different ensemble strategies. In our study, at first, a recently proposed CNN architecture is adopted to build a group of CNNs, each of them is trained on a random subsample of the training dataset. The output probabilities, or some intermediate feature representations, of each CNN, are then extracted from the original data and pooled together to form new features ready for the second level of classification. To make best use of the trained CNN models, we manage to partially recover the lost information due to spatial subsampling in the pooling layers when forming feature vectors. Performance of the ensemble methods are evaluated on BOSSbase by detecting S-UNIWARD at 0.4 bpp embedding rate. Results have indicated that both the recovery of the lost information, and learning from intermediate representation in CNNs instead of output probabilities, have led to performance improvement.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "卷积神经网络(CNN)在图像取证和隐写分析领域的应用越来越受到关注，最近有一些有前景的结果被报道。这些工作主要集中在细胞神经网络的结构设计上，通常是训练单个细胞神经网络模型，然后在实验中进行测试。众所周知，包括CNN在内的神经网络适合于形成集合。从这个角度来看，在本文中，我们采用CNN作为基础学习器，并测试几种不同的集成策略。在我们的研究中，首先，采用最近提出的CNN架构来建立一组CNN，每个CNN在训练数据集的随机子样本上被训练。然后，从原始数据中提取每个CNN的输出概率或一些中间特征表示，并汇集在一起以形成为第二级分类准备的新特征。为了最大限度地利用训练好的CNN模型，我们设法在形成特征向量时部分恢复由于池层中的空间子采样而丢失的信息。通过以0.4 bpp嵌入率检测S-UNIWARD，在BOSSbase上评估集成方法的性能。结果表明，丢失信息的恢复，以及从CNN的中间表示而不是输出概率中学习，都导致了性能的提高。",
                    "title_zh": "用于隐写分析的细胞神经网络集成:一项实证研究"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930799",
                    "title": "Color Image Steganalysis Based On Steerable Gaussian Filters Bank",
                    "authors": "Hasan Abdulrahman, Marc Chaumont, Philippe Montesinos, Baptiste Magnier",
                    "abstract": "This article deals with color images steganalysis based on machine learning. The proposed approach enriches the features from the Color Rich Model by adding new features obtained by applying steerable Gaussian filters and then computing the co-occurrence of pixel pairs. Adding these new features to those obtained from Color-Rich Models allows us to increase the detectability of hidden messages in color images. The Gaussian filters are angled in different directions to precisely compute the tangent of the gradient vector. Then, the gradient magnitude and the derivative of this tangent direction are estimated. This refined method of estimation enables us to unearth the minor changes that have occurred in the image when a message is embedded. The efficiency of the proposed framework is demonstrated on three stenographic algorithms designed to hide messages in images: S-UNIWARD, WOW, and Synch-HILL. Each algorithm is tested using different payload sizes. The proposed approach is compared to three color image steganalysis methods based on computation features and Ensemble Classifier classification: the Spatial Color Rich Model, the CFA-aware Rich Model and the RGB Geometric Color Rich Model.",
                    "files": {
                        "openAccessPdf": "https://hal.archives-ouvertes.fr/hal-01374101/file/IHMMSec2016_Abadulrahman_Chaumont_Montesinos_Magnier_SteerableGaussianFilters.pdf"
                    },
                    "abstract_zh": "本文研究基于机器学习的彩色图像隐写分析。所提出的方法通过添加通过应用可操纵高斯滤波器获得的新特征，然后计算像素对的共现，来丰富来自色彩丰富模型的特征。将这些新特征添加到从色彩丰富的模型中获得的特征中，使我们能够提高彩色图像中隐藏信息的可检测性。高斯滤波器以不同的方向倾斜，以精确计算梯度向量的正切。然后，估计梯度大小和该切线方向的导数。这种精确的估计方法使我们能够发现嵌入消息时图像中发生的微小变化。所提出的框架的效率在三个用于隐藏图像信息的速记算法上得到验证:S-UNIWARD、WOW和Synch-HILL。每种算法都使用不同的有效载荷大小进行测试。将该方法与三种基于计算特征和集成分类器分类的彩色图像隐写分析方法进行比较:空间颜色丰富模型、CFA感知丰富模型和RGB几何颜色丰富模型。",
                    "title_zh": "基于可控高斯滤波器组的彩色图像隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930800",
                    "title": "A Survey and Taxonomy Aimed at the Detection and Measurement of Covert Channels",
                    "authors": "Brent Carrara, Carlisle Adams",
                    "abstract": "New viewpoints of covert channels are presented in this work. First, the origin of covert channels is traced back to acc ess control and a new class of covert channel, air-gap covert channels, is presented. Second, we study the design of covert channels and provide novel insights that differentiate the research area of undetectable communication from that of covert channels. Third, we argue that secure systems can be characterized as fixed-source systems or continuous-source systems, i.e., systems whose security is compromised if their design allows a covert channel to communicate a small, fixed amount of information or communicate information at a sufficiently high, continuous rate, respectively. Consequently, we challenge the traditional method for measuring covert channels, which is based on Shannon capacity, and propose that a new measure, steganographic capacity, be used to accurately assess the risk posed by covert channels, particularly those affecting fixed-source systems. Additionally, our comprehensive review of covert channels has led us to the conclusion that important properties of covert channels have not been captured in previous taxonomies. We, therefore, present novel extensions to existing taxonomies to more accurately characterize covert channels.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "本文提出了隐蔽通道的新观点。首先，追溯了隐通道的起源，提出了一种新的隐通道&气隙隐通道。其次，我们研究了隐蔽信道的设计，并提供了新的见解，将不可检测通信的研究领域与隐蔽信道区分开来。第三，我们认为安全系统可以被表征为固定源系统或连续源系统，即，如果它们的设计分别允许隐蔽信道传递少量固定量的信息或以足够高的连续速率传递信息，则它们的安全性受到损害的系统。因此，我们对传统的基于香农容量的隐蔽信道度量方法提出了挑战，并提出了一种新的度量方法，隐写容量，用于准确评估隐蔽信道带来的风险，特别是那些影响固定源系统的风险。此外，我们对隐蔽通道的全面审查使我们得出结论，隐蔽通道的重要属性在以前的分类法中没有被捕获。因此，我们提出了对现有分类法的新扩展，以更准确地描述隐蔽通道的特征。",
                    "title_zh": "旨在检测和测量隐蔽通道的调查和分类"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930801",
                    "title": "A Novel Embedding Distortion for Motion Vector-Based Steganography Considering Motion Characteristic, Local Optimality and Statistical Distribution",
                    "authors": "Peipei Wang, Hong Zhang, Yun Cao, Xianfeng Zhao",
                    "abstract": "This paper presents an effective motion vector (MV)-based steganography to cope with different steganalytic models. The main principle is to define a distortion scale expressing the multi-level embedding impact of MV modification. Three factors including motion characteristic of video content, MV's local optimality and statistical distribution are considered in distortion definition. For every embedding location, the contributions of three factors are dynamically adjusted according to MV's property. Based on the defined distortion function, two layered syndrome-trellis codes (STCs) are utilized to minimize the overall embedding impact in practical embedding implementation. Experimental results demonstrate that the proposed method achieves higher level of security compared with other existing MV-based approaches, especially for high quality videos.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "针对不同的隐写分析模型，提出了一种有效的基于运动矢量的隐写术。主要原理是定义一个失真尺度，表示MV修改的多级嵌入影响。失真定义考虑了视频内容的运动特性、MV的局部最优性和统计分布三个因素。对于每个嵌入位置，三个因子的贡献根据MV的特性动态调整。基于所定义的失真函数，两层校正子网格码(STCs)被用于最小化实际嵌入实现中的整体嵌入影响。实验结果表明，与现有的基于MV的方法相比，该方法具有更高的安全性，特别是对于高质量的视频。",
                    "title_zh": "一种新的考虑运动特性、局部最优和统计分布的运动矢量隐写嵌入失真"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930802",
                    "title": "Constructing Near-optimal Double-layered Syndrome-Trellis Codes for Spatial Steganography",
                    "authors": "Zengzhen Zhao, Qingxiao Guan, Xianfeng Zhao",
                    "abstract": "In this paper, we present a new kind of near-optimal double-layered syndrome-trellis codes (STCs) for spatial domain steganography. The STCs can hide longer message or improve the security with the same-length message comparing to the previous double-layered STCs. In our scheme, according to the theoretical deduction we can more precisely divide the secret payload into two parts which will be embedded in the first layer and the second layer of the cover respectively with binary STCs. When embed the message, we encourage to realize the double-layered embedding by ±1 modifications. But in order to further decrease the modifications and improve the time efficient, we allow few pixels to be modified by ±2. Experiment results demonstrate that while applying this double-layered STCs to the adaptive steganographic algorithms, the embedding modifications become more concentrative and the number decreases, consequently the security of steganography is improved.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "本文提出了一种新的用于空域隐写的近似最优的双层伴随式网格码(STCs)。与以前的双层STC相比，STC可以隐藏更长的消息或提高相同长度消息的安全性。在我们的方案中，根据理论推导，我们可以更精确地将秘密有效载荷分成两部分，用二进制STC分别嵌入到载体的第一层和第二层。在嵌入消息时，我们鼓励通过1次修改来实现双层嵌入。但是为了进一步减少修改和提高时间效率，我们允许少量像素被修改2。实验结果表明，将该双层STCs应用于自适应隐写算法时，嵌入修改更加集中，数量减少，从而提高了隐写的安全性。",
                    "title_zh": "构造用于空间隐写术的近似最优双层伴随式格码"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930803",
                    "title": "Boosting Steganalysis with Explicit Feature Maps",
                    "authors": "Mehdi Boroumand, Jessica J. Fridrich",
                    "abstract": "Explicit non-linear transformations of existing steganalysis features are shown to boost their ability to detect steganography in combination with existing simple classifiers, such as the FLD-ensemble. The non-linear transformations are learned from a small number of cover features using Nyström approximation on pilot vectors obtained with kernelized PCA. The best performance is achieved with the exponential form of the Hellinger kernel, which improves the detection accuracy by up to 2-3% for spatial-domain contentadaptive steganography. Since the non-linear map depends only on the cover source and its learning has a low computational complexity, the proposed approach is a practical and low cost method for boosting the accuracy of existing detectors built as binary classifiers. The map can also be used to significantly reduce the feature dimensionality (by up to factor of ten) without performance loss with respect to the non-transformed features.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "现有隐写分析特征的显式非线性变换显示出与现有的简单分类器(例如FLD集成)相结合来提高它们检测隐写术的能力。非线性变换是通过对用核化PCA获得的导频向量使用Nyströ近似从少量的覆盖特征中学习的。最佳性能是通过指数形式的Hellinger核实现的，它将空域内容自适应隐写术的检测精度提高了2-3%。由于非线性映射仅依赖于覆盖源，并且其学习具有低计算复杂度，因此所提出的方法是一种实用且低成本的方法，用于提高作为二元分类器构建的现有检测器的准确性。该图还可用于显著降低特征维数(高达十分之一),而相对于未变换的特征没有性能损失。",
                    "title_zh": "使用显式特征图增强隐写分析"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930804",
                    "title": "Information Hiding in the RSA Modulus",
                    "authors": "Stefan Wüller, Marián Kühnel, Ulrike Meyer",
                    "abstract": "The manufacturer of an asymmetric backdoor for a public key cryptosystem manipulates the key generation process in such a way that he can extract the private key or other secret information from the user's public key by involving his own public/private key pair. All asymmetric backdoors in major public key cryptosystems including RSA differ substantially in their implementation approaches and in their quality in satisfying backdoor related properties like confidentiality and concealment. While some of them meet neither of these two properties very well, others provide a high level of confidentiality but none of them is concealing, which limits their use for covert implementation. In this paper we introduce two novel asymmetric RSA backdoors, both following the approach to embed bits of one of the RSA prime factors in the user's public RSA modulus. While our first backdoor provides confidentiality for a sufficiently large key length, it might be detected under certain circumstances. The second backdoor extends the first one such that it additionally provides concealment and is thus particularly suitable for covert implementation.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "公钥密码系统的非对称后门的制造商以这样的方式操纵密钥生成过程，即他可以通过涉及他自己的公钥/私钥对来从用户的公钥中提取私钥或其他秘密信息。包括RSA在内的主要公钥密码系统中的所有非对称后门在实现方法和满足后门相关属性(如保密性和隐蔽性)的质量方面有很大不同。虽然它们中的一些不能很好地满足这两个属性中的任何一个，但是其他的提供了高水平的保密性，但是它们中没有一个是隐藏的，这限制了它们在秘密实现中的使用。在这篇文章中，我们介绍了两个新的非对称RSA后门，都遵循在用户的公开RSA模数中嵌入RSA素数因子之一的比特的方法。虽然我们的第一个后门为足够长的密钥长度提供了保密性，但在某些情况下它可能会被检测到。第二个后门扩展了第一个后门，因此它还提供了隐蔽性，因此特别适合秘密实施。",
                    "title_zh": "RSA模数中的信息隐藏"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930805",
                    "title": "Secure Image Display through Visual Cryptography: Exploiting Temporal Responsibilities of the Human Eye",
                    "authors": "Jong-Uk Hou, Dongkyu Kim, Hyun-Ji Song, Heung-Kyu Lee",
                    "abstract": "We propose a new protection scheme for displaying a static binary image on a screen. The protection is achieved by a visual cryptography algorithm that divides the target images into several divisions. The visual difference between the text and the background is induced by exploiting the temporal responsibilities of the human eye. With the results of our user study, we demonstrate that encrypted visual information was mentally recovered by the human visual system. Moreover, the images captured from our scheme do not provide any meaningful information to the human eye, so that our method provides a strong security measure against screenshot piracy.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "我们提出了一种新的保护方案，用于在屏幕上显示静态二值图像。这种保护是通过视觉加密算法实现的，该算法将目标图像分成几个部分。文本和背景之间的视觉差异是通过利用人眼的时间责任而引起的。根据我们的用户研究结果，我们证明了人类视觉系统可以在精神上恢复加密的视觉信息。此外，从我们的方案中捕获的图像没有向人眼提供任何有意义的信息，因此我们的方法提供了针对截图盗版的强有力的安全措施。",
                    "title_zh": "通过视觉密码术的安全图像显示:利用人眼的时间责任"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2930806",
                    "title": "Image Segmentation Based Visual Security Evaluation",
                    "authors": "Christof Kauba, Stefan Mayer, Andreas Uhl",
                    "abstract": "In this paper we present a metric for visual security evaluation of encrypted images, also known as visual security metric. Such a metric should be able to assess whether an image encryption method is secure or not. In order to consider intelligibility of objects in encrypted images our metric is based on image segmentation and applying a measure designed to evaluate the segmentation result. The visual security metrics' performance is evaluated using a selective encryption approach and compared to some general image quality metrics like PSNR, metrics suggested for encrypted images like Irregular Deviation and two metrics specifically designed for visual security evaluation. Our visual security metric performs better than all of the other tested metrics on the dataset and encryption algorithm we used during our experiments in terms of different correlation measures.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "本文提出了一种用于加密图像视觉安全性评估的度量，也称为视觉安全性度量。这种度量应该能够评估图像加密方法是否安全。为了考虑加密图像中对象的可理解性，我们的度量基于图像分割，并应用设计用于评估分割结果的测量。视觉安全度量的性能使用选择性加密方法进行评估，并与一些通用图像质量度量进行比较，如PSNR、为加密图像建议的度量(如不规则偏差)以及专门为视觉安全评估设计的两个度量。就不同的相关性度量而言，我们的视觉安全性度量在数据集和加密算法上的表现优于所有其他测试度量。",
                    "title_zh": "基于图像分割的视觉安全性评估"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2931096",
                    "title": "Enhanced Collusion Resistance for Segment-wise Recombined Fingerprinting in P2P Distribution Systems: [Extended Abstract]",
                    "authors": "David Megías, Amna Qureshi",
                    "abstract": "Recombined fingerprinting has been recently proposed as a scalable alternative to the traditional client-server model for anonymous fingerprinting. In recombined fingerprinting, the contents are distributed in P2P fashion from a few seed nodes to the rest of buyers. However, the solution of the previous works requires that a hash of the whole fingerprint is also constructed during distribution in such a way that two different codes must be used (one at segment level and one at hash level). This solution is impractical for two reasons: (1) the fingerprint's length is increased, and (2) the construction of the fingerprints through recombination must be supervised in order to obtain a valid hash-level codeword. This work contributes with a solution to this problem, consisting of embedding collusion-resistant codewords only at segment level and removing the need for hash-level encoding. The proposed solution not only results in shorter fingerprints, but also simplifies the distribution protocol since supervision is no longer required. The resulting system is shown to work for four different state-of-the-art collusion-resistant codes by means of thousands of simulations.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "重组指纹最近被提出作为匿名指纹的传统客户端-服务器模型的可扩展替代方案。在重组指纹中，内容以P2P方式从几个种子节点分发到其余的购买者。然而，先前工作的解决方案需要在分发期间以必须使用两个不同代码(一个在段级，一个在散列级)的方式构建整个指纹的散列。这种解决方案是不切实际的，原因有两个:(1)指纹的长度增加了，以及(2)为了获得有效的散列级码字，必须监督通过重组构建指纹。这项工作有助于解决这个问题，包括嵌入抗共谋码字只在段级和消除哈希级编码的需要。所提出的解决方案不仅导致更短的指纹，还简化了分发协议，因为不再需要监管。通过数千次仿真，结果系统被示出对四种不同的最新抗共谋码起作用。",
                    "title_zh": "P2P分发系统中分段重组指纹增强的抗共谋能力"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2931097",
                    "title": "Study of a Verifiable Biometric Matching",
                    "authors": "Hervé Chabanne, Julien Keuffer, Roch Lescuyer",
                    "abstract": "In this paper, we apply verifiable computing techniques to a biometric matching. The purpose of verifiable computation is to give the result of a computation along with a proof that the calculations were correctly performed. We adapt a protocol called sumcheck protocol and present a system that performs verifiable biometric matching in the case of a fast border control. This is a work in progress and we focus on verifying an inner product. We then give some experimental results of its implementation. Verifiable computation here helps to enforce the authentication phase bringing in the process a proof that the biometric verification has been correctly performed.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在本文中，我们将可验证计算技术应用于生物特征匹配。可验证计算的目的是给出计算的结果以及计算被正确执行的证明。我们采用了一种称为sumcheck协议的协议，并提出了一种在快速边界控制的情况下执行可验证的生物特征匹配的系统。这是一项正在进行的工作，我们专注于验证内部产品。然后我们给出了一些实验结果。这里的可验证计算有助于加强身份验证阶段，在该过程中引入生物特征验证已被正确执行的证据。",
                    "title_zh": "一种可验证的生物特征匹配研究"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2933195",
                    "title": "Privacy Protection for JPEG Content on Image-Sharing Platforms",
                    "authors": "Kun He, Christophe Bidan, Gaëtan Le Guelvouit",
                    "abstract": "In this paper, we show that, using the encryption algorithm of He et al., it is possible to ensure privacy protection for JPEG content on several widely used image-sharing platforms (e.g., Flickr, Pinterest, Google+ and Twitter).",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "abstract_zh": "在本文中，我们表明，使用何等人的加密算法，有可能在几个广泛使用的图像共享平台(如Flickr、Pinterest、Google+和Twitter)上确保JPEG内容的隐私保护。",
                    "title_zh": "图像共享平台上JPEG内容的隐私保护"
                },
                {
                    "url": "https://doi.org/10.1145/2909827.2933196",
                    "title": "PPE-Based Reversible Data Hiding",
                    "authors": "Han-Zhou Wu, Hong-Xia Wang, Yun-Qing Shi",
                    "abstract": "We propose to utilize the prediction-error of prediction error (PPE) of a pixel to reversibly carry the secret data in this letter. In the proposed method, the pixels to be embedded are firstly predicted with their neighboring pixels to obtain the prediction errors (PEs). By exploiting the PEs of the neighboring pixels, the prediction of the PEs of the pixels to be embedded can be then determined. And, a sorting technique based on the local complexity of a pixel is used to collect the PPEs to generate an ordered PPE sequence so that, smaller PPEs will be processed first for data embedding. By reversibly shifting the PPE histogram (PPEH) with optimized parameters, the pixels corresponding to the altered PPEH bins can be finally modified to carry the entire secret data. Experimental results have implied that, the proposed algorithm can benefit from the prediction procedure, sorting technique as well as parameters selection, and therefore outperform some state-of-the-art works in terms of payload-distortion performance.",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/1604.04984"
                    },
                    "abstract_zh": "我们提出利用像素的预测误差来可逆地携带秘密数据。在所提出的方法中，要嵌入的像素首先与其相邻像素进行预测，以获得预测误差。通过利用相邻像素的PEs，然后可以确定要嵌入的像素的PEs的预测。并且，使用基于像素的局部复杂度的排序技术来收集PPE，以生成有序的PPE序列，使得较小的PPE将首先被处理用于数据嵌入。通过用优化的参数可逆地移动PPE直方图(PPEH ),对应于改变的PPEH仓的像素可以最终被修改以携带整个秘密数据。实验结果表明，所提出的算法可以受益于预测过程、排序技术以及参数选择，因此在有效载荷失真性能方面优于一些最新的工作。",
                    "title_zh": "基于PPE的可逆数据隐藏"
                }
            ]
        }
    ],
    "2023": [
        {
            "dblp_url": "https://dblp.uni-trier.de/db/conf/ih/ihmmsec2023.html",
            "conf_title": "11th IH&MMSec 2023:\nChicago, IL, USA",
            "conf_url": "https://doi.org/10.1145/3577163",
            "papers": [
                {
                    "url": "https://doi.org/10.1145/3577163.3595115",
                    "title": "Photoshop Fantasies",
                    "authors": "Walter J. Scheirer",
                    "abstract": "The possibility of an altered photo revising history in a convincing way highlights a salient threat of imaging technology. Afterall, seeing is believing. Or is it? The examples history has preserved make it clear that the observer is more often than not meant to understand that something has changed. Surprisingly, the objectives of photographic manipulation have remained largely the same since the camera first appeared in the 19th century. The old battleworn techniques have simply evolved to keep pace with technological developments. In this talk, we will learn about the history of photographic manipulation, from the invention of the camera to the present day. Importantly, we will consider the reception of photo editing and its relationship to the notion of reality, which is more significant than the technologies themselves. Surprisingly, we will discover that creative mythmaking has found a new medium to embed itself in.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "Photoshop幻想",
                    "abstract_zh": "篡改照片以令人信服的方式修改历史的可能性凸显了成像技术的一个突出威胁。毕竟，眼见为实。或者是？历史保存下来的例子清楚地表明，观察者往往意味着理解某些事情已经改变了。令人惊讶的是，自从19世纪照相机首次出现以来，摄影操作的目标基本上保持不变。古老的战斗技术只是为了跟上技术发展的步伐而进化。在这次演讲中，我们将了解摄影操作的历史，从照相机的发明到今天。重要的是，我们将考虑照片编辑的接受程度及其与现实概念的关系，这比技术本身更重要。令人惊讶的是，我们会发现，创造性的神话制作已经找到了一种新的嵌入媒介。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595114",
                    "title": "Steganography on Mobile Apps",
                    "authors": "Jennifer L. Newman",
                    "abstract": "Steganography is an ancient communication technique that hides a message inside a common object so that the message escapes scrutiny - today, we use digital files like photos and videos to hide and pass messages. Code to execute the message hiding inside digital photos was developed (on computers) in the 1990s. However, code to execute message hiding inside photos on an app for a smartphone was developed only in the last ten years. These mobile stego apps expanded steganography's reach from computer experts to the general public, where few technological skills are necessary. How much of a presence does mobile stego have in our day-to-day lives? Beyond collecting download statistics, it is hard to say, as there are no known software tools capable of reliable detection of mobile stego images. Why not? A simple observation is that there exist no training pairs of cover-stego images to develop software solutions. Of course, the answer is more complex. In response to this lack of data, our research team designed and created a steganography database populated with mobile phone images, including stego images created from mobile phone apps. The story of creating StegoAppDB is remarkably involved and far more complex than simply acquiring smartphones, taking pictures, and creating stego images from apps. In this talk, I present our adventure of creating a steganography app database with a diverse team of people, skills, and ideas. This has led to various discoveries connecting steganography with mobile apps, identifying several interesting and difficult open problems.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "移动应用上的隐写术",
                    "abstract_zh": "隐写术是一种古老的通信技术，它将消息隐藏在一个常见的对象中，以便消息逃脱审查-今天，我们使用照片和视频等数字文件来隐藏和传递消息。执行隐藏在数码照片中的信息的代码是在20世纪90年代(在电脑上)开发出来的。然而，在智能手机应用程序中执行照片内信息隐藏的代码是在最近十年才开发出来的。这些移动隐写应用程序将隐写术的范围从计算机专家扩大到普通大众，在普通大众中几乎不需要技术技能。移动隐写在我们的日常生活中有多少存在？除了收集下载统计数据，这很难说，因为没有已知的软件工具能够可靠地检测移动隐写图像。为什么不呢？一个简单的观察是，不存在用于开发软件解决方案的载体-隐写图像的训练对。当然，答案更复杂。针对这种数据缺乏的情况，我们的研究团队设计并创建了一个包含手机图像的隐写术数据库，其中包括从手机应用程序创建的隐写图像。创建StegoAppDB的故事非常复杂，远比简单地购买智能手机、拍照和从应用程序创建隐写图像复杂得多。在这次演讲中，我将展示我们与一个由不同的人、技能和想法组成的团队一起创建隐写术应用数据库的冒险经历。这导致了各种发现，将隐写术与移动应用程序联系起来，确定了几个有趣而困难的公开问题。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3596214",
                    "title": "On the Detection, Localization, and Reverse Engineering of Diverse Image Manipulations",
                    "authors": "Xiaoming Liu",
                    "abstract": "With the abundance of imagery data captured in our daily life, there is an increasing amount of diverse manipulations being applied to imagery, including generative model based image generation and manipulation, adversarial attacks, classic image editing such as splicing, etc. As the imagery plays important roles in our society, it is necessary to understand whether, where and how a given image is manipulated. From the perspective of a defender, this talk will introduce our recent efforts on detecting these manipulations individually and jointly, as well as reverse engineering the various information regarding the manipulation process. We will also share some of the topics that warrant future research.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "不同图像操作的检测、定位和逆向工程",
                    "abstract_zh": "随着我们日常生活中捕获的图像数据的丰富，越来越多的不同操作被应用于图像，包括基于生成模型的图像生成和操作、对抗性攻击、经典图像编辑(如拼接)等。由于意象在我们的社会中扮演着重要的角色，因此有必要了解一个给定的意象是否、在哪里以及如何被操纵。从一个防御者的角度来看，这个演讲将介绍我们最近在单独和联合检测这些操纵方面的努力，以及对有关操纵过程的各种信息进行逆向工程。我们还将分享一些值得未来研究的主题。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595113",
                    "title": "TMCIH: Perceptual Robust Image Hashing with Transformer-based Multi-layer Constraints",
                    "authors": "Yaodong Fang, Yuanding Zhou, Xinran Li, Ping Kong, Chuan Qin",
                    "abstract": "In recent decades, many perceptual image hashing schemes for content authentication have been proposed. However, existing algorithms cannot provide satisfactory robustness and discrimination in the face of complex manipulations in real scenarios. In this work, we propose a novel perceptual robust image hashing scheme with transformer-based multi-layer constraints. Specifically, we first exploit the Transformer structure into the field of perceptual image hashing, and an integrated loss function is designed to optimize the training of the model. In addition, to solve the issue of the simple content-preserving manipulations used in previous datasets, we construct a more challenging image dataset based on various manipulations, which can deal with complex image authentication scenarios. Experimental results demonstrate that our scheme achieves competitive results compared with existing schemes.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "TMCIH:基于变压器的多层约束的感知鲁棒图像哈希算法",
                    "abstract_zh": "近几十年来，已经提出了许多用于内容认证的感知图像哈希方案。然而，面对真实场景中的复杂操作，现有算法不能提供令人满意的鲁棒性和区分度。在这项工作中，我们提出了一个新的感知鲁棒图像哈希方案与变压器为基础的多层约束。具体来说，我们首先将变换器结构应用到感知图像哈希领域，并设计了一个集成的损失函数来优化模型的训练。此外，为了解决以往数据集中使用的简单内容保持操作的问题，我们基于各种操作构建了一个更具挑战性的图像数据集，它可以处理复杂的图像认证场景。实验结果表明，与现有方案相比，我们的方案取得了有竞争力的结果。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595110",
                    "title": "Perceptual Robust Hashing for Video Copy Detection with Unsupervised Learning",
                    "authors": "Gejian Zhao, Chuan Qin, Xiangyang Luo, Xinpeng Zhang, Chin-Chen Chang",
                    "abstract": "In this paper, we propose an end-to-end perceptual robust hashing scheme for video copy detection based on unsupervised learning. Firstly, the spatio-temporal information in videos is effectively fused and condensed into high-dimensional features through a 3D self-attention, multi-scale feature fusion model based on 3D-CNN, in which the Inception block and the 3D self-attention mechanism are integrated. Then, we calculate the correlation distances between the extracted features to differentiate perceptual contents. Based on the similarity relationship, we can dynamically generate the pseudo-labels and exploit them to further guide the model training for video hash generation. In addition, we design the dual constraints to make the hash code obtain satisfactory robustness and discrimination. Extensive experiments demonstrate that the proposed scheme achieves superior performance of copy detection compared with existing schemes and performs well even in the case of untrained manipulations.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "无监督学习的感知鲁棒哈希视频拷贝检测",
                    "abstract_zh": "本文提出了一种基于无监督学习的端到端感知鲁棒哈希视频拷贝检测方案。首先，通过一个基于3D-CNN的3D自关注多尺度特征融合模型，将视频中的时空信息有效融合并浓缩为高维特征。然后，我们计算提取的特征之间的相关距离来区分感知内容。基于相似性关系，我们可以动态地生成伪标签，并利用它们来进一步指导用于视频哈希生成的模型训练。此外，我们设计了双重约束，使哈希码获得满意的鲁棒性和区分度。大量实验表明，与现有方案相比，所提出的方案实现了更好的拷贝检测性能，并且即使在未经训练的操作的情况下也表现良好。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595098",
                    "title": "Video Frame Interpolation via Multi-scale Expandable Deformable Convolution",
                    "authors": "Dengyong Zhang, Pu Huang, Xiangling Ding, Feng Li, Gaobo Yang",
                    "abstract": "Video frame interpolation is a challenging task in the video processing field. Benefiting from the development of deep learning, many video frame interpolation methods have been proposed, which focus on sampling pixels with useful information to synthesize each output pixel using their own sampling operation. However, these works have data redundancy limitations and fail to sample the correct pixel of complex motions. To solve these problems, we propose a new warping framework to sample called multi-scale expandable deformable convolution(MSEConv) which employs a deep fully convolutional neural network to estimate multiple small-scale kernel weights with different expansion degrees and adaptive weight allocation for each pixel synthesis. MSEConv covers most prevailing research methods as special cases of it, thus MSEConv is also possible to be transferred to existing works for performance improvement. To further improve the robustness of the whole network to occlusion, we also introduce a data preprocessing method for mask occlusion in video frame interpolation. Quantitative and qualitative experiments show that our method shows a robust performance comparable to or even superior to the state-of-the-art method. Our source code and visual comparable results are available at https://github.com/Pumpkin123709/MSEConv.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "经由多尺度可扩展可变形卷积的视频帧内插",
                    "abstract_zh": "视频帧插值是视频处理领域中一项具有挑战性的任务。受益于深度学习的发展，已经提出了许多视频帧插值方法，这些方法专注于对具有有用信息的像素进行采样，以使用它们自己的采样操作来合成每个输出像素。然而，这些工作具有数据冗余限制，并且不能对复杂运动的正确像素进行采样。为了解决这些问题，我们提出了一种新的采样扭曲框架，称为多尺度可扩展可变形卷积(MSEConv ),它采用深度全卷积神经网络来估计多个具有不同扩展程度的小尺度核权重，并为每个像素合成自适应分配权重。MSEConv涵盖了大多数流行的研究方法，作为它的特例，因此MSEConv也有可能转移到现有的工作，以提高性能。为了进一步提高整个网络对遮挡的鲁棒性，我们还在视频帧插值中引入了一种针对遮挡的数据预处理方法。定量和定性实验表明，我们的方法表现出相当于甚至优于现有方法的稳健性能。我们的源代码和视觉可比结果可在https://github.com/Pumpkin123709/MSEConv.获得"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595093",
                    "title": "Compatibility and Timing Attacks for JPEG Steganalysis",
                    "authors": "Etienne Levecque, Patrick Bas, Jan Butora",
                    "abstract": "This paper introduces a novel compatibility attack to detect a steganographic message embedded in the DCT domain of a JPEG image at high-quality factors (close to 100). Because the JPEG compression is not a surjective function, i.e. not every DCT blocks can be mapped from a pixel block, embedding a message in the DCT domain can create incompatible blocks. We propose a method to find such a block, which directly proves that a block has been modified during the embedding. This theoretical method provides many advantages such as being completely independent to Cover Source Mismatch, having good detection power, and perfect reliability since false alarms are impossible as soon as incompatible blocks are found. We show that finding an incompatible block is equivalent to proving the infeasibility of an Integer Linear Programming problem. However, solving such a problem requires considerable computational power and has not been reached for 8x8 blocks. Instead, a timing attack approach is presented to perform steganalysis without potentially any false alarms for large computing power.",
                    "files": {
                        "openAccessPdf": "http://arxiv.org/pdf/2306.01317"
                    },
                    "title_zh": "JPEG隐写分析的兼容性和计时攻击",
                    "abstract_zh": "提出了一种新的兼容性攻击方法，以高质量因子(接近100)检测嵌入JPEG图像DCT域的隐写信息。因为JPEG压缩不是满射函数，即不是每个DCT块都可以从像素块映射，所以在DCT域中嵌入消息会产生不兼容的块。我们提出了一种方法来找到这样的块，这直接证明了块在嵌入期间已经被修改。这种理论方法提供了许多优点，例如完全独立于覆盖源失配，具有良好的检测能力，以及完美的可靠性，因为一旦发现不兼容的块，就不可能出现错误警报。我们证明了寻找一个不相容块等价于证明一个整数线性规划问题的不可行性。然而，解决这样的问题需要相当大的计算能力，并且对于8×8块还没有达到。相反，提出了一种定时攻击方法来执行隐写分析，而没有对大计算能力的任何潜在的假警报。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595095",
                    "title": "On Comparing Ad Hoc Detectors with Statistical Hypothesis Tests",
                    "authors": "Eli Dworetzky, Edgar Kaziakhmedov, Jessica J. Fridrich",
                    "abstract": "This paper addresses how to fairly compare ROCs of ad hoc (or data driven) detectors with tests derived from statistical models of digital media. We argue that the ways ROCs are typically drawn for each detector type correspond to different hypothesis testing problems with different optimality criteria, making the ROCs uncomparable. To understand the problem and why it occurs, we model a source of natural images as a mixture of scene oracles and derive optimal detectors for the task of image steganalysis. Our goal is to guarantee that, when the data follows the statistical model adopted for the hypothesis test, the ROC of the optimal detector bounds the ROC of the ad hoc detector. While the results are applicable beyond the field of image steganalysis, we use this setup to point out possible inconsistencies when comparing both types of detectors and explain guidelines for their proper comparison. Experiments on an artificial cover source with a known model with real steganographic algorithms and deep learning detectors are used to confirm our claims.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "特别检测器与统计假设检验的比较",
                    "abstract_zh": "本文阐述了如何公正地比较特设(或数据驱动)检测器的roc与从数字媒体的统计模型得出的测试。我们认为，为每种检测器类型绘制roc的方式通常对应于具有不同最优性标准的不同假设检验问题，使得roc不可比较。为了理解这个问题以及它为什么会发生，我们将自然图像的源建模为场景预言的混合，并推导出用于图像隐写分析任务的最佳检测器。我们的目标是保证，当数据遵循用于假设检验的统计模型时，最佳检测器的ROC限制了特设检测器的ROC。虽然结果适用于图像隐写分析以外的领域，但我们使用此设置来指出比较两种类型的检测器时可能存在的不一致，并解释正确比较它们的准则。在具有真实隐写算法和深度学习检测器的已知模型的人工覆盖源上的实验被用来证实我们的主张。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595097",
                    "title": "Progressive JPEGs in the Wild: Implications for Information Hiding and Forensics",
                    "authors": "Nora Hofer, Rainer Böhme",
                    "abstract": "JPEG images stored in progressive mode have become more prevalent recently. An estimated 30% of all JPEG images on the most popular websites use progressive mode. Presumably, this surge is caused by the adoption of MozJPEG, an open-source library designed for web publishers. So far, the optimizations used by MozJPEG have not been considered by the multimedia security community, although they are highly relevant. The goal of this paper is to document these optimizations and make them accessible to the research community. Most notably, we find that Trellis optimization in MozJPEG modifies quantized DCT coefficients in order to improve the rate-distortion tradeoff using a perceptual model based on PSNR-HVS. This may compromise the reliability of known methods in steganography, steganalysis, and image forensics when dealing with images compressed with MozJPEG. We also find that the type and order of scans in progressive mode, which MozJPEG adjusts to the image, offer novel cues that can aid forensic source identification.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "荒野中的渐进JPEGs:信息隐藏和取证的含义",
                    "abstract_zh": "最近，以渐进模式存储的JPEG图像变得越来越普遍。在最流行的网站上，估计有30%的JPEG图像使用渐进模式。据推测，这种激增是由于采用了MozJPEG，这是一个为网络出版商设计的开源库。到目前为止，MozJPEG使用的优化还没有被多媒体安全社区考虑，尽管它们是高度相关的。本文的目标是记录这些优化，并让研究社区能够访问它们。最值得注意的是，我们发现MozJPEG中的网格优化修改了量化的DCT系数，以便使用基于PSNR-HVS的感知模型来改善率失真权衡。当处理用MozJPEG压缩的图像时，这可能损害隐写术、隐写分析和图像取证中的已知方法的可靠性。我们还发现，渐进模式下扫描的类型和顺序(MozJPEG根据图像进行调整)提供了新的线索，有助于法医源识别。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595092",
                    "title": "Analysis and Mitigation of the False Alarms of the Reverse JPEG Compatibility Attack",
                    "authors": "Jan Butora, Patrick Bas, Rémi Cogranne",
                    "abstract": "The Reverse JPEG Compatibility Attack can be used for steganalysis of JPEG images compressed with Quality Factor 100 by detecting increased variance of decompression rounding errors. In this work, we point out the dangers associated with this attack by showing that in an uncontrolled environment, the variance can be elevated simply by using a different JPEG compressor. If not careful, the steganalyst can wrongly misclassify cover images. In order to deal with the diversity associated to the devices or softwares generating JPEGs, we propose in this paper to build a deep learning detector trained on a huge dataset of downloaded images. Experimental evaluation shows that such a detector can provide operational false alarms as small as 10-4, while still correctly classifying 90% of stego images. Furthermore, it is shown that this performance is directly applicable to other image datasets. As a side product, we indicate that the attack is not applicable to images developed with a specific JPEG compressor based on the trunc quantization function.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "反向JPEG兼容攻击的虚警分析与缓解",
                    "abstract_zh": "通过检测解压缩舍入误差的增加的方差，反向JPEG兼容性攻击可用于以质量因子100压缩的JPEG图像的隐写分析。在这项工作中，我们指出了与这种攻击相关的危险，表明在一个不受控制的环境中，方差可以通过使用不同的JPEG压缩器来提高。如果不小心，隐写分析师可能会错误地将封面图像分类。为了处理与生成JPEGs的设备或软件相关的多样性，我们在本文中提出建立一个在下载图像的巨大数据集上训练的深度学习检测器。实验评估表明，这种检测器可以提供小至10-4的操作错误警报，同时仍然正确地分类90%的隐写图像。此外，还表明这种性能可直接应用于其他图像数据集。作为副产品，我们指出这种攻击不适用于使用基于trunc量化函数的特定JPEG压缩器开发的图像。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595090",
                    "title": "Advancing the JPEG Compatibility Attack: Theory, Performance, Robustness, and Practice",
                    "authors": "Eli Dworetzky, Edgar Kaziakhmedov, Jessica J. Fridrich",
                    "abstract": "The JPEG compatibility attack is a steganalysis method for detecting messages embedded in the spatial representation of an image under the assumption that the cover image was a decompressed JPEG. This paper addresses a number of open problems in previous art, namely the lack of theoretical insight into how and why the attack works, low detection accuracy for high JPEG qualities, robustness to the JPEG compressor and DCT coefficient quantizer, and real-life performance evaluation. To explain the main mechanism responsible for detection and to understand the trends exhibited by heuristic detectors, we adopt a model of quantization errors of DCT coefficients in the recompressed image, and within a simplified setup, we analyze the behavior of the most powerful detector. Empowered by our analysis, we resolve the performance deficiencies using an SRNet trained on a two-channel input consisting of the image and its SQ error. This detector is compared with previous state of the art on four content-adaptive stego methods and for a wide range of payloads and quality factors. The last sections of this paper are devoted to studying robustness of this detector with respect to JPEG compressors, quantizers, and errors in estimating the JPEG quantization table. Finally, to demonstrate practical usability of this attack, we test our detector on stego images outputted by real steganographic tools available on the Internet.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "推进JPEG兼容性攻击:理论、性能、鲁棒性和实践",
                    "abstract_zh": "JPEG兼容性攻击是一种隐写分析方法，用于在假设封面图像是解压缩的JPEG的情况下检测嵌入在图像的空间表示中的消息。该论文解决了现有技术中的许多公开问题，即缺乏对攻击如何和为什么工作的理论洞察、对于高JPEG质量的低检测准确度、对JPEG压缩器和DCT系数量化器的鲁棒性以及真实生活性能评估。为了解释负责检测的主要机制并理解启发式检测器所展示的趋势，我们采用再压缩图像中的DCT系数的量化误差模型，并且在简化的设置中，我们分析最强大的检测器的行为。通过我们的分析，我们使用在双通道输入(由图像及其SQ误差组成)上训练的SRNet来解决性能缺陷。将该检测器与四种内容自适应隐写方法的现有技术进行了比较，并且针对广泛的有效载荷和质量因子。本文的最后部分致力于研究这种检测器相对于JPEG压缩器、量化器和估计JPEG量化表中的误差的鲁棒性。最后，为了证明这种攻击的实际可用性，我们在互联网上可用的真实隐写工具输出的隐写图像上测试了我们的检测器。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595094",
                    "title": "Limits of Data Driven Steganography Detectors",
                    "authors": "Edgar Kaziakhmedov, Eli Dworetzky, Jessica J. Fridrich",
                    "abstract": "While deep learning has revolutionized image steganalysis in terms of performance, little is known about how much modern data driven detectors can still be improved. In this paper, we approach this difficult and currently wide open question by working with artificial but realistic looking images with a known statistical model that allows us to compute the detectability of modern content-adaptive algorithms with respect to the most powerful detectors. Multiple artificial image datasets are crafted with different levels of content complexity and noise power to assess their influence on the gap between both types of detectors. Experiments with SRNet as the heuristic detector indicate that independent noise contributes less to the performance gap than content of the same MSE. While this loss is rather small for smooth images, it can be quite large for textured images. A network trained on many realizations of a fixed textured scene will, however, recuperate most of the loss, suggesting that networks have the capacity to approximately learn the parameters of a cover source narrowed to a fixed scene.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "数据驱动隐写检测器的局限性",
                    "abstract_zh": "虽然深度学习在性能方面彻底改变了图像隐写分析，但很少有人知道现代数据驱动的检测器还可以改进多少。在本文中，我们通过使用具有已知统计模型的人工但逼真的图像来解决这个困难且目前广泛公开的问题，该模型允许我们计算现代内容自适应算法相对于最强大的检测器的可检测性。用不同水平的内容复杂度和噪声功率制作多个人工图像数据集，以评估它们对两种类型的检测器之间的间隙的影响。使用SRNet作为启发式检测器的实验表明，独立噪声比相同MSE的内容对性能差距的贡献更小。虽然这种损失对于平滑图像来说相当小，但是对于纹理图像来说可能相当大。然而，在固定纹理场景的许多实现上训练的网络将恢复大部分损失，这表明网络具有近似学习缩小到固定场景的覆盖源的参数的能力。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595100",
                    "title": "Calibration-based Steganalysis for Neural Network Steganography",
                    "authors": "Na Zhao, Kejiang Chen, Chuan Qin, Yi Yin, Weiming Zhang, Nenghai Yu",
                    "abstract": "Recent research has shown that neural network models can be used to steal sensitive data or embed malware. Therefore, steganalysis for neural networks is urgently needed. However, existing neural network steganalysis methods do not perform well under small embedding rates. In addition, because of the large number of parameters, the neural network steganography method under a small embedding rate can embed enough information into the model for malicious purposes. To address this problem, this paper proposes a calibration-based steganalysis method, which fine-tunes the original neural network model without implicit constraints to obtain a reference model, then extracts and fuses statistical moments from the parameter distributions of the original model and its reference model, and finally trains a logistic regressor for detection. Extensive experiments show that the proposed method has superior performance in detecting steganographic neural network models under small embedding rates.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "基于标定的神经网络隐写术隐写分析",
                    "abstract_zh": "最近的研究表明，神经网络模型可以用来窃取敏感数据或嵌入恶意软件。因此，迫切需要对神经网络进行隐写分析。然而，现有的神经网络隐写分析方法在低嵌入率下表现不佳。此外，由于参数数量大，小嵌入率下的神经网络隐写方法可以在模型中嵌入足够多的信息用于恶意目的。针对这一问题，提出一种基于标定的隐写分析方法，该方法在没有隐含约束的情况下对原始神经网络模型进行微调，得到一个参考模型，然后从原始模型及其参考模型的参数分布中提取统计矩并进行融合，最后训练一个逻辑回归器进行检测。大量实验表明，该方法在低嵌入率下检测隐写神经网络模型具有优越的性能。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595111",
                    "title": "SCL-Stega: Exploring Advanced Objective in Linguistic Steganalysis using Contrastive Learning",
                    "authors": "Juan Wen, Liting Gao, Guangying Fan, Ziwei Zhang, Jianghao Jia, Yiming Xue",
                    "abstract": "Text steganography is becoming increasingly secure by eliminating the distribution discrepancy between normal and stego text. On the other hand, the existing cross-entropy-based steganalysis models struggle to distinguish subtle distribution differences and lack robustness regarding confusable samples. To enhance steganalysis accuracy on hard-to-detect samples, this paper draws on contrastive learning to design a text steganalysis framework incorporating supervised contrastive loss into the training process. This framework improves feature representation by pushing apart embeddings from different classes while pulling closer embeddings from the same class. The experimental results show that our method makes remarkable improvement compared to the four baseline models. Additionally, as the embedding rate increases, our method's advantages become increasingly apparent, with maximum improvements of 13.98%, 12.47%, and 13.65% over the baseline methods across three common linguistic steganalysis datasets, Twitter, IMDB, and News, respectively. Our code is available at https://github.com/Katelin-glt/SCL-Stega https://github.com/katelin-glt/SCL-Stega.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "SCL-斯蒂格:使用对比学习探索语言隐写分析的高级目标",
                    "abstract_zh": "通过消除正常文本和隐写文本之间的分布差异，文本隐写术变得越来越安全。另一方面，现有的基于交叉熵的隐写分析模型难以区分细微的分布差异，并且缺乏对易混淆样本的鲁棒性。为了提高难以检测样本的隐写分析精度，借鉴对比学习方法，设计了一个将监督对比损失融入训练过程的文本隐写分析框架。该框架通过从不同的类推开嵌入，同时从同一类拉近嵌入来改进特征表示。实验结果表明，与四种基线模型相比，我们的方法取得了显著的改进。此外，随着嵌入率的增加，我们的方法的优势变得越来越明显，在三个常见的语言隐写分析数据集Twitter、IMDB和News上，与基线方法相比分别提高了13.98%、12.47%和13.65%。我们的代码可在https://github.com/Katelin-glt/SCL-Stega https://github.com/katelin-glt/SCL-Stega.获得"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595091",
                    "title": "An Improved Reversible Database Watermarking Method based on Histogram Shifting",
                    "authors": "Cheng Li, Xinhui Han, Wenfa Qi, Zongming Guo",
                    "abstract": "Database watermarking is typically employed to address the issues of data theft, illegal replication, and copyright infringement that may arise during the sharing of databases. Unfortunately, the existing methods often cause permanent distortion to the original data, and it is challenging to strike a balance between the watermark embedding capacity and data distortion. Therefore, this paper proposes a reversible database watermarking method based on histogram shifting, rhombus prediction, and double embedding with high capacity and low distortion, called RPDE-HSW. By utilizing the rhombus prediction, we respectively constructed two prediction error histograms in each subgroup and expanded the watermark capacity through the adoption of double-layer embedding and single-bin embedding 2 bits. A scrambling algorithm is used to make the attribute value distribution more discretized, resulting in a sparse distribution of the database histogram. Subsequently, we optimized the selection rules for the watermark embedding carrier, effectively eliminating the redundant distortion caused by histogram shifting. Experimental results demonstrate that the proposed method achieves smaller data distortion and higher watermark embedding capacity, outperforming some other state-of-the-art works, and does not affect the classification results and data mining.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "一种改进的基于直方图平移的可逆数据库水印方法",
                    "abstract_zh": "数据库水印通常用于解决在数据库共享过程中可能出现的数据窃取、非法复制和版权侵犯等问题。不幸的是，现有的方法经常导致原始数据的永久失真，并且在水印嵌入容量和数据失真之间取得平衡是具有挑战性的。因此，本文提出了一种基于直方图平移、菱形预测和高容量低失真双重嵌入的可逆数据库水印方法，称为RPDE-HSW。利用菱形预测，在每个子组中分别构造两个预测误差直方图，并采用双层嵌入和单仓嵌入2比特来扩展水印容量。使用置乱算法使属性值分布更加离散化，从而导致数据库直方图的稀疏分布。随后，优化了水印嵌入载体的选择规则，有效地消除了直方图平移带来的冗余失真。实验结果表明，该方法具有较小的数据失真和较高的水印嵌入容量，优于其他一些最新的方法，并且不影响分类结果和数据挖掘。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595099",
                    "title": "Applying a Zero-Knowledge Watermarking Protocol to Secure Elections",
                    "authors": "Scott Craver, Nicholas Rosbrook",
                    "abstract": "Zero-knowledge protocols for digital watermarking allow a copyright holder to prove that a signal is hidden in a cover without revealing the signal or where it is. This same approach can be applied outside of watermarking to other problems where anonymity is required, and in particular to secure elections. We outline steganographic zero-knowledge protocols---proving in zero knowledge the existence of a hidden object in a cover---and adapt them to prove that selected ballot values must exist within a volume of anonymized voter receipts. We also improve the space efficiency of such protocols by employing camouflaged elliptic curve cryptography.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "零知识水印协议在安全选举中的应用",
                    "abstract_zh": "数字水印的零知识协议允许版权所有者证明信号隐藏在封面中，而不暴露信号或信号在哪里。这种相同的方法可以应用于水印之外的需要匿名的其他问题，特别是安全选举。我们概述了隐写零知识协议——在零知识下证明封面中隐藏对象的存在——并对它们进行了修改，以证明选定的选票值必须存在于大量匿名的选民收据中。我们还通过使用伪装的椭圆曲线密码来提高协议的空间效率。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595096",
                    "title": "On the Feasibility of Post-Mortem Hand-Based Vascular Biometric Recognition",
                    "authors": "Simon Kirchgasser, Christof Kauba, Bernhard Prommegger, Fabio Monticelli, Andreas Uhl",
                    "abstract": "Recently, there is a growing interest to employ biometrics in post-mortem forensics, mainly to replace cost intensive radiology based imaging devices. While it has been shown that post-mortem biometric recognition is feasible for fingerprints, face and iris, no studies regarding post-mortem vasculature pattern recognition have been published. Based on the first reported post-mortem hand- and finger-vein dataset, the hypothesis, that hand vasculature biometrics can be used as post-mortem biometric modality, is falsified. Using an indirect proof, it is shown that no usable vascular features are present in the small amount of sample data collected, by visual inspection as well as by applying several biometric quality metrics, which confirm that hand-based vasculature biometrics can not be used as post-mortem biometric modality.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "基于死后手部血管生物特征识别的可行性研究",
                    "abstract_zh": "最近，人们越来越有兴趣在死后取证中采用生物识别技术，主要是为了取代基于成像设备的高成本放射学。虽然已经表明死后生物统计识别对于指纹、面部和虹膜是可行的，但是还没有关于死后脉管系统模式识别的研究被公开。基于首次报道的死后手部和手指静脉数据集，手部脉管系统生物特征可用作死后生物特征模态的假设被证伪。使用间接证明，通过视觉检查以及通过应用若干生物计量质量度量，表明在收集的少量样本数据中不存在可用的血管特征，这证实了基于手的血管生物计量不能用作死后生物计量模态。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595102",
                    "title": "Differentially Private Adversarial Auto-Encoder to Protect Gender in Voice Biometrics",
                    "authors": "Oubaïda Chouchane, Michele Panariello, Oualid Zari, Ismet Kerenciler, Imen Chihaoui, Massimiliano Todisco, Melek Önen",
                    "abstract": "Over the last decade, the use of Automatic Speaker Verification (ASV) systems has become increasingly widespread in response to the growing need for secure and efficient identity verification methods. The voice data encompasses a wealth of personal information, which includes but is not limited to gender, age, health condition, stress levels, and geographical and socio-cultural origins. These attributes, known as soft biometrics, are private and the user may wish to keep them confidential. However, with the advancement of machine learning algorithms, soft biometrics can be inferred automatically, creating the potential for unauthorized use. As such, it is crucial to ensure the protection of these personal data that are inherent within the voice while retaining the utility of identity recognition. In this paper, we present an adversarial Auto-Encoder-based approach to hide gender-related information in speaker embeddings, while preserving their effectiveness for speaker verification. We use an adversarial procedure against a gender classifier and incorporate a layer based on the Laplace mechanism into the Auto-Encoder architecture. This layer adds Laplace noise for more robust gender concealment and ensures differential privacy guarantees during inference for the output speaker embeddings. Experiments conducted on the VoxCeleb dataset demonstrate that speaker verification tasks can be effectively carried out while concealing speaker gender and ensuring differential privacy guarantees; moreover, the intensity of the Laplace noise can be tuned to select the desired trade-off between privacy and utility.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "在语音生物测定中保护性别的差分私有对抗性自动编码器",
                    "abstract_zh": "在过去的十年中，自动说话人确认(ASV)系统的使用变得越来越广泛，以响应对安全和有效的身份确认方法的日益增长的需求。语音数据包含大量个人信息，包括但不限于性别、年龄、健康状况、压力水平以及地理和社会文化背景。这些被称为软生物特征的属性是私有的，用户可能希望对它们保密。然而，随着机器学习算法的进步，可以自动推断出软生物特征，从而产生了未经授权使用的可能性。因此，在保留身份识别功能的同时，确保语音中固有的这些个人数据得到保护至关重要。在本文中，我们提出了一种基于对立自动编码器的方法来隐藏说话人嵌入中的性别相关信息，同时保持其对说话人确认的有效性。我们针对性别分类器使用对抗过程，并且将基于拉普拉斯机制的层合并到自动编码器架构中。该层增加了拉普拉斯噪声，用于更鲁棒的性别隐藏，并确保在推断输出说话者嵌入期间的差分隐私保证。在VoxCeleb数据集上进行的实验表明，说话人验证任务可以有效地执行，同时隐藏说话人性别并确保不同的隐私保证；此外，拉普拉斯噪声的强度可以被调整以选择隐私和效用之间的期望的折衷。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595107",
                    "title": "Hand Vein Spoof GANs: Pitfalls in the Assessment of Synthetic Presentation Attack Artefacts",
                    "authors": "Andreas Vorderleitner, Jutta Hämmerle-Uhl, Andreas Uhl",
                    "abstract": "I2I translation techniques for unpaired data are used for the creation of biometric presentation attack artefact samples. For the assessment of these synthetic samples, we analyse their behaviour when attacking hand vein recognition systems, comparing these results to such obtained from actually crafted presentation attack samples. We observe that although visual appearance and sample set correspondence are suprisingly good, respectively, the assessment of the behaviour of the data in a conducted attack is more difficult. Even if for some recognition schemes we find a good accordance in terms of IAPMR (for others we don't), the attack score distributions turn out to be highly dissimilar. More work is needed for reliable assesment of such data, to be able to correctly interpret corresponding results with respect to the usefulness in attack simulation.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "手静脉欺骗GANs:评估合成演示攻击人工制品的陷阱",
                    "abstract_zh": "不成对数据的I2I翻译技术用于创建生物特征表示攻击假象样本。为了评估这些合成样本，我们分析了它们攻击手静脉识别系统时的行为，将这些结果与从实际制作的演示攻击样本中获得的结果进行比较。我们观察到，尽管视觉外观和样本集对应性分别令人惊讶地好，但对有组织攻击中的数据行为的评估却更加困难。即使对于一些识别方案，我们在IAPMR方面找到了很好的一致性(对于其他方案，我们没有)，攻击分数分布也变得非常不同。需要做更多的工作来可靠地评估这些数据，以便能够正确地解释与攻击模拟的有用性相关的相应结果。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595105",
                    "title": "First Learning Steps to Recognize Faces in the Noise",
                    "authors": "Lukas Lamminger, Heinz Hofbauer, Andreas Uhl",
                    "abstract": "A UNet-type encoder-decoder inpainting network is applied to weaken the protection strength of selectively encrypted face samples. Based on visual assessment, FaceQNet quality, and ArcFace recognition accuracy the strategy is shown to be successful, however, to a different extent depending on the original protection strength. For almost cryptographic strength, inpainting does not cause a practically relevant protection weakening, while for lower original protection strength inpainting almost removes the protection entirely.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "在噪音中识别人脸的第一步学习",
                    "abstract_zh": "应用UNet型编码器-解码器修复网络来削弱选择性加密的人脸样本的保护强度。基于视觉评估、FaceQNet质量和ArcFace识别准确度，该策略被证明是成功的，然而，在不同程度上取决于原始保护强度。对于几乎加密强度，修补不会导致实际相关的保护减弱，而对于较低的原始保护强度，修补几乎完全移除了保护。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595101",
                    "title": "Comprehensive Dataset of Synthetic and Manipulated Overhead Imagery for Development and Evaluation of Forensic Tools",
                    "authors": "Brandon B. May, Kirill Trapeznikov, Shengbang Fang, Matthew C. Stamm",
                    "abstract": "We present a first of its kind dataset of overhead imagery for development and evaluation of forensic tools. Our dataset consists of real, fully synthetic and partially manipulated overhead imagery generated from a custom diffusion model trained on two sets of different zoom levels and on two sources of pristine data. We developed our model to support controllable generation of multiple manipulation categories including fully synthetic imagery conditioned on real and generated base maps, and location. We also support partial in-painted imagery with same conditioning options and with several types of manipulated content. The data consist of raw images and ground truth annotations describing the manipulation parameters. We also report benchmark performance on several tasks supported by our dataset including detection of fully and partially manipulated imagery, manipulation localization and classification.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "用于开发和评估法医工具的合成和处理过的空中影像的综合数据集",
                    "abstract_zh": "我们提出了第一个用于开发和评估法医工具的空中影像数据集。我们的数据集由真实的、完全合成的和部分处理过的高架影像组成，这些影像是从一个定制的扩散模型生成的，该模型是在两组不同的缩放级别和两个原始数据源上训练的。我们开发了我们的模型，以支持多种操作类别的可控生成，包括基于真实和生成的底图以及位置的完全合成图像。我们还支持具有相同条件选项和多种类型的操作内容的部分内绘图像。数据由原始图像和描述操作参数的地面实况注释组成。我们还报告了我们的数据集支持的几个任务的基准性能，包括检测完全和部分操纵的图像，操纵定位和分类。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595108",
                    "title": "MetaFake: Few-shot Face Forgery Detection with Meta Learning",
                    "authors": "Nanqing Xu, Weiwei Feng",
                    "abstract": "With remarkable progress achieved by facial forgery technologies, their potential security risks cause serious concern to society since they can easily fool face recognition systems and even human beings. Current forgery detection methods have achieved excellent performance when training with a large-scale database. However, they usually fail to give correct predictions in real applications where only a few fake samples created by unseen forgery methods are available. In this paper, we propose a novel method to boost the performance of identifying samples generated by unseen techniques, dubbed MetaFake, which requires only a few fake samples. Our MetaFake enjoys the part features located by meta forgery prototypes created adaptively based on each task. The local-aggregated module helps to integrate these part features for the final prediction. Besides, we establish a large database of about 0.6 million images to verify the proposed method, including fake samples synthesized by 18 forgery techniques. Extensive experiments demonstrate the superior performance of the proposed method.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "MetaFake:基于元学习的少镜头人脸伪造检测",
                    "abstract_zh": "随着人脸伪造技术的显著进步，其潜在的安全风险引起了社会的严重关注，因为它们可以轻易地欺骗人脸识别系统甚至人类。当前的伪造检测方法在用大规模数据库训练时已经取得了优异的性能。然而，在实际应用中，它们通常不能给出正确的预测，在实际应用中，只有少数由看不见的伪造方法产生的假样本可用。在本文中，我们提出了一种新的方法来提高识别由未知技术产生的样本的性能，称为MetaFake，它只需要少量的伪样本。我们的MetaFake享受基于每个任务自适应创建的元伪造原型所定位的部件特征。局部聚集模块有助于为最终预测整合这些零件特征。此外，我们建立了一个约60万幅图像的大型数据库来验证所提出的方法，其中包括由18种伪造技术合成的假样本。大量实验证明了该方法的优越性能。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595112",
                    "title": "Synthesized Speech Attribution Using The Patchout Spectrogram Attribution Transformer",
                    "authors": "Kratika Bhagtani, Emily R. Bartusiak, Amit Kumar Singh Yadav, Paolo Bestagini, Edward J. Delp",
                    "abstract": "The malicious use of synthetic speech has increased with the recent availability of speech generation tools. It is important to determine whether a speech signal is authentic (spoken by a human) or is synthesized and to determine the generation method used to create it. Identifying the synthesis method is known as synthetic speech attribution. In this paper, we propose the use of a transformer deep learning method that analyzes mel-spectrograms for synthetic speech attribution. Our method known as Patchout Spectrogram Attribution Transformer (PSAT) can distinguish new, unseen speech generation methods from those seen during training. PSAT demonstrates high performance in attributing synthetic speech signals. Evaluation on the DARPA SemaFor Audio Attribution Dataset and the ASVSpoof2019 Dataset shows that our method achieves more than 95% accuracy in synthetic speech attribution and performs better than existing deep learning approaches.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "使用拼接声谱图属性转换器的合成语音属性",
                    "abstract_zh": "随着最近语音生成工具的出现，合成语音的恶意使用增加了。重要的是确定语音信号是真实的(由人说出)还是合成的，并确定用于创建它的生成方法。识别合成方法被称为合成语音归属。在本文中，我们提出了使用transformer深度学习方法来分析合成语音属性的mel频谱图。我们的方法称为Patchout声谱图属性转换器(PSAT)，可以区分新的，看不见的语音生成方法和那些在训练中看到的方法。PSAT在识别合成语音信号方面表现出很高的性能。在DARPA SemaFor音频属性数据集和ASVSpoof2019数据集上的评估表明，我们的方法在合成语音属性方面达到了95%以上的准确率，并且性能优于现有的深度学习方法。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595104",
                    "title": "Extracting Efficient Spectrograms From MP3 Compressed Speech Signals for Synthetic Speech Detection",
                    "authors": "Ziyue Xiang, Amit Kumar Singh Yadav, Stefano Tubaro, Paolo Bestagini, Edward J. Delp",
                    "abstract": "Many speech signals are compressed with MP3 to reduce the data rate. In many synthetic speech detection methods the spectrogram of the speech signal is used. This usually requires the speech signal to be fully decompressed. We show that the design of MP3 compression allows one to approximate the spectrogram of the MP3 compressed speech efficiently without fully decoding the compressed speech. We denote the spectograms obtained using our proposed approach by Efficient Spectrograms (E-Specs). E-Spec can reduce the complexity of spectrogram computation by ~77.60 percentage points (p.p.) and save ~37.87 p.p. of MP3 decoding time. E-Spec bypasses the reconstruction artifacts introduced by the MP3 synthesis filterbank, which makes it useful in speech forensics tasks. We tested E-Spec in the synthetic speech detection, where a detector is asked to determine whether a speech signal is synthesized or recorded from a human. We examined 4 different neural network architectures to evaluate the performance of E-Spec compared to speech features extracted from the fully decoded speech signal. E-Spec achieved the best synthetic speech detection performance for 3 architectures; it also achieved the best overall detection performance across architectures. The computation of E-Spec is an approximation to Short Time Fourier Transform (STFT). E-Spec can be extended to other audio compression methods.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "从MP3压缩语音信号中提取有效频谱图用于合成语音检测",
                    "abstract_zh": "许多语音信号用MP3压缩以降低数据速率。在许多合成语音检测方法中，使用语音信号的频谱图。这通常要求语音信号被完全解压缩。我们表明，MP3压缩的设计允许人们有效地近似MP3压缩语音的频谱图，而无需完全解码压缩语音。我们用有效光谱图(E-Specs)来表示使用我们提出的方法获得的光谱图。E-Spec可将声谱图计算的复杂度降低约77.60个百分点(p.p .)，并节省约37.87 p.p .的MP3解码时间。E-Spec绕过了MP3合成滤波器组引入的重构伪像，这使得它在语音取证任务中非常有用。我们在合成语音检测中测试了E-Spec，其中要求检测器确定语音信号是合成的还是从人类记录的。我们检查了4种不同的神经网络架构，以评估E-Spec与从完全解码的语音信号中提取的语音特征相比的性能。E-Spec实现了3种架构的最佳合成语音检测性能；它还实现了跨架构的最佳整体检测性能。E-Spec的计算是对短时傅立叶变换(STFT)的近似。E-Spec可以扩展到其他音频压缩方法。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595103",
                    "title": "Exposing Deepfakes using Dual-Channel Network with Multi-Axis Attention and Frequency Analysis",
                    "authors": "Yue Zhou, Bing Fan, Pradeep K. Atrey, Feng Ding",
                    "abstract": "This paper proposes a dual-channel network for DeepFake detection. The network comprises two channels: one using a stacked Maxvit block to process the downsampled original images, and the other using a stacked ResNet basic block to capture features from the discrete cosine transform of the image spectrums. The components extracted from the two channels are concatenated using a linear layer to train the entire model for exposing DeepFakes. Experimental results demonstrate that the proposed method could achieve satisfactory forensics performance. Besides, the experiments of cross-dataset evaluations prove it is also high in generalizability.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "使用具有多轴注意力和频率分析的双通道网络来揭露Deepfakes",
                    "abstract_zh": "提出了一种用于深度伪造检测的双通道网络。该网络包括两个通道:一个使用堆叠的Maxvit块来处理下采样的原始图像，另一个使用堆叠的ResNet基本块来从图像频谱的离散余弦变换中捕获特征。使用线性层将从两个通道提取的组件连接起来，以训练用于暴露DeepFakes的整个模型。实验结果表明，该方法能够取得令人满意的取证效果。此外，跨数据集评价实验证明该方法具有较高的泛化能力。"
                },
                {
                    "url": "https://doi.org/10.1145/3577163.3595106",
                    "title": "Fooling State-of-the-art Deepfake Detection with High-quality Deepfakes",
                    "authors": "Arian Beckmann, Anna Hilsmann, Peter Eisert",
                    "abstract": "Due to the rising threat of deepfakes to security and privacy, it is most important to develop robust and reliable detectors. In this paper, we examine the need for high-quality samples in the training datasets of such detectors. Accordingly, we show that deepfake detectors proven to generalize well on multiple research datasets still struggle in real-world scenarios with well-crafted fakes. First, we propose a novel autoencoder for face swapping alongside an advanced face blending technique, which we utilize to generate 90 high-quality deepfakes. Second, we feed those fakes to a state-of-the-art detector, causing its performance to decrease drastically. Moreover, we fine-tune the detector on our fakes and demonstrate that they contain useful clues for the detection of manipulations. Overall, our results provide insights into the generalization of deepfake detectors and suggest that their training datasets should be complemented by high-quality fakes since training on mere research data is insufficient.",
                    "files": {
                        "openAccessPdf": ""
                    },
                    "title_zh": "用高质量的Deepfakes愚弄最先进的Deepfake检测",
                    "abstract_zh": "由于deepfakes对安全和隐私的威胁越来越大，开发鲁棒和可靠的检测器是最重要的。在本文中，我们研究了这种检测器的训练数据集中对高质量样本的需求。因此，我们表明deepfake检测器被证明在多个研究数据集上推广良好，但在真实世界的场景中仍然难以对付制作精良的假货。首先，我们提出了一种新颖的自动编码器用于人脸交换，同时采用了一种先进的人脸混合技术，我们利用这种技术生成了90个高质量的deepfakes。第二，我们将这些假货输入最先进的检测器，导致其性能急剧下降。此外，我们对我们的假货检测器进行了微调，并证明它们包含了检测操纵的有用线索。总的来说，我们的结果提供了对deepfake检测器的泛化的见解，并建议他们的训练数据集应该由高质量的假货来补充，因为仅依靠研究数据进行训练是不够的。"
                }
            ]
        }
    ]
}